---

layout: post
title: Fairseq Tutorial
date: 2019-05-22 10:28:09
tags: [NLP]
categories: [NLP]
typora-root-url: ../../static
---

A reading notes of fairseq [document](https://fairseq.readthedocs.io/en/latest/) and [github page](https://github.com/pytorch/fairseq). 

# Overview

## Command-Line

æµç¨‹åˆ†ä¸ºä»¥ä¸‹å‡ æ­¥ï¼š

1. `fairseq-preprocess`æ¥é¢„å¤„ç†æ•°æ®ï¼Œç”¨æ¥å»ºç«‹è¯è¡¨ï¼Œå¹¶ç”Ÿæˆè®­ç»ƒä½¿ç”¨çš„äºŒè¿›åˆ¶æ•°æ®ã€‚
2. `fairseq-train`æ¥è®­ç»ƒæ¨¡å‹ï¼Œé»˜è®¤ä½¿ç”¨å…¨éƒ¨GPUï¼Œä½¿ç”¨`CUDA_VISIBLE_DEVICES`æ¥è°ƒæ•´ç¨‹åºå¯è§çš„GPUæ•°ç›®ï¼Œâš ï¸æ³¨æ„batch_sizeçš„è®¾å®šæ˜¯æ ¹æ®tokensæ¥çš„ï¼Œå‚æ•°`max-tokens`ç”¨äºæŒ‡å®šbatchä¸­æœ€å¤§tokenæ•°ã€‚
3. è®­ç»ƒå®Œåï¼Œç”¨`fairseq-generate`åœ¨äºŒè¿›åˆ¶æ•°æ®ä¸Šè¿›è¡Œæµ‹è¯•ï¼Œraw textåˆ™éœ€è¦ä½¿ç”¨`fairseq-interactive`ã€‚å…¶ä¸­ç»§æ‰¿äº†beam-searchçš„æ–¹æ³•ã€‚

## Extend

é™¤äº†ä½¿ç”¨ç°æˆçš„æ¨¡å‹å’Œä»»åŠ¡ä¹‹å¤–ï¼Œè¿˜å¯ä»¥è‡ªå·±æ‰©å±•äº”ç§ç±»å‹ï¼š

- Modelsç”¨äºå®šä¹‰æ¨¡å‹çš„ç»“æ„ï¼ŒåŒ…æ‹¬æ‰€æœ‰éœ€è¦è®­ç»ƒçš„å‚æ•°ã€‚
- Criterionæ¥å®šä¹‰æ–°çš„lossè®¡ç®—æ–¹æ³•ï¼Œæ ¹æ®targetå’Œoutputæ¥ç®—ã€‚
- âš ï¸Tasksç”¨äºä¿å­˜è¯è¡¨ï¼Œæä¾›datasetçš„è¯»å–å’Œè¿­ä»£æ–¹æ³•ï¼Œåˆå§‹åŒ–Modelå’ŒCriterionå¹¶è®¡ç®—Lossã€‚åƒä¸ªæ¡†æ¶ã€‚
- OptimizeræŒ‡å®šå¦‚ä½•æ›´æ–°æ¨¡å‹ä¸­çš„å‚æ•°ã€‚
- Learning Rate Schedulerså®šä¹‰äº†è®­ç»ƒè¿‡ç¨‹ä¸­å­¦ä¹ ç‡çš„å˜åŒ–ã€‚

æ•´ä¸ªçš„é«˜å±‚å·¥ä½œæµç¨‹ä¸ºï¼š

```python
for epoch in range(num_epochs):
    itr = task.get_batch_iterator(task.dataset('train'))
    for num_updates, batch in enumerate(itr):
        task.train_step(batch, model, criterion, optimizer) # åŒ…æ‹¬äº†backward()
        average_and_clip_gradients()
        optimizer.step()
        lr_scheduler.step_update(num_updates)
    lr_scheduler.step(epoch)
```

## Register

æ–°æ¨¡å—åªæœ‰ç»è¿‡æ³¨å†Œä»¥åæ‰èƒ½åœ¨å‘½ä»¤è¡Œä¸­ä½¿ç”¨ï¼Œæ–¹æ³•å¦‚ä¸‹ï¼š

```python
@register_model('my_lstm')
class MyLSTM(FairseqModel):
    (...)
```

è‡ªå·±å†™çš„æ¨¡å—ä¹Ÿå¯ä»¥ä¿å­˜åœ¨è‡ªå·±çš„è·¯å¾„ä¸‹é¢ï¼ˆè€Œéæ¡†æ¶é»˜è®¤çš„fairseq/modelä¸‹ï¼‰ï¼Œè¿™ç§æ—¶å€™åœ¨å‘½ä»¤è¡Œä¸­å°±è¦ä½¿ç”¨`--user-dir`å‚æ•°æŒ‡å®šcustomæ–‡ä»¶å¤¹ï¼Œ

# ä¸€ä¸ªä¾‹å­

åšä¸€ä¸ªLSTM encoder-decoderæ¨¡å‹ã€‚

## Encoder/Decoder

è¿™ä¸¤éƒ¨åˆ†ç»§æ‰¿`FairseqEncoder/Decoder`ï¼ŒåŸºç±»æœ¬èº«ä¹Ÿæ˜¯ç»§æ‰¿è‡ª`nn.Module`ï¼Œåªæ˜¯æ™®é€šçš„ä¸¤ä¸ªPyTorchæ¨¡å—ã€‚

æ³¨æ„åˆ°åœ¨initä¸­è¿™ä¿©æ¨¡å—éƒ½ä¼ å…¥äº†dictionaryå‚æ•°ï¼Œå¹¶`super().__init__(dictionary)`äº†ä¸€ä¸‹ã€‚æ˜¯ä¸ºäº†è·å–è¯è¡¨é•¿åº¦ï¼Œpadå­—ç¬¦ç­‰ä¿¡æ¯ã€‚

## Register the Model

é€šè¿‡`@register_model('new_model')`çš„è£…é¥°æ–¹æ³•æ³¨å†Œæ–°æ¨¡å‹ï¼Œåªæœ‰registerçš„modelæ‰å¯ä»¥ç”¨å‘½ä»¤è¡Œç›´æ¥è°ƒç”¨ã€‚

æ‰€æœ‰Modeléƒ½å¿…é¡»å®ç°`BasicFairseqModel`çš„æ‰€æœ‰æ¥å£ï¼Œå¯¹äºSeq2seqä»»åŠ¡æˆ‘ä»¬å¯ä»¥ç»§æ‰¿`FairseqModel`ï¼Œå› ä¸ºå®ƒç»§æ‰¿äº†Basicå¹¶é»˜è®¤äº†encoder-decoderç»“æ„ã€‚

```python
from fairseq.models import FairseqModel, register_model

@register_model('simple_lstm')	# å¿…é¡»åœ¨Modelç±»å‰åŠ è£…é¥°
class SimpleLSTMModel(FairseqModel):
  
    @staticmethod
    def add_args(parser):	# é‡å†™è¿™ä¸ªBasicç±»ä¸­çš„æ¥å£ï¼Œæ·»åŠ å‘½ä»¤è¡Œå‚æ•°
        parser.add_argument(
            '--encoder-embed-dim', type=int, metavar='N',
            help='dimensionality of the encoder embeddings',
        )
        
    @classmethod
    def build_model(cls, args, task):	# ç”¨äºåˆå§‹åŒ–æ¨¡å‹ï¼Œæ˜¯classmethod
        encoder = SimpleLSTMEncoder()	# çœç•¥å‚æ•°
        decoder = SimpleLSTMDecoder()
        model = SimpleLSTMModel(encoder, decoder)	# è¿™é‡Œç”¨äº†SimpleLSTMModelç±»æœ¬èº«
        
        print(model)
        return model	# å¿…é¡»è¿”å›æ¨¡å‹ï¼Œmodelåœ¨build_modelä¸­å®šä¹‰
      
    # å¯é‡å†™åˆå§‹åŒ–ï¼Œè¿™é‡ŒFairseqModelé»˜è®¤çš„encoder-decoderç»“æ„
    def __init__(self, encoder, decoder):
        super().__init__()

        self.encoder = encoder
        self.decoder = decoder
        assert isinstance(self.encoder, FairseqEncoder)
        assert isinstance(self.decoder, FairseqDecoder)
      
    # å¯é‡å†™forwardæ–¹æ³•æ¥å†³å®šencoderå’Œdecoderäº¤äº’ç»†èŠ‚ï¼Œä¸‹é¢æ˜¯FairseqModelä¸­defaultçš„åšæ³•
    def forward(self, src_tokens, src_lengths, prev_output_tokens):
        encoder_out = self.encoder(src_tokens, src_lengths)
        decoder_out = self.decoder(prev_output_tokens, encoder_out)
        return decoder_out
```

âš ï¸è¿™é‡Œçš„build_modelæ˜¯ä¼šåœ¨taskä¸­ä»¥classmethodæ–¹æ³•ç›´æ¥è°ƒç”¨çš„ï¼Œå¯è§†ä¸ºç‹¬ç«‹äºç±»å¤–çš„å‡½æ•°ã€‚

âš ï¸`__init__`å‡½æ•°ä¸­superå¿…æœ‰ï¼Œç„¶åæŒ‡å®šforwardä¸­ä¼šç”¨åˆ°çš„ä¸œè¥¿å°±å¯ä»¥ã€‚

âš ï¸forwardçš„è¾“å…¥è¾“å‡ºæ˜¯ç”±taskå†³å®šçš„ï¼Œå…·ä½“æ¥è®²æ˜¯æ¯ä¸ªminibatchä¸­çš„`next_input`ã€‚

## Register Architecture

å°†æˆ‘ä»¬çš„æ¨¡å‹èµ·ä¸€ä¸ªarchitectureçš„åå­—ï¼Œå¹¶æ³¨å†Œï¼Œè¿™æ ·å°±å¯ä»¥åœ¨å‘½ä»¤è¡Œé€‰æ‹©`â€”arch`å‚æ•°äº†ã€‚

```python
from fairseq.models import register_model_architecture

@register_model_architecture('simple_lstm', 'tutorial_simple_lstm')
def tutorial_simple_lstm(args):	# ä¸‹é¢è®¾ç½®äº†é»˜è®¤å‚æ•°ï¼Œå¦‚æœcommandlineæ²¡æŒ‡å®šå°±ç”¨
    args.encoder_embed_dim = getattr(args, 'encoder_embed_dim', 256)
    args.encoder_hidden_dim = getattr(args, 'encoder_hidden_dim', 256)
    args.decoder_embed_dim = getattr(args, 'decoder_embed_dim', 256)
    args.decoder_hidden_dim = getattr(args, 'decoder_hidden_dim', 256)
```

âš ï¸Architectureå°±æ˜¯å¸¦æœ‰ä¸€ç»„å›ºå®šå‚æ•°çš„modelï¼Œæ˜¯**precise network configuration**ã€‚

## Register Task

å®˜æ–¹è®²çš„æ˜¯ï¼š

> A new FairseqTask will load our dictionaries and dataset. Tasks can also control how the data is batched into mini-batches.

å³taskå†³å®šäº†ä¸€ä¸ªä»»åŠ¡çš„æ•°æ®/è¯å…¸è½½å…¥æ–¹æ³•ï¼Œä»¥åŠå¦‚ä½•å°†æ•°æ®åŒ…è£…ä¸ºminibatchçš„å½¢å¼ã€‚å¦‚æœæœ‰æ–°çš„ä»»åŠ¡ï¼Œä¾‹å¦‚åå­—åˆ†ç±»ç­‰ä¸æ˜¯seq2seq/lmçš„ä»»åŠ¡ï¼Œè¦æ³¨å†Œæ–°çš„taskã€‚

```python
import os
import torch

from fairseq.data import Dictionary, LanguagePairDataset	# è¿™é‡Œä¼šç”¨åˆ°è¯­è¨€æ¨¡å‹çš„dataset
from fairseq.tasks import FairseqTask, register_task


@register_task('simple_classification')	# ä¸æ³¨å†Œmodelä¸€æ ·ä¹Ÿè¦ç´§é ç€
class SimpleClassificationTask(FairseqTask):

    @staticmethod
    def add_args(parser):	# æ·»åŠ ä¸dataç›¸å…³çš„å‚æ•°ï¼Œæœ€å¤§é•¿åº¦ç­‰
        parser.add_argument('data', metavar='FILE',
                            help='file prefix for data')
        parser.add_argument('--max-positions', default=1024, type=int,
                            help='max input length')

    @classmethod
    def setup_task(cls, args, **kwargs):
      	# è¿™é‡Œçš„ä½œç”¨ä¸modelä¸­çš„build_modelç±»ä¼¼ï¼Œè¿”å›ç±»æœ¬èº«ï¼Œå¯è§†ä¸ºç›¸å¯¹ç‹¬ç«‹çš„æ„é€ 
        input_vocab = Dictionary.load(os.path.join(args.data, 'dict.input.txt'))
        label_vocab = Dictionary.load(os.path.join(args.data, 'dict.label.txt'))
        print('| [input] dictionary: {} types'.format(len(input_vocab)))
        print('| [label] dictionary: {} types'.format(len(label_vocab)))

        return SimpleClassificationTask(args, input_vocab, label_vocab)
		
    #	è¿™æ˜¯çœŸæ­£çš„æ„é€ å‡½æ•°ï¼Œå³setup_taskçš„è¿”å›å€¼ä¼šè°ƒç”¨çš„å‡½æ•°ï¼Œå…¶ä¸­å®šä¹‰ç±»ä¸­å…¶ä»–å‡½æ•°ä¼šç”¨çš„selfå‚æ•°
    def __init__(self, args, input_vocab, label_vocab):
        super().__init__(args)
        self.input_vocab = input_vocab
        self.label_vocab = label_vocab
		
    #	load train/valid/testé›†åˆï¼Œä¼šwarpåœ¨ä¸€ä¸ªforå¾ªç¯ä¸­ï¼Œsplitåˆ†åˆ«æ˜¯t/v/t
    def load_dataset(self, split, **kwargs):

        prefix = os.path.join(self.args.data, '{}.input-label'.format(split))

        # Read input sentencesï¼Œè¿”å›ä¸€ä¸ªåä¸ºsentencesçš„listä¿å­˜å„ä¸ªå¥å­
        sentences, lengths = [], []
        with open(prefix + '.input', encoding='utf-8') as file:
            for line in file:
                sentence = line.strip()

                # Tokenize the sentence, splitting on spaces
                tokens = self.input_vocab.encode_line(
                    sentence, add_if_not_exist=False,
                )

                sentences.append(tokens)
                lengths.append(tokens.numel())

        # Read labelsï¼Œä¹Ÿæ˜¯ç”¨listä¿å­˜
        labels = []
        with open(prefix + '.label', encoding='utf-8') as file:
            for line in file:
                label = line.strip()
                labels.append(
                    # Convert label to a numeric ID.
                    torch.LongTensor([self.label_vocab.add_symbol(label)])
                )

        assert len(sentences) == len(labels)
        print('| {} {} {} examples'.format(self.args.data, split, len(sentences)))

        # We reuse LanguagePairDataset since classification can be modeled as a
        # sequence-to-sequence task where the target sequence has length 1.
        # å°†ä¸Šé¢çš„source/targetä¸¤ä¸ªlistè¾“å…¥è¿™ä¸ªdatasetç±»ä¸­
        self.datasets[split] = LanguagePairDataset(
            src=sentences,
            src_sizes=lengths,
            src_dict=self.input_vocab,
            tgt=labels,
            tgt_sizes=torch.ones(len(labels)),  # targets have length 1
            tgt_dict=self.label_vocab,
            left_pad_source=False,
            max_source_positions=self.args.max_positions,
            max_target_positions=1,
            # Since our target is a single class label, there's no need for
            # input feeding. If we set this to ``True`` then our Model's
            # ``forward()`` method would receive an additional argument called
            # *prev_output_tokens* that would contain a shifted version of the
            # target sequence.
            input_feeding=False,
        )
        
    def max_positions(self):
        # ç›´æ¥è¿”å›ä¸¤ä¸ªæ•°å­—ï¼Œç¬¬ä¸€ä¸ªæ˜¯sourceä¸­æœ€å¤§é•¿åº¦ï¼Œç¬¬äºŒä¸ªæ˜¯labelæœ€å¤§é•¿åº¦
        return (self.args.max_positions, 1)

    @property
    def source_dictionary(self):
        """Return the source :class:`~fairseq.data.Dictionary`."""
        return self.input_vocab

    @property
    def target_dictionary(self):
        """Return the target :class:`~fairseq.data.Dictionary`."""
        return self.label_vocab
```

å¯ä»¥çœ‹åˆ°taskä¸­è§„å®šäº†å¦‚ä½•ä»æºæ–‡ä»¶è¯»å–æ•°æ®/å­—å…¸ï¼ˆæ³¨æ„å­—å…¸æ˜¯preprocessæ—¶å»ºç«‹å¥½çš„ï¼‰ï¼Œå¹¶é€šè¿‡è¯»å–å¥½çš„listå»ºç«‹datasetç±»ï¼Œç„¶åget_batch_iteratorå‡½æ•°ä»datasetä¸­è·å¾—ä¸€ä¸ªiteratoræ¥è¯»å–æ•°æ®ã€‚

## Train

ç°åœ¨å¯ä»¥ç”¨`fairseq-train`çš„æ–¹æ³•åœ¨å‘½ä»¤è¡Œé‡Œè®­ç»ƒäº†ã€‚

```bash
fairseq-train data-bin/iwslt14.tokenized.de-en \
  --arch tutorial_simple_lstm \
  --encoder-dropout 0.2 --decoder-dropout 0.2 \
  --optimizer adam --lr 0.005 --lr-shrink 0.5 \
  --max-tokens 12000
```

æŒ‡å®šarchå’Œtaskå°±å¯ä»¥äº†ï¼Œè¿™é‡Œtaské»˜è®¤æ˜¯translationï¼Œå³æŒ‰ç…§ç¿»è¯‘ä»»åŠ¡æ¥è¯»å–æ•°æ®ç­‰ã€‚

# æ¡†æ¶

1âƒ£ï¸åˆå§‹åŒ–GPUå‚æ•°

2âƒ£ï¸task.setup_taskï¼Œå¦‚translation_taskï¼ŒLM taskç­‰ç­‰ã€‚å…¶ä»»åŠ¡æ˜¯æ‰“å¼€æ•°æ®é›†ï¼ŒåŠ è½½è¯è¡¨ï¼Œå¹¶æ‰“å°åˆ°å±å¹•ã€‚

3âƒ£ï¸valid_dataset

4âƒ£ï¸å»ºæ¨¡modelå’Œcriterionï¼Œç›´æ¥è°ƒç”¨task.build_modelï¼Œè¿”å›æˆ‘ä»¬åˆšåˆšå†™çš„build_modelçš„æ¨¡å‹ã€‚build_criterionä¹Ÿæ˜¯ä¸€æ ·çš„ã€‚

5âƒ£ï¸æ‰“å°å‡ºæ¨¡å‹å„ç§ä¿¡æ¯ï¼ˆæ¨¡å‹åå­—è¾“å‡ºçš„æ˜¯archçš„åå­—ï¼‰ã€‚

6âƒ£ï¸buildä¸€ä¸ªtrainerï¼Œè¾“å‡ºäº†ä¸€äº›GPUä¿¡æ¯ã€‚

7âƒ£ï¸å¦‚æœæœ‰é¢„è®­ç»ƒçš„æ¨¡å‹çš„è¯ï¼ŒåŠ è½½è¿›æ¥ã€‚

8âƒ£ï¸åˆå§‹åŒ–å¾ˆå¤šç›¸å…³å‚æ•°ï¼Œmax_epochç­‰ï¼Œé»˜è®¤æœ€å¤§epochæ˜¯æ— ç©·.

9âƒ£ï¸å¼€å§‹è®­ç»ƒï¼æ•´ä¸ªwhileå¾ªç¯çš„æ¡ä»¶æ˜¯lrä¸èƒ½å¤ªå°/epochä¸èƒ½å¤ªå¤§/æ›´æ–°æ¬¡æ•°ä¸èƒ½å¤ªå¤šï¼ˆæ­¥æ•°ï¼‰ã€‚æ¥ä¸‹æ¥å¼€å§‹ä¸€ä¸ªepochçš„å¾ªç¯ï¼Œéƒ½å°è£…åœ¨trainé‡Œé¢äº†ã€‚å…³äºè®­ç»ƒï¼Œtrainå‡½æ•°è°ƒç”¨trainer.train_stepï¼Œè€Œtrainer.train_stepä¸­ä¼šè°ƒç”¨task.train_stepï¼Œè€Œtask.train_stepä¸­æŠŠmodelå’Œsampleé€å…¥fairseq.criterionæ±‚lossã€‚

ğŸ”Ÿè®­ç»ƒä¸€ä¸ªepochï¼ˆtrainå‡½æ•°å®ç°è®­ç»ƒä¸€ä¸ªepochï¼‰ï¼Œæ˜¯validçš„è¯validä¸€ä¸‹ï¼Œæ›´æ–°lrï¼Œä¿å­˜checkpointã€‚æœ€ååœä¸‹æ¥ã€‚



âš ï¸æ‰€æœ‰taskç»§æ‰¿Fairseq_taskå¹¶å®ç°å…¶ä¸­çš„æ¥å£ã€‚



# Autoencoder

é¦–å…ˆï¼Œå¥å­çº§åˆ«çš„autoencoderä¹Ÿæ˜¯ä¸€ä¸ªseq2seqæ¨¡å‹ï¼Œåªä¸è¿‡è¾“å…¥è¾“å‡ºæ˜¯ç›¸åŒçš„ã€‚

æˆ‘ä»¬è¯´AEçš„decoderæ˜¯ä¸€ä¸ªCLMï¼Œé‚£æ‰€æœ‰çš„seq2seqçš„decoderå…¶å®éƒ½æ˜¯CLMï¼Œæ‰€ä»¥æ˜¯åºŸè¯ã€‚