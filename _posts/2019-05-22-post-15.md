---

layout: post
title: Fairseq Tutorial
date: 2019-05-22 10:28:09
tags: [NLP]
categories: [NLP]
typora-root-url: ../../static
---

A reading notes of fairseq [document](https://fairseq.readthedocs.io/en/latest/) and [github page](https://github.com/pytorch/fairseq). 

# Overview

## Command-Line

流程分为以下几步：

1. `fairseq-preprocess`来预处理数据，用来建立词表，并生成训练使用的二进制数据。
2. `fairseq-train`来训练模型，默认使用全部GPU，使用`CUDA_VISIBLE_DEVICES`来调整程序可见的GPU数目，⚠️注意batch_size的设定是根据tokens来的，参数`max-tokens`用于指定batch中最大token数。
3. 训练完后，用`fairseq-generate`在二进制数据上进行测试，raw text则需要使用`fairseq-interactive`。其中继承了beam-search的方法。

## Extend

除了使用现成的模型和任务之外，还可以自己扩展五种类型：

- Models用于定义模型的结构，包括所有需要训练的参数。
- Criterion来定义新的loss计算方法，根据target和output来算。
- ⚠️Tasks用于保存词表，提供dataset的读取和迭代方法，初始化Model和Criterion并计算Loss。像个框架。
- Optimizer指定如何更新模型中的参数。
- Learning Rate Schedulers定义了训练过程中学习率的变化。

整个的高层工作流程为：

```python
for epoch in range(num_epochs):
    itr = task.get_batch_iterator(task.dataset('train'))
    for num_updates, batch in enumerate(itr):
        task.train_step(batch, model, criterion, optimizer) # 包括了backward()
        average_and_clip_gradients()
        optimizer.step()
        lr_scheduler.step_update(num_updates)
    lr_scheduler.step(epoch)
```

## Register

新模块只有经过注册以后才能在命令行中使用，方法如下：

```python
@register_model('my_lstm')
class MyLSTM(FairseqModel):
    (...)
```

自己写的模块也可以保存在自己的路径下面（而非框架默认的fairseq/model下），这种时候在命令行中就要使用`--user-dir`参数指定custom文件夹，

# 一个例子

做一个LSTM encoder-decoder模型。

## Encoder/Decoder

这两部分继承`FairseqEncoder/Decoder`，基类本身也是继承自`nn.Module`，只是普通的两个PyTorch模块。

注意到在init中这俩模块都传入了dictionary参数，并`super().__init__(dictionary)`了一下。是为了获取词表长度，pad字符等信息。

## Register the Model

通过`@register_model('new_model')`的装饰方法注册新模型，只有register的model才可以用命令行直接调用。

所有Model都必须实现`BasicFairseqModel`的所有接口，对于Seq2seq任务我们可以继承`FairseqModel`，因为它继承了Basic并默认了encoder-decoder结构。

```python
from fairseq.models import FairseqModel, register_model

@register_model('simple_lstm')	# 必须在Model类前加装饰
class SimpleLSTMModel(FairseqModel):
  
    @staticmethod
    def add_args(parser):	# 重写这个Basic类中的接口，添加命令行参数
        parser.add_argument(
            '--encoder-embed-dim', type=int, metavar='N',
            help='dimensionality of the encoder embeddings',
        )
        
    @classmethod
    def build_model(cls, args, task):	# 用于初始化模型，是classmethod
        encoder = SimpleLSTMEncoder()	# 省略参数
        decoder = SimpleLSTMDecoder()
        model = SimpleLSTMModel(encoder, decoder)	# 这里用了SimpleLSTMModel类本身
        
        print(model)
        return model	# 必须返回模型，model在build_model中定义
      
    # 可重写初始化，这里FairseqModel默认的encoder-decoder结构
    def __init__(self, encoder, decoder):
        super().__init__()

        self.encoder = encoder
        self.decoder = decoder
        assert isinstance(self.encoder, FairseqEncoder)
        assert isinstance(self.decoder, FairseqDecoder)
      
    # 可重写forward方法来决定encoder和decoder交互细节，下面是FairseqModel中default的做法
    def forward(self, src_tokens, src_lengths, prev_output_tokens):
        encoder_out = self.encoder(src_tokens, src_lengths)
        decoder_out = self.decoder(prev_output_tokens, encoder_out)
        return decoder_out
```

⚠️这里的build_model是会在task中以classmethod方法直接调用的，可视为独立于类外的函数。

⚠️`__init__`函数中super必有，然后指定forward中会用到的东西就可以。

⚠️forward的输入输出是由task决定的，具体来讲是每个minibatch中的`next_input`。

## Register Architecture

将我们的模型起一个architecture的名字，并注册，这样就可以在命令行选择`—arch`参数了。

```python
from fairseq.models import register_model_architecture

@register_model_architecture('simple_lstm', 'tutorial_simple_lstm')
def tutorial_simple_lstm(args):	# 下面设置了默认参数，如果commandline没指定就用
    args.encoder_embed_dim = getattr(args, 'encoder_embed_dim', 256)
    args.encoder_hidden_dim = getattr(args, 'encoder_hidden_dim', 256)
    args.decoder_embed_dim = getattr(args, 'decoder_embed_dim', 256)
    args.decoder_hidden_dim = getattr(args, 'decoder_hidden_dim', 256)
```

⚠️Architecture就是带有一组固定参数的model，是**precise network configuration**。

## Register Task

官方讲的是：

> A new FairseqTask will load our dictionaries and dataset. Tasks can also control how the data is batched into mini-batches.

即task决定了一个任务的数据/词典载入方法，以及如何将数据包装为minibatch的形式。如果有新的任务，例如名字分类等不是seq2seq/lm的任务，要注册新的task。

```python
import os
import torch

from fairseq.data import Dictionary, LanguagePairDataset	# 这里会用到语言模型的dataset
from fairseq.tasks import FairseqTask, register_task


@register_task('simple_classification')	# 与注册model一样也要紧靠着
class SimpleClassificationTask(FairseqTask):

    @staticmethod
    def add_args(parser):	# 添加与data相关的参数，最大长度等
        parser.add_argument('data', metavar='FILE',
                            help='file prefix for data')
        parser.add_argument('--max-positions', default=1024, type=int,
                            help='max input length')

    @classmethod
    def setup_task(cls, args, **kwargs):
      	# 这里的作用与model中的build_model类似，返回类本身，可视为相对独立的构造
        input_vocab = Dictionary.load(os.path.join(args.data, 'dict.input.txt'))
        label_vocab = Dictionary.load(os.path.join(args.data, 'dict.label.txt'))
        print('| [input] dictionary: {} types'.format(len(input_vocab)))
        print('| [label] dictionary: {} types'.format(len(label_vocab)))

        return SimpleClassificationTask(args, input_vocab, label_vocab)
		
    #	这是真正的构造函数，即setup_task的返回值会调用的函数，其中定义类中其他函数会用的self参数
    def __init__(self, args, input_vocab, label_vocab):
        super().__init__(args)
        self.input_vocab = input_vocab
        self.label_vocab = label_vocab
		
    #	load train/valid/test集合，会warp在一个for循环中，split分别是t/v/t
    def load_dataset(self, split, **kwargs):

        prefix = os.path.join(self.args.data, '{}.input-label'.format(split))

        # Read input sentences，返回一个名为sentences的list保存各个句子
        sentences, lengths = [], []
        with open(prefix + '.input', encoding='utf-8') as file:
            for line in file:
                sentence = line.strip()

                # Tokenize the sentence, splitting on spaces
                tokens = self.input_vocab.encode_line(
                    sentence, add_if_not_exist=False,
                )

                sentences.append(tokens)
                lengths.append(tokens.numel())

        # Read labels，也是用list保存
        labels = []
        with open(prefix + '.label', encoding='utf-8') as file:
            for line in file:
                label = line.strip()
                labels.append(
                    # Convert label to a numeric ID.
                    torch.LongTensor([self.label_vocab.add_symbol(label)])
                )

        assert len(sentences) == len(labels)
        print('| {} {} {} examples'.format(self.args.data, split, len(sentences)))

        # We reuse LanguagePairDataset since classification can be modeled as a
        # sequence-to-sequence task where the target sequence has length 1.
        # 将上面的source/target两个list输入这个dataset类中
        self.datasets[split] = LanguagePairDataset(
            src=sentences,
            src_sizes=lengths,
            src_dict=self.input_vocab,
            tgt=labels,
            tgt_sizes=torch.ones(len(labels)),  # targets have length 1
            tgt_dict=self.label_vocab,
            left_pad_source=False,
            max_source_positions=self.args.max_positions,
            max_target_positions=1,
            # Since our target is a single class label, there's no need for
            # input feeding. If we set this to ``True`` then our Model's
            # ``forward()`` method would receive an additional argument called
            # *prev_output_tokens* that would contain a shifted version of the
            # target sequence.
            input_feeding=False,
        )
        
    def max_positions(self):
        # 直接返回两个数字，第一个是source中最大长度，第二个是label最大长度
        return (self.args.max_positions, 1)

    @property
    def source_dictionary(self):
        """Return the source :class:`~fairseq.data.Dictionary`."""
        return self.input_vocab

    @property
    def target_dictionary(self):
        """Return the target :class:`~fairseq.data.Dictionary`."""
        return self.label_vocab
```

可以看到task中规定了如何从源文件读取数据/字典（注意字典是preprocess时建立好的），并通过读取好的list建立dataset类，然后get_batch_iterator函数从dataset中获得一个iterator来读取数据。

## Train

现在可以用`fairseq-train`的方法在命令行里训练了。

```bash
fairseq-train data-bin/iwslt14.tokenized.de-en \
  --arch tutorial_simple_lstm \
  --encoder-dropout 0.2 --decoder-dropout 0.2 \
  --optimizer adam --lr 0.005 --lr-shrink 0.5 \
  --max-tokens 12000
```

指定arch和task就可以了，这里task默认是translation，即按照翻译任务来读取数据等。

# 框架

1⃣️初始化GPU参数

2⃣️task.setup_task，如translation_task，LM task等等。其任务是打开数据集，加载词表，并打印到屏幕。

3⃣️valid_dataset

4⃣️建模model和criterion，直接调用task.build_model，返回我们刚刚写的build_model的模型。build_criterion也是一样的。

5⃣️打印出模型各种信息（模型名字输出的是arch的名字）。

6⃣️build一个trainer，输出了一些GPU信息。

7⃣️如果有预训练的模型的话，加载进来。

8⃣️初始化很多相关参数，max_epoch等，默认最大epoch是无穷.

9⃣️开始训练！整个while循环的条件是lr不能太小/epoch不能太大/更新次数不能太多（步数）。接下来开始一个epoch的循环，都封装在train里面了。关于训练，train函数调用trainer.train_step，而trainer.train_step中会调用task.train_step，而task.train_step中把model和sample送入fairseq.criterion求loss。

🔟训练一个epoch（train函数实现训练一个epoch），是valid的话valid一下，更新lr，保存checkpoint。最后停下来。



⚠️所有task继承Fairseq_task并实现其中的接口。



# Autoencoder

首先，句子级别的autoencoder也是一个seq2seq模型，只不过输入输出是相同的。

我们说AE的decoder是一个CLM，那所有的seq2seq的decoder其实都是CLM，所以是废话。