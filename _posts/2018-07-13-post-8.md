---
layout: post
title: Logistic Regression Bag-of-Words Classifier 词袋模型
date: 2018-07-13 09:28:09
tags: [NLP]
categories: [NLP]

---
最近读PyTorch文档，写得很精彩。里面NLP应用章节介绍到一个简单的词袋（Bag-of-Words）模型，记录一下。

# 概念

词袋已经是十几年前的概念了，每次学到这种古老的东西就有种学也不是，绕也绕不过去的感觉……

词袋的核心思想是单纯统计文本中每个词出现的频率，而与其出现的顺序无关。就像把一篇文章所有的词倒入一个袋子中，只在乎每个词出现几次，而不在乎其出现的位置。利用生成的词袋向量来表征这篇文章，常用于分类任务。目前词袋模型也推广到图片分类任务中了，将图片的特征看作词，获得图片的词袋向量。

例如我们的词表内只有两个单词"hello"和"world"，分别位于位置1和0，那句子"hello world"的词袋向量（BoW Vector）就是[1, 1]，"hello hello world"就是[2, 1]。

将这个BoW向量作为输入，经过softmax层，就可以得到属于各个类别的概率。

# 实现

## 从语料库构建词袋

构建词到向量的映射，首先构建词袋。

```python
data = [("me gusta comer en la cafeteria".split(), "SPANISH"),
        ("Give it to me".split(), "ENGLISH"),
        ("No creo que sea una buena idea".split(), "SPANISH"),
        ("No it is not a good idea to get lost at sea".split(), "ENGLISH")]

test_data = [("Yo creo que si".split(), "SPANISH"),
             ("it is lost on me".split(), "ENGLISH")]

word_to_ix = {}
for sent, _ in data + test_data:
	for word in sent:
		if word not in word_to_ix:
			word_to_ix[word] = len(word_to_ix)
print(word_to_ix)

VOCAB_SIZE = len(word_to_ix)
NUM_LABELS = 2
```

1. 观察数据结构，是以tuple为元素的list。
2. `string.split()`函数以参数为分隔符切割字符串，返回一个多string元素的list。
3. for循环中可以直接用`+`连接两个list。
4. 构建词表部分，注意dict的key和value，key是索引，这里以词为索引，序号为值，所以名为word_to_ix。
5. 注意从0开始算，len(dict)是下一个元素序号。
6. 词袋构建完成后，VOCAB_SIZE表示总的词袋大小，NUM_LABELS表示分类标签，这里为不同的语言。

## 定义网络

这里用一层FC+softmax直接分类。

```python
class BoWClassifier(nn.Module):
	def __init__(self, vocab_size, num_labels):
		super(BoWClassifier, self).__init__()
		self.linear = nn.Linear(vocab_size, num_labels)

	def forward(self, bow_vec):
		return F.log_softmax(self.linear(bow_vec), dim=1)
```

分类问题在FC之后，每行为一个样本的输出，所以log_softmax的dim设置为1，表示按行来归一化并求log。注意softmax之后每个元素的范围(0, 1)，再取log都是负的，越接近于0表示概率越大。

使用`log_softmax`的原因是普通softmax的输出值域仅在(0, 1)内，经过对数运算可以提高精度。

## 对输入生成BoW向量

注意逻辑，上面先利用所有的语料来生成词袋，这里针对每一句输入（不管是训练集还是测试集中的句子），生成一个属于它们自己的词袋向量。

```python
def sent_to_vec(sentence, word_to_ix):
	vector = t.zeros(VOCAB_SIZE)
	for word in sentence:
		vector[word_to_ix(word)] += 1
	return vector.view(1, -1)
```

1. 注意初始化时Tensor要初始化为全0向量。
2. BoW Vector的意义，统计每个词出现的频率，而与顺序无关。
3. PyTorch中一个样本一般用一行来表示。

## 标签向量化

不同的标签用不同的向量表示。
```python
label_to_ix = {"SPANISH": 0, "ENGLISH": 1}

def make_target(label, label_to_ix):
	return t.LongTensor([label_to_ix[label]])
```

1. 标签要用LongTensor类型。
2. 注意这里生成LongTensor用的是一个实际的List，最外层有中括号的。

## 训练过程

```python
loss_function = nn.NLLLoss()
optimizer = optim.SGD(model.parameters(), lr=0.1)
for epoch in range(100):
	for instance, label in data:
		mode.zero_grad()
		bow_vec = sent_to_vec(instance, word_to_ix)
		target = make_target(label, label_to_ix)
		log_probs = model(bow_vec)
		loss = loss_function(log_probs, target)
		loss.backward()
		optimizer.step()
```

1. `nn.NLLLoss()`是Negative Log Likelihood Loss，与`nn.CrossEntropyLoss()`的区别是，后者继承了**LogSoftmax + NLLLoss**，而前者需要在网络的最后一层使用log_softmax函数。即**CrossEntropy = LogSoftmax + NLLLoss**。
2. 新的epoch开始时要梯度清零`model.zero_grad()`，因为是是累积的。

## 测试过程

```python
with t.no_grad():
	for instance, label in test_data:
		bow_vec = sent_to_vec(instance, word_to_ix)
		log_probs = model(bow_vec)
		print(log_probs)
```

测试过程必须用`torch.no_grad()`来wrap起来，否则将在计算图中引入不必要的麻烦。