---
layout: post
title: TorchText tutorial
date: 2019-05-08 09:28:09
tags: [PyTorch]
categories: [PyTorch]
typora-root-url: ../../static
---

参考这两篇博客[1](http://mlexplained.com/2018/02/15/language-modeling-tutorial-in-torchtext-practical-torchtext-part-2/)和[2](http://mlexplained.com/2018/02/08/a-comprehensive-tutorial-to-torchtext/)，整理了torchtext的概念和用法。是一个不错的文本预处理工具。

# Torchtext

## spacy

使用流程：先`spacy.load(en)`作为模型，之后用模型处理一段string文本，其中每个元素为token。

```python
python -m spacy download en	# first download English model

spacy_en = spacy.load('en')
doc = 'hi i''m danny'	# 必须是string类型
for tok in spacy_en(doc):	# 这里与spacy_en.tokenizer(doc)相同
  print(tok.text)
```

注意tok为`spacy.Token`类型，调用其`.text`方法可以得到string类型。

添加自定义分词的方法：

```python
my_tok = spacy.load('en')
my_tok.tokenizer.add_special_case("don't", [{ORTH: "do"}, {ORTH: "n't"}])
```

## Translation Task

对于翻译任务而言：

### 1. 构造Field变量

顾名思义就是语料中有几个**域**，不同field对应着不同的**预处理方法**(例如tokenizer等)。像翻译任务有source和target两个field，有如下参数：

- tokenize: 是一个string->list的函数，默认为string.split，一般使用spacy构造一个函数。
- sequential: 该field是不是序列数据，默认为True，若False则不应用tokenize。
- fix_length: 是否将后面**Iterator取出的**所有样本padding到同样的固定长度，**否则按照该batch最长值填充**。
- batch_first: 决定Iterator取出的样本第一个维度是不是batch，**为假的话batch在第二个维度**。
- lower: 样本全部变成小写。 
- Init_token/eos_token/pad_token/unk_token: default None/None/<pad>/<unk>，**在查看dataset时不会出现，在Iterator取出后会添加，且eos之后再pad。**
- use_vocab: vocab->number，如果该field都是数字类型，那么设置为false。

### 2. 构建dataset

#### 从文件中读取

分为从文件中读取dataset和使用pre-build dataset两种方法。

- data.TabularDataset(.splits)(): 从结构化数据(csv等)中读取，**若只有一个文件那就不用.splits，参数不变，这个splits方法是同时处理train/vld/test并行读取，*并不是将一个文件分割成几份***。只有这种方法是在data.下面的，其他的封装好的dataset类型都在datasets下。
- datasets.TranslationDataset(path, exts, fields): path是文件公共前缀，exts是一个tuple其中包括两种语言不同的后缀名字，path+exts是完整的文件目录即可。fields也是tuple包括不同的data.Field类型。

其他的例如LanguageModelingDataset构造方法都是类似，注意⚠️这里***返回的都是data.Dataset类型变量***。它有`data.Dataset.split()`方法来划分train/(vld/)test集，参数可以是一个ratio表示train的比例，此时返回两个dataset，也可以是list of relative number，三个数字则返回三个dateset。

我们可以查看Dataset变量内部的情况，`dataset.__dict__.keys()`发现dataset类有两个属性，example和fields，可以使用`dataset.example/fields`的方法查看它们。Example即有多少样本，fields为每个样本中有几个域，翻译任务中有src和trg两个域。

可以用`len(dataset)`的形式查看dataset的长度。对于translation dataset，`dataset[0]`返回一个Example类型，每个Example实例含有刚才fields中查看的属性，`dataset[0].__dict__`可以查看语对的字典，形式如下：

```python
{'src': ['nice', 'to', 'meet', 'you'],
 'trg': ['很', '高兴', '遇到', '你']}
```

每个样例是一个key为`src/trg`字典的形式，其值为tokenize之后的list。虽然dataset[0]本身是一个Example类型，但也可以**通过dataset[0].src/trg**直接查看这个list。***注意这里的数据还都是raw text，没有数字化*。**

#### 内置知名数据库

torchtext.datasets中pre-build很多广泛使用的数据库，例如想用IWSLT翻译数据，直接调用：

```python
train, val, test = dataset.IWSLT.splits(exts=('.de', '.en'), fields=(SRC, TGT),
  filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and len(vars(x)['trg']) <= MAX_LEN)
```

这里`filter_pred`参数是一个***对Example的过滤器***，只用后面成立的Example，这里是长度过滤。

⚠️`vars`和`.__dict__`方法是一样的，返回该类属性和属性值组成的dict，通过`.keys()`查看有哪些属性。

#### 总结

Dataset变量从文件中读取数据，并存储为一对一对的list of raw text。

### 3. 建立词表

为Iterator将raw text变为tensor的必要步骤：
```python
field.build_vocab(dataset, vector, min_freq, max_size)
```
参数为遍历哪一个dataset建立词典，vector可以指定使用预训练好的词向量，例如`"glove.6B.100d"`会下载并使用100维的glove向量，***但是这个向量不会在Iterator取样本时自动使用，而是要显示的复制到embedding层中。***

```python
weight_matrix = TEXT.vocab.vectors	#TEXT is a Field variable
embedding = nn.Embedding(weight_matrix.size(0), weight_matrix.size(1))	#维度根据下载的来
embedding.weight.data.copy_(weight_matrix)	#拷贝参数
```

之后将其视为正常Embedding层使用。

返回词表大小的方法：`len(field.vocab)`。

### 4. 建立迭代器

同样分为data.Iterator和data.Iterator.splits两种方法，带splits的要以tuple的形式传入(train, val, test)三个dataset然后返回三个迭代器。

- dataset: 不用splits方法时候传一个dataset，用splits方法传tuple。
- batch_size: 即batch_size。
- device: 指定GPU编号，-1是CPU。

之后通过迭代可返回一个Batch类型的实例，这个类似于Example是torchtext定义的。通过`.__dict__.keys()`方法可以查看其关键字，其中`.src`和`.trg`即我们需要的源和目标语句。

这两者(src/trg)的维度都是(batch_size, length)，**⚠️注意batch_size在第一个维度还是第二个维度取决于Field中的定义**，而**length在默认情况下是该batch中的max_length，每个batch会有所不同，如果在Field中指定fix_length则为固定长度。**且**这一步并不会有Embedding维，即使在vocab中指定了使用pre-trained embedding。**

其中数据都是`torch.LongTensor`类型，是每个词在词表中的index。

## Language Model

与翻译不同，语言模型往往只使用一个输入文件，通过位移的方法确定源句和输出，即预测下一词。总体的框架与翻译是相同的，只说一下不同之处，以WikiText为例。

### 建立dataset

语言模型dataset的建立方法比较简单：

```python
train, valid, test = torchtext.datasets.WikiText2.splits(TEXT)
```

注意使用预置的语料建立train/valid/test往往只需要用splits方法➕Field就可以了，在上面翻译例子中由于多文件多了指定扩展名和一个过滤器。

语言模型的dataset比较特殊，如果用`len(train)`查看长度会发现只有1，即其中只有1个Example，且只有一个名为`text`的Field。即**所有数据都被存在train[0].text中。**

### field.build_vocab()

### BPTTIterator

全名BackPropagation Through Time，专门针对于语言模型使用的迭代器。

```python
train_iter, valid_iter, test_iter = data.BPTTIterator.splits(
    (train, valid, test),
    batch_size=32,
    bptt_len=30, # this is where we specify the sequence length
    device=torch.device('cuda:4'),
    repeat=False)
```

使用splits方法针对三个dataset并行操作，其中的参数`bptt_len`为每个batch中取多少个词。

```python
test = next(iter(train_iter))
test.__dict__.keys()
```

输出`dict_keys(['batch_size', 'dataset', 'fields', 'text', 'target'])`，这其中text就是输入，target即text位移一个位置后的目标。

## Inference

- Filed.vocab.stoi[string]: 在Field build vocab之后，可以用该方法返回string对应的index，不在词表范围之内的返回<unk>的编号。
- Filed.vocab.itos[index]: 在Field build vocab之后，可以用该方法返回index对应的string。Index可以是数字或tensor，一次只能取一个。

**⚠️注意stoi和itos实质上是两个list，按照编号取对应元素！**

## batch & Batch & example

- 对dataset进行for的循环迭代，每次输出一个example对象。dataset[0]直接返回example对象。每个example的属性可以通过dataset.fields查看，例如翻译中每个example有.src和.trg两个属性。

- torchtext.data.batch(dataset, size)返回一个generator，以list的形式从dataset中读example按size大小返回，**注意这个generator只能被迭代一次**。
- torchtext.data.Batch(data, dataset, device)是Iterator返回的对象类型。Batch跟example一样拥有dataset.fields中的属性，但是其添加了batch维度，这点是和batch最大的区别。***这个类的目的是将传入的data添加batch维度，传入的data是list形式，dataset用于读取相关fields信息。***
- Iterator的逻辑：***传入dataset->调用batch->传入Batch->返回***。