---
layout: post
title: Encoder-Decoder Details
date: 2018-12-20 09:28:09
tags: [RNN]
categories: [RNN]

---


# NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE

Summarize [NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE](https://arxiv.org/abs/1409.0473).

Generally speaking, an encoder reads the input sentence `X`, and convert it to a `variable/fixed-length` vector c.

$$
h _ { t } = f \left( x _ { t } , h _ { t - 1 } \right)
$$

$$
c = q \left( h _ { 1 } , h _ { 2 } , \dots , h _ { T _ { x } } \right)
$$

Most of the previous works used an LSTM as `f()`, and `q()` is the last hidden state in the sequence.

The decoder predicts the next word $y_{t}$ given the context vector `c` and `all the previously predicted words (y1, y2, ..., yt-1)`.

$$
p \left( y _ { t } | y _ { 1 } , \ldots , y _ { t - 1 } , X \right) = g \left( y _ { t - 1 } , s _ { t } , c \right)
$$

where $y _ {t-1}$ is previously predicted word and $s _ {t}$ is the hidden state of RNN.

## encoder

This paper proposed a BiRNN to make the `annotation(hidden state in encoder)` of each word to summarize not only the preceding words, but also the following words.

The annotation for each word $x_{j}$ is a `concatenation` of the forward hidden state and the backward one.

## decoder with attention

$$
p \left( y _ { t } | y _ { 1 } , \ldots , y _ { t - 1 } , X \right) = g \left( y _ { t - 1 } , s _ { t } , c _ { t } \right)
$$

It should be noted that the third parameter is `c_t`, a distinct context vector for time-state of decoding time, instead of `c`. 

$$
c _ { i } = \sum _ { j = 1 } ^ { T _ { x } } \alpha _ { i j } h _ { j }
$$

where $h _ {j}$ is the output of the encoder. The $\alpha_{ij}$ is computed by

$$
\alpha _ { i j } = \operatorname { softmax } \left( \operatorname { score } \left( s _ { i - 1 } , H \right) \right)
$$

where $s_{i-1}$ is the hidden state of decoder, $H$ is the output sequence of encoder. In this paper this `aligned model` is `parameterized as a feedforward neural network`.

In each decoding step, there are 3 inputs to the decoder: $s _ {t-1}$, $y _ {t-1}$(after embedding) and context vector with attention $c_{i}$.

## Reverse the order of source sentences

In conventional `one-direction RNN model`, while the LSTM is capable of solving problems with long term dependencies, it learns much better when the source sentences are reversed(the target sentences are not).

But BiRNN learns the bi-directional information better. So this method is rarely used.

## Concat or plus?

There is no difference.

$$
W _ { a } x + W _ { b } y = \left( W _ { a } , W _ { b } \right) ( x , y ) ^ { T }
$$
