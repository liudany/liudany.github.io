---
layout: post
title: CLassifying Names with a Character-Level RNN 字符级RNN实现姓名分类
date: 2018-09-21 09:28:09
tags: [PyTorch, LSTM]
categories: [PyTorch, LSTM]

---
Tutorials到这里换了一个作者，画风大变。

# 概念

训练一个RNN网络来判断一个名字是属于哪种语言的。

# 数据读取

建立一个dict，以**{language1: [name1, name2, ...], language2...}**的形式保存所有名字与对应的类别。牵扯到一些文件操作。

欧洲一些姓氏中有特殊的字符，为了使问题简单一些，都转化为ASCII形式。

```python
import unicodedata
import string

all_letters = string.ascii_letters + " .,;'"
n_letters = len(all_letters)

# Turn a Unicode string to plain ASCII, thanks to http://stackoverflow.com/a/518232/2809427
def unicodeToAscii(s):
    return ''.join(
        c for c in unicodedata.normalize('NFD', s)
        if unicodedata.category(c) != 'Mn'
        and c in all_letters
    )

print(unicodeToAscii('Ślusàrski'))
```

`unicodedata.normilize()`用来将一个字符串转化为罗马字母+特殊符号的组合list，注意不可以并排输出，否则没变化。后面的`category()`用来除掉特殊符号。

```python
from __future__ import unicode_literals, print_function, division
from io import open
import glob
import os

def findFiles(path): return glob.glob(path)
category_lines = {}
all_categories = []

# Read a file and split into lines
def readLines(filename):
    lines = open(filename, encoding='utf-8').read().strip().split('\n')
    return [unicodeToAscii(line) for line in lines]

for filename in findFiles('data/names/*.txt'):
    category = os.path.splitext(os.path.basename(filename))[0]
    all_categories.append(category)
    lines = readLines(filename)
    category_lines[category] = lines

n_categories = len(all_categories)
```

`open().read().strip().split('\n')`打开文件 -> 读取所有内容 -> 去掉首尾空白符 -> 按换行符划分。

# Name -> Tensor

转化为矩阵形式作为输入。这里对字母用的one-hot编码，每个名字形如[len, 1, n_letters]的矩阵。

```python
import torch

def letterToIndex(letter):
    return all_letters.find(letter)

def letterToTensor(letter):
    tensor = torch.zeros(1, n_letters)
    tensor[0][letterToIndex(letter)] = 1
    return tensor

def lineToTensor(line):
    tensor = torch.zeros(len(line), 1, n_letters)
    for li, letter in enumerate(line):
        tensor[li][0][letterToIndex(letter)] = 1
    return tensor
```

# 定义网络

**对于普通RNN网络的输出是很灵活的，在AK的文章里把输出定义为y=W\*h，即经过tanh为activation的普通FC层输出的隐状态再经过一层FC；在Colah文章里提到，标准RNN把隐状态h=tanh(W\*input+b)直接作为输出，LSTM网络的输出也是每次的隐状态h；而在这里，作者将输出用softmax(W\*[input, hidden])来表示，即经过一个与普通hidden对称（相似）的运算得到另一种意义上的隐状态，再经过softmax。所以看待RNN时，注意力放在hidden_state的运算过程就好了，输出有多种不同的处理方法。**

**在维度上，根据Colah来看，输出就是hidden_size。但是此处的RNN更像是DIY的，所以不一样。**
```python
import torch.nn as nn

class RNN(nn.Module):
	def __init__(self, input_size, hidden_size, output_size):
		super(RNN, self).__init__()
		self.hidden_size = hidden_size

		self.i2h = nn.Linear(input_size+hidden_size, hidden_size)
		self.i2o = nn.Linear(input_size+hidden_size, output_size)
		self.softmax = nn.LogSoftmax(dim=1)

	def forward(self, input, hidden):
		combined = torch.cat((input, hidden), 1)
		hidden = self.i2h(combined)
		output = self.i2o(combined)
		output = self.softmax(output)
		return output, hidden

	def initHidden(self):
		return torch.zeros(1, self.hidden_size)

n_hidden = 128
rnn = RNN(n_letters, n_hidden, n_categories)
```

从不同作者写的代码里可以看出很多不同的想法：
1. 构造函数中都要有hidden_state初始化的函数，这个函数在每个epoch开始时被调用以更新hidden_state。之前的作者用LSTM网络直接返回一个(hidden, cell)的向量。相同之处是都用0初始化即可。
2. 这个作者喜欢将LogSoftmax当作网络的一层来定义，之前的作者喜欢在前向传播中将其作为函数使用。注意网络是首字母大写的形式，而函数是以下划线分隔的。相同之处是都用log_softmax+NLLLoss。
3. 这个RNN类叫做rnn_cell更合适，因为它本身没涉及到循环操作，需要我们在训练过程中自己循环。

这里注意一些切片的方法，一个tensor按照tensor[n]的方式取某个维度后会`降维`，二维会变成一维，而在进行`torch.cat((tensor1, tensor2), dim=n)`时，是沿着维度n来连接，比如n是1，那就是一列一列的往右排，连接起来。这里的背景中，每行作为一个输入，是按照列来拼接的，所以cat的第二个参数是1。

demo中在cat时用的是(input[0], hidden)，注意作者在构建input时是(time_step, batch_size, input_size)这样的形式，而morvan喜欢将batch放在第一个，各有利弊。作者这种写法，可以方便的用input[n]的方式来进行批训练，将每一个batch的第n步降维合成一个tensor，而在batch_size_first的形式中，就要进行input[:, n, :]这样稍微复杂一点的切片操作。

# 辅助函数

## 从Softmax输出中确定类别

`torch.topk(tensor, num)`或者`tensor.topk(num)`返回前num个最大的值和对应的坐标，是两个一维的tensor。

```python
def categoryFromOutput(output):
    top_n, top_i = output.topk(1)
    category_i = top_i[0].item()
    return all_categories[category_i], category_i

print(categoryFromOutput(output))
```

tensor[n]还是一个tensor，而`.item()`将其转化为int，可以作为下面的索引了。整个函数返回类别和类别对应的编号。