---
layout: post
title: Unsupervised NMT几篇文章-Notes
date: 2018-10-26 19:28:09
tags: [论文]
categories: [论文]
typora-root-url: ../../static
---

最近毕业论文要开题了，其实压力有点大的哎。

# Learning bilingual word embeddings with (almost) no bilingual data

## 概述

文章提出了一种self-learning的**框架**来对齐两个词向量的空间，不可以说是完全的无监督，起始过程中需要一定数量的词汇作为种子来启动训练过程。

## framwork

![](/img/self-learning.png)

- D为初始化的种子词对，X和Z分别为source和target的语言空间，W为学习的线性变换。
- 从D中现有的词对学习linear transformation W。
- 由上一步得到的W将X空间中的向量全部映射到Z区间，即XW与Z同在Z空间，之后在Z空间寻找新的词对，扩充D。
- 由扩充的D，重新学习W，重复上面两步。

## Embedding mapping W

$$
W ^ { * } = \operatorname { argmin } _ { W } \sum _ { i } \sum _ { j } D _ { i j } \left\| X _ { i } * W - Z _ { j \downarrow } \right\| ^ { 2 }
$$

最小化上式，其中$D_{ij}$表示X中的第i个向量$X _ {i\*}$与Z中的第j个向量$Z _ {j\*}$是否为对齐的，即是否为对应词。同时有以下操作：
- 对X和Z进行length normalize and mean center的预处理。
- 限制W为一个正交矩阵**（这一步的目的？）**。

这样最小化上式等价于最大化：

$$
W ^ { \backslash ^ { * } } = \operatorname { argmax } _ { W } \operatorname { Tr } \left( X W Z ^ { T } D ^ { T } \right)
$$
该问题的最优正交解由下式给出：

$$
W ^ { \backslash ^ { * } } = U V ^ { T }
$$
这是一个Singular Value Decomposition（奇异值分解）：

$$
X ^ { T } D Z = U \sum V ^ { T }
$$
由于D是一个稀疏的矩阵，所以计算会非常有效率。**（？）**

## Dictionary induction D

Previous work通常使用nearest neighbor retrieval方法，而本文作者使用dot product来衡量两个向量之间的相似性（XW和Z之间），这一步基于前面的length normalize和mean center来保证准确性。

set $D _ {ij}=1$ if $j = argmax _ {k}(X _ {i\*}W)\cdot Z _ {k\*}$ and $D _ {ij}=0$ otherwise.

这一步也可以通过vectorize的方法来减少运算资源。

# Unsupervised Machine Translation

## 概述

这个模型是基于上面一篇的Unsupervised embedding mappings的工作来的，完全使用两种语言各自的单语料库进行训练，具体模型也是modified attentional encoder-decoder model，细节上结合了denoise（降噪）和back-translation（回译）技术。

## 模型结构

![](/img/shared-encoder.png)

1. 左边是一个shared-encoder，将两种语言映射到同一个latent space，并期待捕捉到真正关乎**语义**、与语言种类无关的信息。这一步的工作也是作者团队之前的工作[Learning bilingual word embeddings with (almost) no bilingual data](http://aclweb.org/anthology/P17-1042)。

2. 右边是两个decoder，分别用于将encoder编码后的句子decompose回两种语言空间内。

3. 用于两种语言的是**fixd embedding**，具体来讲word2vec对两种单语料训练完毕后，在后续训练中不再更新。encoder所做的工作就是将这些fixed embeddings映射到同一个表征语义的laten space。

## 训练过程

基于上面的模型训练过程中交替执行denoise和on-the-fly back translation两种步骤：

### denoise

即L1通过shared-encoder之后再经过L1-decoder来reconstruct原句，期望decoder能够学到从语义空间重构句子的能力。但是这里训练起来很容易变成一个**word-by-word copy**的过程，为了避免这种问题，作者在原句子中添加一些噪声，这里具体的指交换部分词语的顺序。这里也是为了让decoder能够学到不同语言的语序信息。

### on-the-fly back-translation

back-translation是Sennrich 15年提出的解决语料缺少问题的方法[Improving Neural Machine Translation Models with Monolingual Data](https://arxiv.org/abs/1511.06709)，普通的back-translaiton是用训练好的翻译模型将L1单语料库一次性翻译为L2语言，之后再将其**用于L2->L1**的翻译训练中，**这是因为对于源语言的准确度不做要求，翻译问题更关注的是翻译出的句子要是高质量的**，这点与生活中的翻译是相似的，优秀的翻译对于稍显凌乱的输入也要有一定的容错率，但是翻译出来的句子一定要是通顺连贯的。

本文中on-the-fly的意思为对于每一个mini-batch进行back-translation（通过shared-encoder和另一种语言的decoder），这样随着训练过程回译的句子质量也会提升。

具体的流程为L1语言的句子s1经过encoder编码（没有denoise过程），再经过L2decoder解码，得到s2，得到了peseudo pair(s2, s1)，**这个过程模型处于inference mode，不对任何参数做更新**。得到了句子对之后，进行翻译过程的训练：s2经过encoder编码，L1decoder解码，得到translated s1，通过最大化P(s1|s2)来更新参数。所以这一步更新了**encoder和L1 decoer的参数，并没有更新L2 decoder的参数**。

### overall

整个模型的训练过程按mini-batch为单位重复以下4步：

1. L1 noise and reconstruct，更新encoder和L1 decoder的参数。
2. L2 noise and reconstruct，更新encoder和L2 decoder的参数。
3. on-the-fly back-translation from L1 to L2，更新encoder和L1 decoder的参数。
4. on-the-fly back-translation from L2 to L1，更新encoder和L2 decoder的参数。


# Word Translation without Parallel Data

## 概述

这和第一篇文章都旨在对齐两个空间，第一篇文章提出了一种self-learning的框架，而这篇使用的是完全无监督的adversirial traing的方法来对齐，以及一些零碎的工作拼凑在一起，比如使用Procrustes algorithm方法来做refinement，并在生成词典时使用自己提出的CSLS方法。

## Schema

![](/img/fb-word-translation.png)

1. In panel(B)，用GAN方法来对齐两个空间，mapping matrix W相当于generator，descriminator来鉴别WX与Y，使之越来越相近。这样初步训练完毕后，得到了一个roughly对齐的线性变换X。
2. In panel(C)，使用**Procrustes**来进一步refine。这一步使用上一步中的**高频词**作为anchor来进一步优化W，并将优化过后的W再应用于source space，这样可以迭代几步。
3. In panel(D), 完成word translation，使用WX将X映射到Y空间，并利用作者提出的方法(dubbed CSLS)来寻找word pairs，这种方法可以使一些分布密集处的词汇(dubbed 'hub')变得稀疏一些，从而利于找到真正对应的词。

所以整个系统的训练过程分为两步，一是Adversarial Training，二是利用普氏分析来refine（这里用到了CSLS）。

## Adversarial Learning

两个空间的词向量都直接使用fastText训练好的（300维，基于Wikipedia），W视为generator，另外训练一个D用于鉴别WX和Y，训练具体方法都参照Goodfellow文章中的建议。

要注意的是矩阵W保持正交矩阵，研究表明这会提高性能，本文通过如下方法：

$$ W\leftarrow (1+\beta )W-\beta (WW^{T})W $$

## CSLS

**Cross-domain Similarity Local Scaling**是作者提出的一种寻找word pair或者说寻找最近邻的新指标。首先是$r _ {T}(Wx _ {s})$这个指标来表征平均相似度：
$$
r _ { T } \left( W x _ { s } \right) = \frac { 1 } { K } \sum _ { y _ { t } \in N _ { T } \left( W x _ { s } \right) } \cos \left( W x _ { s } , y _ { t } \right)
$$


用$N _ {T}(Wx _ {s})$表示在Target空间中Wxs最近的K个embedding，$cos$表示余弦相似度。

将$Wx _ {s}$与$y _ {t}$的相似度定义为：

$$
C S L S \left( W x _ { s } , y _ { t } \right) = 2 \cos \left( W x _ { s } , y _ { t } \right) - r _ { T } \left( W x _ { s } \right) - r _ { S } \left( y _ { T } \right)
$$


## Refinement

生成对抗网络训练结束后已经得到一个映射W了，但是作者认为其中的rare word没有得到充分的对齐，且也更难以对齐，所以想再加一步refine其对齐的精度，使用的是Procrustes Algorithm（普氏分析，图像中常用人脸对齐）。

具体操作为：
1. 从上一步得到的WX和Y中，根据CSLS和mutual nearest neighbors的原则（word pair必须互为CSLS原则下的最近邻），选取一个高频率且高质量的词典。
2. 对词典应用SVD分解得到的solution，从而得到一个更佳的W。这一步基于W是正交阵的条件。
3. 重复1，由更佳的W再次推断一个新的dictionary（理论上会更加丰富），从而迭代。

作者提到这种方法在多次迭代后，实际上提高的word translation准确率只有1%左右。**（所以只进行一次Refine就好？）**

## Validation Metric & Model Selection

由于是一个无监督的系统，工作在没有平行语料的前提下，所以作者提出了这种情况下的验证和模型选择方法：

1. 从source words中选择频率最高的10k个词，用CSLS原则寻找他们的翻译词。
2. 计算这些翻译词的平均余弦距离，并将其作为validation metric。

结果显示这个简单的指标与translation accuracy well correlated，所以将其作为train时的stopping criterion和最后模型选择时的标准。

# Unsupervised Machine Translation Using Monolingual Corpora Only

## System Overview

整体想法和前两篇文章的团队基本一致，竟然是同年发表，想法撞车了。

![](/img/fb-unsupervised-system.png)

所谓iterative training指的是每次encoder和decoder更新之后，就会再次进行一次训练过程。和上一篇文章的交替目标函数不同，这个系统每次将上图的4个loss加起来训练。

### Denoising auto-encoder

- 首先明确auto-encoder也是由encoder和decoder组成的，其作用就是先将输入压缩到一个latent space，再重构出输入。
- 若使用最初的输入，auto-encoder只会从中学会word-by-word copy的行为，学习不到有用的信息。因此在输入中加入噪声，具体在本文中是**swap&drop**，这样auto-encoder就必须学习到噪声化输入中的有用信息。
- 学习到的这些有用信息，就成为**latent space**，作者希望这个空间是与语种无关的，单纯的保留语义信息的一个空间。因此才会有后面的adversarial过程，想要对齐这个潜在空间。
- 这个auto-encoder中的decoder学到的实质上就是一个语言模型。

### Cross-domain training(back-translation)

- 理论上讲，如果encoder完美的将不同语言输入映射到同一个表征语义的laten space，decoder又完美的学会恢复到不同的语言的话，翻译系统就完成了。
- 但是在denoising auto-encoder的训练过程中，我们只能保证decoder可以从latent space恢复出原有语种的句子，而没有进行跨语种重构的训练。因此要有cross-domain training的存在。即在训练过程中显示的构造翻译过程。
- 首先利用上一个时刻的翻译模型，得到伪句对，之后将伪句对按照**反方向**进行翻译训练。

### Adversarial training

鉴别编码之后的向量是来自source还是target，根本目的也是将encoder后的表征对齐到同一个latent space，能骗过descriminator。

## Remark

- 对两种语言使用同一个encoder和decoder，在不同的lookup table中切换即可。
- word-by-word translation会保留大部分的语义信息。虽然语序和结构还有点差别。


## 自然自语

### FB 论文总

- 首先，FB这篇文章真的是总结吗？？？果然不是，是综合了前两篇的好处，提出了第三种，因为BPE的应用，共享的embedding和decoder完全不同于之前的。
- 有几个encoder和decoder？
- 不确定正确：FB文章所讲的language model/autoencoder，意义在于，我第一轮word-by-word translation之后，语序上，句子结构上都有很大的问题，我希望通过autoencoder把它调整为正确的句子，也就是流畅一些的翻译。但是这样的话，latent space的意义？或者后面不用word-by-word了，这样做还有意义吗？
- PPT page2中，C和D的连续性？这两个过程有啥关系？D好像是割裂的一个back-translation过程。
- 第一步word-by-word translation也可以用BPE，FB这篇EMNLP best paper对相似语料共同使用BPE算法来分sub-words。
- iterative back-translation?普通的back-translation利用平行语料库训练一个T->S的模型，再利用这个模型把大量的monolingual corpora of T换成带噪声的S'，这样大量的S'->T用来数据增强。在无监督中，这个过程被加强为，S->T的模型被加强以后，再利用这个模型去生成大量的T'，训练T->S这个方向的模型，提高效果。提高了之后又可以T->S增加语料。形成了dual-learning的结构。
- dual-learning是不是本身就是基于back-translation的？dual-learning的论文中说，back-translation虽然可以augment，但是不能保证质量。there is no guarantee/control on the quality of the pseudo bilingual sentence pairs.
- initialize的时候也不用双语词典了，直接BPE jonitly on monolingual data of S&T。然后训练embedding，应用在encoder和decoder。

## Cho组论文

- 所谓back-translation。如若我有一个模型Mst和Mts，S-Mst->T'， T'-Mts->S' <-> S 更新 Mts得到新的Mts。
T-Mts->S', S'->Mst->T' <-> T 更新Mst得到新的Mst。
迭代。 
所以问题是，如何得到最初的Mst和Mts？In Cho组，是不是就是通过第一轮的reconstruct L1 and L2初始化整个encoder和decoder？

- 这个shared encoder的意义在于？除了噱头上寻找语义空间，但是寻找这个空间是必须的吗？我用两个encoder不行吗？为了创新？因为好的效果？知乎说：使用 shared embedding 减少参数，降低了学习难度？
答：可能，使用shared encoder才会让reconstruction这个过程对翻译是有益的？如果不是shared encoder，一个reconstruct的训练过程对于翻译是无益的。没用。

- fixed-cross-lingual-embedding 让输入统一化，本身就可以捕捉语义了（初步，无语序之类的）。但是语序不行！这也是为什么要autoencoder？


## facebook第一篇论文

- 用word-by-word生成第一轮翻译之后呢？用它作为平行语料，来训练encoder和decoder？对！那Cho的文章第一轮，是直接用encoder+decoder来生成平行语料了。facebook这篇的第二轮才开始用encoder-decoder生成平行语料。

## facebook最终论文
- 所谓的language model，就是在word-by-word之后再对齐进行重构，以期望得到一个流畅正确的句子。这和我们训练auto-encoder是一样的。word-by-word + language model(auto-encoder)这就是个早期的，第一轮翻译模型。

- 如果iterative指的是每次翻译model更新（encoder+decoder）后再次训练，那这与普通的深度网络训练有何区别？iteratively在哪里？
- 所谓对齐两个空间，只是学到一个变换W吗？要不要把词向量对齐到同一个latent space？还是说只在无监督的系统中，将句子对齐到同一个latent space？
- 只有1个encoder和1个decoder怎么控制decoder得到的是哪个domain内的？同理，encoder也要注意所编码的语言的语种吗，是同一套参数吗。
- 这个GAN的意义在哪里？是让latent space更能学习到语义上的信息吗。
- 最后一点区别就是，Cho 那篇是从头开始训模型的；而 FAIR 这篇是用 word-by-word translation 生成第一轮时的初始翻译。**从头开始训练的？没有用对齐的词典？**
- Cho 的文章直接让两种语言共用一个 encoder 和共享的词向量矩阵；而这篇文章是两种语言各有一个 encoder 和 decoder，两个 encoder 共享隐层参数，两个 decoder 共享隐层和 attention 参数，输入和输出层则不共享，但是用一个判别器分辨 encoder 编码的结果来自哪种语言，通过对抗的方法让两个 encoder 编码的语义空间重合到一块儿去，从而词向量不会在训练过程中跑偏。这种做法和前面对齐词向量的方法一脉相承，工作的延续性更好一些；而 Cho 那个拼凑感有些强。
- Final objective function中有两个auto和两个crossdomain，一轮迭代中为什么会有两个？不应该是只有一个输入句子吗？难道是从source中取两个，从target中取两个，都做这样的运算？i
- 混杂语料训练？