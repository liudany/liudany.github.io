---
layout: post
title: Convolutional Sequence to Sequence Learning
date: 2019-05-21 10:28:09
tags: [NLP]
categories: [NLP]
typora-root-url: ../../static

---

Convolutional Sequence to Sequence Learnin from Facebook.

## Intro

1. Although convolution creates representations for **fixed-size context**, we could make it larger by **stacking  serveral layers** on top of each other.
2. This multi-layer structure create **hierarchy representations** over input sequence, where **nearby input elements(tokens) interact at lower layers while distant element interact at higer layers**.
3. Hierarchy structure provides **a shorter path(O(N/k))** to capture long distance dependencies than RNN(O(N)).
4. Proposed model is equipped with **GLU, residual connection and attention** in decoder layer.

## Position Embedding

To equip the model with **a sence of order** by embedding the **absolute position** of input element.

Specifically, the position embedding is **added** to the word embedding both in encoder and decoder.

## Convolutional Block Structure

### Block Structure

Both encoder and decoder are of **block/layer structure**. Each block contains a 1d convolution and followed by a non-linearity(GLU).

### Non Linearity

The number of output channel(kernel) is 2d where d is the embedding size. Then 2d is divided into [d, d] for GLU calculation:
$$
v ( [ A, B ] ) = A \otimes \sigma ( B )
$$

### Residual

Residual connections** are supplemented in every block to enable deep convolutional networks.

### Padding

For each encoder block, we ensure the **output length matches the input length** by padding the input.

For each decoder block, we have to take care that **no future information is available** for decoder.

## Multistep Attention

First we combine the hidden state hi **and previous target element** gi(层数多了以后原始信息少，所以加gi):
$$
d _ { i } ^ { l } = W _ { d } ^ { l } h _ { i } ^ { l } + b _ { d } ^ { l } + g _ { i }
$$
Second compute the attention(score) by dot-product function between di and encoder final output z:
$$
a _ { i j } ^ { l } = \frac { \exp \left( d _ { i } ^ { l } \cdot z _ { j } ^ { u } \right) } { \sum _ { t = 1 } ^ { m } \exp \left( d _ { i } ^ { l } \cdot z _ { t } ^ { u } \right) }
$$
Then we compute a weighted sum of encoder output z **as well as input element embeddings e**(同样为了增加原始信息类似残差):
$$
c _ { i } ^ { l } = \sum _ { j = 1 } ^ { m } a _ { i j } ^ { l } \left( z _ { j } ^ { u } + e _ { j } \right)
$$
Once c has been computed, **it is simply added to hi**.

![](/img/convseq2seq.png)