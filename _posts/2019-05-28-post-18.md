---
layout: post
title: CCMT 2019 CE-EC Task
date: 2019-05-28 10:28:09
tags: [NLP]
categories: [NLP]
draft: true
typora-root-url: ../../static
---

记一下重复工作。

## Pre-processs

语料的预处理，包括中文的分词，英文的tokenize，lowercase等。

中文部分基于Jieba，英文部分基于Mosesdecoder。

英文部分经过以下预处理：首先将所有字符转换为小写形式，然后去除不可识别的符号，之后进行英语的tokenize，最后再清除掉长于250和短于3的句子。中文部分利用Jieba分词对于语料进行并行化分词处理，对应的清理掉过长和过短的句子，并按照100:1的比例划分训练集与验证集。

```bash
SCRIPTS=/home/dyliu/fairseq/examples/translation/mosesdecoder/scripts
BPEROOT=/data/dyliu/fairseq/examples/translation/subword-nmt
JIEBA=/data/dyliu/jieba/test/parallel

TOKENIZER=$SCRIPTS/tokenizer/tokenizer.perl
LC=$SCRIPTS/tokenizer/lowercase.perl
CLEAN=$SCRIPTS/training/clean-corpus-n.perl
NORM_PUNC=$SCRIPTS/tokenizer/normalize-punctuation.perl
REM_NON_PRINT_CHAR=$SCRIPTS/tokenizer/remove-non-printing-char.perl

src=en
tgt=zh
lang=en-zh

OUTDIR=ccmt2019
prep=$OUTDIR
tmp=$prep/tmp

mkdir -p $tmp $prep

file=input

echo "pre-processing English data..."
cat $file.$src | \
	perl $LC | \
	perl $NORM_PUNC en | \
	perl $REM_NON_PRINT_CHAR | \
	perl $TOKENIZER -threads 8 -a -l en >> $tmp/train.tags.$lang.tok.$src


echo "pre-processing Chinese data..."

python $JIEBA/test_file.py ./$file.$tgt 

mv ./train.tags.en-zh.tok.zh $tmp/

echo "splitting train and valid..."
for l in $src $tgt; do
    awk '{if (NR%100 == 0)  print $0; }' $tmp/train.tags.$lang.tok.$l > $tmp/valid.$l
    awk '{if (NR%100 != 0)  print $0; }' $tmp/train.tags.$lang.tok.$l > $tmp/train.$l
done

#echo "learn_bpe.py on ${file}..."
#BPE_TOKENS=40000
#BPR_CODE=./code
#python $BPEROOT/learn_bpe.py -s $BPE_TOKENS < $file > $BPE_CODE

#echo "apply_bpe.py to ${file}..."
#python $BPEROOT/apply_bpe.py -c $BPE_CODE < $file > bpe.$file


echo "cleaning data..."
perl $CLEAN -ratio 1.5 $tmp/train $src $tgt $prep/train 1 250
perl $CLEAN -ratio 1.5 $tmp/valid $src $tgt $prep/valid 1 250
```

⚠️这里不确定BPE对中英翻译是否有效，先注释掉后面再试。

## Train

### fairseq-preprocess

```bash
TEXT=/data/dyliu/script/ccmt2019
fairseq-preprocess --source-lang en --target-lang zh \
    --trainpref $TEXT/train --validpref $TEXT/valid \
    --destdir data-bin/ccmt2019 --nwordstgt 50000 --nwordssrc 50000
```

这一步建立词表并转换为二进制文件以备后面训练。

### fairseq-train

```python
mkdir -p checkpoints/fconv-ccmt-test
CUDA_VISIBLE_DEVICES=3 fairseq-train data-bin/ccmt2019 \
    --lr 0.25 --clip-norm 0.1 --dropout 0.2 --max-tokens 4000 \
    --arch transformer --save-dir checkpoints/fconv-ccmt-test
```

默认使用全部GPU，所以设置可见GPU的数目。

⚠️注意这里以max-tokens的形式确定batch的大小。

## 结果记录

### fconv casia2015

当前词表阈值设置为0，预处理的英文词表30w+，中文20w+。模型参数265809872。

fconv框架下出现

```bash
| WARNING: ran out of memory with exception: CUDA out of memory. Tried to allocate 4.37 GiB (GPU 0; 10.92 GiB total capacity; 4.59 GiB already allocated; 1.52 GiB free; 4.18 GiB cached);
 Skipping batch
| WARNING: OOM in all workers, skipping update
```

改变词表试一下，preprocess里面用`--thresholdtgt` 和`--thresholdsrc`参数，设置个2试试。

设置为2的时候，en语料清洗0.5%左右，10W词表。zh17W词表，1%左右被替换。

还是同样的错误。

使用`—nwordssrc`和`-nwordstgt`直接设置词表大小。

5W词表大小，替换1.18%的英文。替换掉3.5%的中文。参数大小139592864。

| vocab_size_en | vocab_size_zh | threshold | Replace(en/zh%) | parameters  | oom  |
| :-----------: | :-----------: | :-------: | :-------------: | :---------: | :--: |
|    300,000    |    200,000    |     0     |        0        | 265,809,872 | yes  |
|    100,000    |    170,000    |     2     |        2        |             | yes  |
|    50,000     |    50,000     |     -     |    1.18/3.5     | 139,592,864 |  no  |



看了几篇ACL文章，都是30k左右的vocab，英文覆盖率普遍比较高，99%以上，中文在97%以上。

### Transformer

先跑iwslt14 de-en试试。

```python
CUDA_VISIBLE_DEVICES=1 fairseq-train data-bin/iwslt14.tokenized.de-en \
  -a transformer_iwslt_de_en --optimizer adam --lr 0.0005 -s de -t en \
  --label-smoothing 0.1 --dropout 0.3 --max-tokens 4000 \
  --min-lr '1e-09' --lr-scheduler inverse_sqrt --weight-decay 0.0001 \
  --criterion label_smoothed_cross_entropy --max-update 50000 \
  --warmup-updates 4000 --warmup-init-lr '1e-07' \
  --adam-betas '(0.9, 0.98)' --save-dir checkpoints/transformer
```

报错：

```bash
TypeError: Class advice impossible in Python3.  Use the @implementer class decorator instead.
```

先尝试更新pytorch版本，0.4->1.0，解决了。



处理语料，合并所有提供的语料并跑预处理脚本。

先预处理

```bash
TEXT=/data/dyliu/script/ccmt2019
fairseq-preprocess --source-lang en --target-lang zh \
    --trainpref $TEXT/train --validpref $TEXT/valid \
    --destdir data-bin/ccmt2019 --nwordstgt 50000 --nwordssrc 50000
```

在4卡上，综合语料上，600W+，跑5W的词表，跑en-zh。



```bash
CUDA_VISIBLE_DEVICES=0 fairseq-train data-bin/ccmt2019 \
  -a transformer --optimizer adam --lr 0.0005 -s en -t zh \
  --label-smoothing 0.1 --dropout 0.3 --max-tokens 4000 \
  --min-lr '1e-09' --lr-scheduler inverse_sqrt --weight-decay 0.0001 \
  --criterion label_smoothed_cross_entropy --max-update 50000 \
  --warmup-updates 4000 --warmup-init-lr '1e-07' \
  --adam-betas '(0.9, 0.98)' --save-dir checkpoints/transformer-en-zh
```



跑完了，这个条件是50000步，不知道会不会有问题。

下面测试。要处理xml格式数据。

先试试interactive，首先要指定模型和文件夹，下面有语言，要显示规定语言对

```bash
fairseq-interactive data-bin/ccmt2019/ --path checkpoints/transformer/checkpoint_best.pt \
	 		-s en -t zh
```

总共需要4个参数，第一个是字典存在的文件夹，第二个是需要的checkpoint，接下来是指定的语言扩展名，用于找词典。

交互式的方法要注意人工添加BPE标志，人工



在8卡上，同样的预料，泡个fcov。注意切到fairseq环境下，source activate fairseq。

```bash
CUDA_VISIBLE_DEVICES=3 fairseq-train data-bin/ccmt2019 \
    --lr 0.25 --clip-norm 0.1 --dropout 0.2 --max-tokens 4000 \
    --arch fconv --save-dir checkpoints/fconv-ccmt2019
```

参数139592864。词表一样大无所谓语言？

## 文件处理

```bash
grep pattern file
grep '<seg id' filename
```

文本搜索命令，返回的是符合的行。



sed命令一次处理一行，

```bash
sed [-nefri] [动作]
```

参数方面一般指定e，n是silent模式，f直接写在文件里，i直接改变文件

动作是[n1, n2] function，前两个数字指定行数。

```bash
sed "s/my/ldy's/g" test.txt
```

s是替代，my是匹配的，ldy's是替换成的，g是每一行所有匹配

sed命令没有改写，除非 -i 直接改变文件，或者 > 

-e 是多个匹配时用。


在每行的头添加字符，比如"HEAD"，命令如下：

sed 's/^/HEAD&/g' test.file

在每行的行尾添加字符，比如“TAIL”，命令如下：

sed 's/$/&TAIL/g' test.file

具体来讲处理xml文件：

```bash
    grep '<seg id' $orig/test-full/newstest2014-fren-$t.$l.sgm | \
        sed -e 's/<seg id="[0-9]*">\s*//g' | \
        sed -e 's/\s*<\/seg>\s*//g' | \
        sed -e "s/\’/\'/g" | \
    perl $TOKENIZER -threads 8 -a -l $l > $tmp/test.$l
```

## 现在来generate

现在preprocess它

```bash
fairseq-preprocess --source-lang en --target-lang zh \
    --testpref test\
    --destdir data-bin/ccmt2019 --srcdict data-bin/ccmt2019/dict.en.txt\
    --tgtdict data-bin/ccmt2019/dict.zh.txt
```

这样可以只处理test，注意preprocess和待处理的文件应该在同一个路径下面执行。

指定两种lang可以限制名字。

然后generate

```bash
CUDA_VISIBLE_DEVICES=3 fairseq-generate data-bin/ccmt2019/ --path checkpoints/transformer-en-zh-infinite/checkpoint_best.pt --batch-size 128 --beam 5 --source-lang en --target-lang zh
```

说名字不对，test没找到，路径问题，上面preprocess在当前路径下执行。

改好了。AttributeError: 'NoneType' object has no attribute 'sizes' 。

这是因为要有对比的，会有一个h s p 和target的对比，即必须要有target语言。即preprocess不可以只提供一种语言，不可以only-source，在data-bin里必须要target的内容。

注意要以>的形式输出到文件。

之后正则匹配掉前后无关的东西。

```bash
grep 'H' dev.output | sed 's/H-[0-9]*\t-[0-9]*.[0-9]*\t//g'|sed 's/ //g '>test.output
```

就ok了。



## 回过头调transformer

之前

```bash
CUDA_VISIBLE_DEVICES=3 fairseq-train data-bin/ccmt2019 \
  -a transformer --optimizer adam --lr 0.0005 -s zh -t en \
  --label-smoothing 0.1 --dropout 0.3 --max-tokens 4000 \
  --min-lr '1e-09' --lr-scheduler inverse_sqrt --weight-decay 0.0001 \
  --criterion label_smoothed_cross_entropy \
  --warmup-updates 4000 --warmup-init-lr '1e-07' \
  --adam-betas '(0.9, 0.98)' --save-dir checkpoints/transformer-zh-en
```

有三种停止条件，max-epoch max-update和min-lr，默认的min_lr 1e-09。

```bash
Namespace(activation_dropout=0.0, activation_fn='relu', adam_betas='(0.9, 0.98)', adam_eps=1e-08, adaptive_input=False, adaptive_softmax_cutoff=None, adaptive_softmax_dropout=0, arch='transformer', attention_dropout=0.0, bucket_cap_mb=25, clip_norm=25, cpu=False, criterion='label_smoothed_cross_entropy', curriculum=0, data='data-bin/ccmt2019', dataset_impl='cached', ddp_backend='c10d', decoder_attention_heads=8, decoder_embed_dim=512, decoder_embed_path=None, decoder_ffn_embed_dim=2048, decoder_final_norm=False, decoder_input_dim=512, decoder_layers=6, decoder_learned_pos=False, decoder_normalize_before=False, decoder_output_dim=512, device_id=0, disable_validation=False, distributed_backend='nccl', distributed_init_method=None, distributed_no_spawn=False, distributed_port=-1, distributed_rank=0, distributed_world_size=1, dropout=0.3, encoder_attention_heads=8, encoder_embed_dim=512, encoder_embed_path=None, encoder_ffn_embed_dim=2048, encoder_layers=6, encoder_learned_pos=False, encoder_normalize_before=False, find_unused_parameters=False, fix_batches_to_gpus=False, fp16=False, fp16_init_scale=128, fp16_scale_tolerance=0.0, fp16_scale_window=None, keep_interval_updates=-1, keep_last_epochs=-1, label_smoothing=0.1, lazy_load=False, left_pad_source='True', left_pad_target='False', log_format=None, log_interval=1000, lr=[0.0005], lr_scheduler='inverse_sqrt', max_epoch=0, max_sentences=None, max_sentences_valid=None, max_source_positions=1024, max_target_positions=1024, max_tokens=4000, max_update=0, memory_efficient_fp16=False, min_loss_scale=0.0001, min_lr=1e-09, no_epoch_checkpoints=False, no_progress_bar=False, no_save=False, no_token_positional_embeddings=False, num_workers=0, optimizer='adam', optimizer_overrides='{}', raw_text=False, required_batch_size_multiple=8, reset_lr_scheduler=False, reset_meters=False, reset_optimizer=False, restore_file='checkpoint_last.pt', save_dir='checkpoints/transformer-en-zh-infinite', save_interval=1, save_interval_updates=0, seed=1, sentence_avg=False, share_all_embeddings=False, share_decoder_input_output_embed=False, skip_invalid_size_inputs_valid_test=False, source_lang='en', target_lang='zh', task='translation', tensorboard_logdir='', threshold_loss_scale=None, train_subset='train', update_freq=[1], upsample_primary=1, user_dir=None, valid_subset='valid', validate_interval=1, warmup_init_lr=1e-07, warmup_updates=4000, weight_decay=0.0001)
| [en] dictionary: 50000 types
| [zh] dictionary: 50000 types
```





## tensorboard

怎么开这个。



## 结果记录

现在30023 8卡机器在跑的是fconv-ccmt2019，跑完了。测一下





40023 4卡机器在跑的是transformer-en-zh-infinite，这名字是一开始我以为不设置结束条件就无限制（其实有min_lr）。





现在问题是生成的句子是乱序的

去了H，然后sort -n，再去数字



应该在generate的时候同时算bleu，不要后来算



算BLUE分字还是分词？



## 方案

back-translation

model fusion + LM

多模型融合

## 测试阶段

第一步，处理好test**双语**数据，包括分词，放到fairseq文件夹下 main。

```python
CUDA_VISIBLE_DEVICES=0 fairseq-train data-bin/yzp-2 \
  -a transformer --optimizer adam --lr 0.0005 -s zh -t en \
  --label-smoothing 0.1 --dropout 0.3 --max-tokens 4000 \
  --min-lr '1e-09' --lr-scheduler inverse_sqrt --weight-decay 0.0001 \
  --criterion label_smoothed_cross_entropy \
  --warmup-updates 4000 --warmup-init-lr '1e-07' \
  --adam-betas '(0.9, 0.98)' --save-dir checkpoints/yzp-2
```

