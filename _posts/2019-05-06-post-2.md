---
layout: post
title: The Annotated Transformer
date: 2019-05-06 09:28:09
tags: [Autoencoder]
categories: [Autoencoder]
typora-root-url: ../../static

---

详解[Harvard实现的这个transformer代码](http://nlp.seas.harvard.edu/2018/04/03/attention.html#greedy-decoding)。

## Embedding

- Positional Encoding: 在[CNN seq2seq](https://arxiv.org/pdf/1705.03122.pdf)这篇文章里面提到了许多positional encoding的方法，有learned和fixed。
- Embedding and Softmax: Share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [cite](https://arxiv.org/abs/1608.05859).

## Optimizer

利用了warm up的先增后减的学习率。

## Regularization

[Label smoothing](https://arxiv.org/abs/1512.00567)，因为one-hot向量会使模型over-confident，在训练集小的时候更会出现这种情况，减小one-hot中1的大小，并均匀的分配到其他位置，但是和依然为1。

## Decoder/Encoder

`Decoder(layer, n)`模块是将`DecoderLayer`重复`n`次，然后**跟一个LayerNrom层**。

`DecoderLayer`类中还有一个参数`self.sublayer = clones(SublayerConnection(size, dropout), 3)`。

`SublayerConnection`这个类构造时没指定包含什么模块，但在传向传播时`forward(x, sublayer)`指定输入和层，进行了如下图的操作过程。

![](/img/flow1.png)

继续看`DecoderLayer`其传播过程为将第一层sublayer指定为***对x进行的self-attention***，第二层***计算encoder输出序列(memory)和x(output of previous sublayer)之间的前后attention***，用的函数是一样的，第三层使用***一个全连接层***。

所以decoder的流程就是不断重复上段的过程，再看**encoder和decoder的接口部分**。

Encoder的输出为memory，用来计算前后attention，decoder中的**x是target的embedding**，这一点与RNN相同。

而target_mask和src_mask的应用与实现分别在self_attn和src_attn中，都是`MultiHeadAttention`类的实例。

***Encoder结构大致相同，只是每个encoder layer中没有src_attention层。***

## Attention

本文所用的Scaled Dot Product Attention实现为一个***函数(而非类)***：

```python
def attention(query, key, value, mask=None, dropout=None):
    d_k = query.size(-1)
    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)
    if mask is not None:
        scores = scores.masked_fill_(mask == 0, value=-1e9)
    p_attn = softmax(scores, dim=-1)
    if dropout is not None:
        p_attn = dropout(p_attn)
    return torch.matmul(p_attn, value), p_attn
```

Dot-product就是利用query和key之间的***点乘***运算来计算scores，Scaled的意思是除以根号维度(scaling factor 缩放因子)。用query和key计算出对于每个key的分数，然后利用分数求value的加权和。

BTW，常用的另一种attention计算方法是addictive attention，利用一个前向网络来求分数。

这里的`masked_fill_`函数的用法是，利用`mask == 0`产生一个二值的ByteTensor，其值为1的位置需要用value(float)来代替。注意value可以直接用数字或者用0维tensor(torch.tensor(5))。

这里的mask分src_mask(指定pad)，和tgt_mask(阶梯)。

`src_mask`在encoder和decoder的src_attn子层中用到，作用为将query和key作multiply得到的乘积矩阵，其中源句为pad的位置置换为无穷小的数字，之后再经过softmax求分数后该位置变为0，***最终的影响是利用得分求weighted average的时候会忽略到value中对应源句pad位置的向量。***但是这不等于pad位置后面的向量都是0了，因为输入输出是不一定等长的，这些位置也会有输出。

`tgt_mask`在decoder的self-attn层中用到，其作用是让分数矩阵变成一个三角阵，这样在求weighted average的时候第t个位置只能根据(1, t)位置上的向量进行运算，之后的都乘以0不参与计算。***相当于第t个位置最终的预测不依赖其之后的词语，这样让decoder更符合真实场景下的预测。***

***⚠️这里encoder里是attention(x, x, x)，decoder是(x, m, m)，value是m，过多的关注memory了。***

此外这里的dropout将分数向量中的一部分随机换成0，其他的乘以缩放因子；往下想就是随机抛弃了一些encoder输出中的位置。

接下来是MultiHeadAttention的实现，只看前向传播部分：

```python
if mask is not None:
    # Same mask applied to all h heads.
    mask = mask.unsqueeze(1)
nbatches = query.size(0)
# 1) Do all the linear projections in batch from d_model => h x d_k
query, key, value = [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2) 
                     for l, x in zip(self.linears, (query, key, value))]
# 2) Apply attention on all the projected vectors in batch.
x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)
# 3) "Concat" using a view and apply a final linear.
x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)
return self.linears[-1](x)
```

**1）zip的结果是[(linear1, query), (linear2, key), (linear3, value)]，之后每一组前向传播之后transpose变形为`[batch, h, time-step, d_k]`，其中h就是head，然后注意最外层的中括号将其保存为`[new_q, new_k, new_v]`。**

2）用到了上面的attention函数，只对最后两个维度进行了attention分数计算，前面两个维度不变了。

3）将上一步得到的结果（加权和），变换回`[batch, time-step, h, d_k]`的形式，然后合并后两个维度。

4）最后再经过一个全联接层（所以总共用到4个FFN）。

**这里注意前两行，如果有mask的话，将其扩展维度到4维。**

## 训练框架

训练部分被`run_epoch(data_iter, model, loss)`函数封装，第一个参数产生batch数据，第二个是模型（多GPU并行），第三个是计算多GPU下的Loss值。

### The first parameter: data_iter

`run_epoch()`函数是一个for循环，从`data_iter`里面取batch，前向传播，计算loss并统计。

在**复制任务**实例中，data_iter用的是`data_gen(vocab_size, batch_size, nbatches)`函数，这个函数里用了个`numpy.random.randint(low, high, size)`函数生成假的数据，词表大小即high，size二维`[batch, 10]`，因为是复制任务所以源和目标序列相同，最后`yeild Batch(src, tgt, 0)`，所以继续看`Batch()`。

`Batch(src, trg, pad)`中对target处理为trg和trg_y，前者去掉最后一个，后者从第二个开始。这个类里有一个方法`make_std_mask`：

首先看其中的`subsequent_mask`：

```python
def subsequent_mask(size):
    """
    Mask out subsequent positions.
    """
    attn_shape = (1, size, size)
    mask = numpy.triu(numpy.ones(attn_shape), k=1).astype('uint8')
    return torch.from_numpy(mask) == 0
```

上面代码👆中triu为**upper triangle**上三角阵，第k对角线往下都是0，k=0时是标准的上三角，主对角线以下的全是0，***这里k=1的话主对角线上也都是0***。返回值是mask矩阵中为0的位置返回1，其他位置0，这两行结合起来就***返回了一个（1, size, size）下三角全1的矩阵，包括对角线***。

再看总的：

```python
def make_std_mask(tgt, pad):
    """
    Create a mask to hide padding and future words.
    """
    tgt_mask = (tgt != pad).unsqueeze(-2)
    tgt_mask = tgt_mask & Variable(subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))
    return tgt_mask
```

第一步target中pad的位置为0，其他地方都是1，得到tgt_mask形状[30, 1, 9]，variable返回的即上面说的[1, 9, 9]，作&运算时broadcast，变成了[30, 9, 9]，***本质就是有30个1x9的句子，每个句子自身复制为9x9之后再与下三角矩阵作与运算，第一行只保留第一个词，同理直到句子中pad之前的最大长度。***

上面说的是subsequent_mask函数的输出，即Batch类中的trg_mask变量。回到最初说的，`run_epoch()`函数循环调用`data_iter`（复制任务中即data_gen），yeild的用法就是每次生成一次新的，***这样不占用内存***。

总结前向传播的四个参数：

1. `batch.src`是没有经过embedding的输入，形状**(batch, time_steps)**。
2. `batch.trg`是输出序列，从第一个到倒数第二个，形状(**batch, time_steps-1)**。
3. `batch.trg_y`用于计算loss，是第二个到最后一个，形状(**batch, time_steps-1)**。
4. `batch.src_mask`是源序列中不是pad的位置是1，是pad的位置是，形状**(batch, 1, time_steps-1)**。
5. `batch.trg_mask`是上面所说的，阶梯形状的二值矩阵，形状**(batch, time_steps-1, time_steps-1)**。

### The third parameter: loss_compute

复制任务中用的`SimpleLossCompute(model.generator, criterion, model_opt)`，负责将模型输出映射到目标空间，并计算loss值，并做出一个优化步。

其中第一个参数generator作模型维度到词表的变换然后softmax，第二个参数`LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)`用到了labelsmoothing，其本质就是一个loss_function（这里用的KLLoss）的扩展。第三个参数是优化器optimizer，也是一个Adam优化器扩展，使用了Noam的方法，即结合了warmup和decay两种学习率调整的方法。

> The "noam" scheme is just a particular way how to put the warmup and decay together (linear warmup for a given number of steps followed by exponential decay).

所以run_epoch这个函数依次做了：

1. 从data_iter中得到源和目标句子，并完成目标位移和mask等工作。
2. 利用model.forward()进行前向传播得到out。
3. 利用loss_compute()函数依次将out映射到vocab空间，再和位移过的target之间的差距，并反向传播一次。

Realworld任务中用的`MultiGPULossCompute(generator, criterion, devices, opt=None, chunk_size=5)`做的事情是差不多的，

## Greedy Decode

预测阶段swtich the model to evalue mode，用greedy_decode函数来预测输出。***在只有源句没有目标句的真实预测场景下，不再能使用mask出的阶梯矩阵，必须one-by-one预测。***

```python
def greedy_decode(model, src, src_mask, max_len, start_symbol):
    memory = model.encode(src, src_mask)
    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)
    for i in range(max_len - 1):
        out = model.decode(memory, src_mask, ys, 
                           subsequent_mask(ys.size(1)).type_as(src.data))
        prob = model.generator(out[:, -1])
        _, next_word = torch.max(prob, dim=1)
        next_word = next_word.data[0]
        ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)
    return ys
```

一次性将src句子encode出memory，ys持续记录新的预测词汇，首先定义为1x1的sos，利用他去预测下一个，然后不断的cat到后面，注意decoder的输出并没有一个明确的限制，***你1x512的也可以直接输入进去，作为query和10x512的作attention最后输出1x512的没问题。***而且batch维度也不是必须的，attention计算只在乎最后两维。

## 维度变化

![](/img/flow2.png)


## Multi-GPU

按照作者的说法，他们实现了多GPU的word generation，就是在训练阶段将词的生成分解为不同的块在GPU上并行实现，用的pytorch并行原语实现的。

实现为`MultiGPULossCompute(generator, criterion, devices, opt, chunk_size)`，本质跟上面提到的复制任务的SimpleLoss是一样的，**映射到vocab，求损失并优化。**

- nn.parallel.scatter(tensor, devices)返回一个tuple，其中元素是按第一个维度分割的tensor。
- nn.parallel.replicate(model, devices)将一个在primary gpu的model复制为n份放到不同devices上，返回一个list，元素是不同的model。
- nn.parallel.parallel_apply(models, tensors)就是上面两个语句生成的东西，都必须是tuple/list形式，将会在不同GPU上一一对应的进行前向传播，然后***将out结果保存为一个list，其中元素为位于不同GPU上的各个输出***。第二个参数其中tensor再被一个中括号包围也无所谓。
- nn.parallel.gather(distributed_tensors, target_devices)第一个参数是上面一步👆生成的list of tensor，第二个参数是一个**数字或者torch.device变量**，***这一步返回一个tensor，将list中所有tensor的第一个维度压缩在一起。***

target的形状会少一维，没经过embedding。

criterion本质上也是一个前向网络，这个从Label_smoothing的写法上也能看出来。

1. 一些准备工作。

```python
total = 0.0
generator = nn.parallel.replicate(self.generator, devices=self.devices)
out_scatter = nn.parallel.scatter(out, target_gpus=self.devices)
out_grad = [[] for _ in out_scatter]	# 一个list里有几个空的list
targets = nn.parallel.scatter(target, target_gpus=self.devices)
```
​	注意这个out_grad的格式为[[], [], …, []]，元素数目和GPU数目相同。
2. 将词的生成变为chunk-level，分成了几份。

```python
for i in range(0, out_scatter[0].size(1), chunk_size):
	# 接下来进行的都在这个循环之内
```

3. 分布式预测每个位置的词汇。

```python
out_column = [[Variable(o[:, i:i + chunk_size].data, requires_grad=self.opt is not None)] 							for o in out_scatter]	# [[tensor], [tensor], ..., [tensor]]
gen = nn.parallel.parallel_apply(generator, out_column)
```

​	第一句话对分布在不同GPU上的out_scatter进行切片，取前chunk_size行（个位置），去预测该位置的词汇。

​	这一步是分布在各个GPU上进行的，因为out_scatter中的元素本来就是在不同GPU上的。

***⚠️特别注意这里的.data，使得脱离了之前的计算图独立！变成了一个新的leaf_node！是后面逻辑的起点。***

4. 计算损失。

```python
y = [(g.contiguous().view(-1, g.size(-1)), t[:, i:i + chunk_size].contiguous().view(-1)) 			 for g, t in zip(gen, targets)] # g: (batch/n, chunk_size, vocab), t: (batch/n, vocab)
loss = nn.parallel.parallel_apply(self.criterion, y) # criterion is also a pytroch module
```

​	第一句做了一个维度reduction，保持最后一个vocab维度不变，压缩了batch_new和chunk两个维度；同时让target变成一排，这样方便第二步作loss计算。

​	第二步将y[(g1, t1), (g2, t2), .., (gn, tn)]，输入到对应的criterion网络，[c1, c2, …, cn]。返回一个loss的list。注意这里的criterion在1步中也已经复制到了各块GPU上。**注意⚠️这里的loss list中的每一个值都是torch.size([])的0维tensor。**

5. 累积loss，**并normalize**

```python
l = nn.parallel.gather(loss, target_device=self.devices[0])
l = l.sum()
if l.shape == torch.Size([]):  # handle 1 GPU case
		total += l.item() / normalize
else:
		l = l[0] / normalize  
		total += l.data[0]
```

​	首先第一行第一个参数loss是一个list of 0-dimensional tensors，其元素分布在不同的GPU上，经过gather之后维度会发生变化，成为1维tensor，形状由***torch.size([])->torch.size([n])***。

​	然后做sum()运算又变成0维tensor，**这里的条件语句应该有问题，我觉得永远都是第一种情况。**total变量***累积本次循环的chunk的loss值***，这里传的normalize参数是batch.ntokens，是一个batch中所有的token数目。

6. 反向传播计算梯度，在train时opt有，是Noam；eval时没有opt。

```python
if self.opt is not None:
		l.backward()
		for j, l in enumerate(loss):
				out_grad[j].append(out_column[j][0].grad.data.clone())
```

​	注意这里的的`l`的计算图到out_column为止，因为创建out_column时调用了`.data`。

​	这里的loss变量是4.步中最后得到的，分布在不同GPU上的0维tensor。这个for循环主要是用这个loss调取GPU个数对应的j，***out_column是一个两层list包裹的tensor，所以要取两次[]***，将对应GPU上的out_column的这个chunk的grad append 到 out_grad这个list对应的2级list位置里，out_grad也是一个两层的list。

​	所以out_grad这个list里有n个list，其中每个list是一个(batch/n, chunk_size, out_size)为元素的list，其内容是刚刚backward得到的grad。

​	循环就到这里结束。

7. 所有**循环结束以后**，进行下面的：

```python
if self.opt is not None:
		out_grad = [torch.cat(og, dim=1) for og in out_grad]
    o1 = out
    o2 = nn.parallel.gather(out_grad, target_device=self.devices[0])
    o1.backward(gradient=o2)
    self.opt.step()
    self.opt.optimizer.zero_grad()
return total * normalize
```

​	第一步把out_grad中每个二级list(即每个GPU上的)不同chunk的grad按dim=1，也就是行(因为这里是3维，第一个维度是batch第二个是行第三个是列)，拼接起来，回到没分chunk之前的形状。此时out_grad是一个元素为tensor的list。

​	所以gather不同GPU上的grad就理所当然了，注意这个grad是***loss wrt. out***，根据链式法则要乘以***out wrt. Parameter***，所以会有`o1.backward(gradient=o2)`这句话，将o2认为是权重即可。

​	之后进行一个优化step并清零梯度。

​	最后返回loss * normalize，之前除以normalize这里再乘。



