---
layout: post
title: The Annotated Transformer
date: 2019-05-06 09:28:09
tags: [Autoencoder]
categories: [Autoencoder]
typora-root-url: ../../static

---

è¯¦è§£[Harvardå®ç°çš„è¿™ä¸ªtransformerä»£ç ](http://nlp.seas.harvard.edu/2018/04/03/attention.html#greedy-decoding)ã€‚

## Embedding

- Positional Encoding: åœ¨[CNN seq2seq](https://arxiv.org/pdf/1705.03122.pdf)è¿™ç¯‡æ–‡ç« é‡Œé¢æåˆ°äº†è®¸å¤špositional encodingçš„æ–¹æ³•ï¼Œæœ‰learnedå’Œfixedã€‚
- Embedding and Softmax: Share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [cite](https://arxiv.org/abs/1608.05859).

## Optimizer

åˆ©ç”¨äº†warm upçš„å…ˆå¢åå‡çš„å­¦ä¹ ç‡ã€‚

## Regularization

[Label smoothing](https://arxiv.org/abs/1512.00567)ï¼Œå› ä¸ºone-hotå‘é‡ä¼šä½¿æ¨¡å‹over-confidentï¼Œåœ¨è®­ç»ƒé›†å°çš„æ—¶å€™æ›´ä¼šå‡ºç°è¿™ç§æƒ…å†µï¼Œå‡å°one-hotä¸­1çš„å¤§å°ï¼Œå¹¶å‡åŒ€çš„åˆ†é…åˆ°å…¶ä»–ä½ç½®ï¼Œä½†æ˜¯å’Œä¾ç„¶ä¸º1ã€‚

## Decoder/Encoder

`Decoder(layer, n)`æ¨¡å—æ˜¯å°†`DecoderLayer`é‡å¤`n`æ¬¡ï¼Œç„¶å**è·Ÿä¸€ä¸ªLayerNromå±‚**ã€‚

`DecoderLayer`ç±»ä¸­è¿˜æœ‰ä¸€ä¸ªå‚æ•°`self.sublayer = clones(SublayerConnection(size, dropout), 3)`ã€‚

`SublayerConnection`è¿™ä¸ªç±»æ„é€ æ—¶æ²¡æŒ‡å®šåŒ…å«ä»€ä¹ˆæ¨¡å—ï¼Œä½†åœ¨ä¼ å‘ä¼ æ’­æ—¶`forward(x, sublayer)`æŒ‡å®šè¾“å…¥å’Œå±‚ï¼Œè¿›è¡Œäº†å¦‚ä¸‹å›¾çš„æ“ä½œè¿‡ç¨‹ã€‚

![](/img/flow1.png)

ç»§ç»­çœ‹`DecoderLayer`å…¶ä¼ æ’­è¿‡ç¨‹ä¸ºå°†ç¬¬ä¸€å±‚sublayeræŒ‡å®šä¸º***å¯¹xè¿›è¡Œçš„self-attention***ï¼Œç¬¬äºŒå±‚***è®¡ç®—encoderè¾“å‡ºåºåˆ—(memory)å’Œx(output of previous sublayer)ä¹‹é—´çš„å‰åattention***ï¼Œç”¨çš„å‡½æ•°æ˜¯ä¸€æ ·çš„ï¼Œç¬¬ä¸‰å±‚ä½¿ç”¨***ä¸€ä¸ªå…¨è¿æ¥å±‚***ã€‚

æ‰€ä»¥decoderçš„æµç¨‹å°±æ˜¯ä¸æ–­é‡å¤ä¸Šæ®µçš„è¿‡ç¨‹ï¼Œå†çœ‹**encoderå’Œdecoderçš„æ¥å£éƒ¨åˆ†**ã€‚

Encoderçš„è¾“å‡ºä¸ºmemoryï¼Œç”¨æ¥è®¡ç®—å‰åattentionï¼Œdecoderä¸­çš„**xæ˜¯targetçš„embedding**ï¼Œè¿™ä¸€ç‚¹ä¸RNNç›¸åŒã€‚

è€Œtarget_maskå’Œsrc_maskçš„åº”ç”¨ä¸å®ç°åˆ†åˆ«åœ¨self_attnå’Œsrc_attnä¸­ï¼Œéƒ½æ˜¯`MultiHeadAttention`ç±»çš„å®ä¾‹ã€‚

***Encoderç»“æ„å¤§è‡´ç›¸åŒï¼Œåªæ˜¯æ¯ä¸ªencoder layerä¸­æ²¡æœ‰src_attentionå±‚ã€‚***

## Attention

æœ¬æ–‡æ‰€ç”¨çš„Scaled Dot Product Attentionå®ç°ä¸ºä¸€ä¸ª***å‡½æ•°(è€Œéç±»)***ï¼š

```python
def attention(query, key, value, mask=None, dropout=None):
    d_k = query.size(-1)
    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)
    if mask is not None:
        scores = scores.masked_fill_(mask == 0, value=-1e9)
    p_attn = softmax(scores, dim=-1)
    if dropout is not None:
        p_attn = dropout(p_attn)
    return torch.matmul(p_attn, value), p_attn
```

Dot-productå°±æ˜¯åˆ©ç”¨queryå’Œkeyä¹‹é—´çš„***ç‚¹ä¹˜***è¿ç®—æ¥è®¡ç®—scoresï¼ŒScaledçš„æ„æ€æ˜¯é™¤ä»¥æ ¹å·ç»´åº¦(scaling factor ç¼©æ”¾å› å­)ã€‚ç”¨queryå’Œkeyè®¡ç®—å‡ºå¯¹äºæ¯ä¸ªkeyçš„åˆ†æ•°ï¼Œç„¶ååˆ©ç”¨åˆ†æ•°æ±‚valueçš„åŠ æƒå’Œã€‚

BTWï¼Œå¸¸ç”¨çš„å¦ä¸€ç§attentionè®¡ç®—æ–¹æ³•æ˜¯addictive attentionï¼Œåˆ©ç”¨ä¸€ä¸ªå‰å‘ç½‘ç»œæ¥æ±‚åˆ†æ•°ã€‚

è¿™é‡Œçš„`masked_fill_`å‡½æ•°çš„ç”¨æ³•æ˜¯ï¼Œåˆ©ç”¨`mask == 0`äº§ç”Ÿä¸€ä¸ªäºŒå€¼çš„ByteTensorï¼Œå…¶å€¼ä¸º1çš„ä½ç½®éœ€è¦ç”¨value(float)æ¥ä»£æ›¿ã€‚æ³¨æ„valueå¯ä»¥ç›´æ¥ç”¨æ•°å­—æˆ–è€…ç”¨0ç»´tensor(torch.tensor(5))ã€‚

è¿™é‡Œçš„maskåˆ†src_mask(æŒ‡å®špad)ï¼Œå’Œtgt_mask(é˜¶æ¢¯)ã€‚

`src_mask`åœ¨encoderå’Œdecoderçš„src_attnå­å±‚ä¸­ç”¨åˆ°ï¼Œä½œç”¨ä¸ºå°†queryå’Œkeyä½œmultiplyå¾—åˆ°çš„ä¹˜ç§¯çŸ©é˜µï¼Œå…¶ä¸­æºå¥ä¸ºpadçš„ä½ç½®ç½®æ¢ä¸ºæ— ç©·å°çš„æ•°å­—ï¼Œä¹‹åå†ç»è¿‡softmaxæ±‚åˆ†æ•°åè¯¥ä½ç½®å˜ä¸º0ï¼Œ***æœ€ç»ˆçš„å½±å“æ˜¯åˆ©ç”¨å¾—åˆ†æ±‚weighted averageçš„æ—¶å€™ä¼šå¿½ç•¥åˆ°valueä¸­å¯¹åº”æºå¥padä½ç½®çš„å‘é‡ã€‚***ä½†æ˜¯è¿™ä¸ç­‰äºpadä½ç½®åé¢çš„å‘é‡éƒ½æ˜¯0äº†ï¼Œå› ä¸ºè¾“å…¥è¾“å‡ºæ˜¯ä¸ä¸€å®šç­‰é•¿çš„ï¼Œè¿™äº›ä½ç½®ä¹Ÿä¼šæœ‰è¾“å‡ºã€‚

`tgt_mask`åœ¨decoderçš„self-attnå±‚ä¸­ç”¨åˆ°ï¼Œå…¶ä½œç”¨æ˜¯è®©åˆ†æ•°çŸ©é˜µå˜æˆä¸€ä¸ªä¸‰è§’é˜µï¼Œè¿™æ ·åœ¨æ±‚weighted averageçš„æ—¶å€™ç¬¬tä¸ªä½ç½®åªèƒ½æ ¹æ®(1, t)ä½ç½®ä¸Šçš„å‘é‡è¿›è¡Œè¿ç®—ï¼Œä¹‹åçš„éƒ½ä¹˜ä»¥0ä¸å‚ä¸è®¡ç®—ã€‚***ç›¸å½“äºç¬¬tä¸ªä½ç½®æœ€ç»ˆçš„é¢„æµ‹ä¸ä¾èµ–å…¶ä¹‹åçš„è¯è¯­ï¼Œè¿™æ ·è®©decoderæ›´ç¬¦åˆçœŸå®åœºæ™¯ä¸‹çš„é¢„æµ‹ã€‚***

***âš ï¸è¿™é‡Œencoderé‡Œæ˜¯attention(x, x, x)ï¼Œdecoderæ˜¯(x, m, m)ï¼Œvalueæ˜¯mï¼Œè¿‡å¤šçš„å…³æ³¨memoryäº†ã€‚***

æ­¤å¤–è¿™é‡Œçš„dropoutå°†åˆ†æ•°å‘é‡ä¸­çš„ä¸€éƒ¨åˆ†éšæœºæ¢æˆ0ï¼Œå…¶ä»–çš„ä¹˜ä»¥ç¼©æ”¾å› å­ï¼›å¾€ä¸‹æƒ³å°±æ˜¯éšæœºæŠ›å¼ƒäº†ä¸€äº›encoderè¾“å‡ºä¸­çš„ä½ç½®ã€‚

æ¥ä¸‹æ¥æ˜¯MultiHeadAttentionçš„å®ç°ï¼Œåªçœ‹å‰å‘ä¼ æ’­éƒ¨åˆ†ï¼š

```python
if mask is not None:
    # Same mask applied to all h heads.
    mask = mask.unsqueeze(1)
nbatches = query.size(0)
# 1) Do all the linear projections in batch from d_model => h x d_k
query, key, value = [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2) 
                     for l, x in zip(self.linears, (query, key, value))]
# 2) Apply attention on all the projected vectors in batch.
x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)
# 3) "Concat" using a view and apply a final linear.
x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)
return self.linears[-1](x)
```

**1ï¼‰zipçš„ç»“æœæ˜¯[(linear1, query), (linear2, key), (linear3, value)]ï¼Œä¹‹åæ¯ä¸€ç»„å‰å‘ä¼ æ’­ä¹‹åtransposeå˜å½¢ä¸º`[batch, h, time-step, d_k]`ï¼Œå…¶ä¸­hå°±æ˜¯headï¼Œç„¶åæ³¨æ„æœ€å¤–å±‚çš„ä¸­æ‹¬å·å°†å…¶ä¿å­˜ä¸º`[new_q, new_k, new_v]`ã€‚**

2ï¼‰ç”¨åˆ°äº†ä¸Šé¢çš„attentionå‡½æ•°ï¼Œåªå¯¹æœ€åä¸¤ä¸ªç»´åº¦è¿›è¡Œäº†attentionåˆ†æ•°è®¡ç®—ï¼Œå‰é¢ä¸¤ä¸ªç»´åº¦ä¸å˜äº†ã€‚

3ï¼‰å°†ä¸Šä¸€æ­¥å¾—åˆ°çš„ç»“æœï¼ˆåŠ æƒå’Œï¼‰ï¼Œå˜æ¢å›`[batch, time-step, h, d_k]`çš„å½¢å¼ï¼Œç„¶ååˆå¹¶åä¸¤ä¸ªç»´åº¦ã€‚

4ï¼‰æœ€åå†ç»è¿‡ä¸€ä¸ªå…¨è”æ¥å±‚ï¼ˆæ‰€ä»¥æ€»å…±ç”¨åˆ°4ä¸ªFFNï¼‰ã€‚

**è¿™é‡Œæ³¨æ„å‰ä¸¤è¡Œï¼Œå¦‚æœæœ‰maskçš„è¯ï¼Œå°†å…¶æ‰©å±•ç»´åº¦åˆ°4ç»´ã€‚**

## è®­ç»ƒæ¡†æ¶

è®­ç»ƒéƒ¨åˆ†è¢«`run_epoch(data_iter, model, loss)`å‡½æ•°å°è£…ï¼Œç¬¬ä¸€ä¸ªå‚æ•°äº§ç”Ÿbatchæ•°æ®ï¼Œç¬¬äºŒä¸ªæ˜¯æ¨¡å‹ï¼ˆå¤šGPUå¹¶è¡Œï¼‰ï¼Œç¬¬ä¸‰ä¸ªæ˜¯è®¡ç®—å¤šGPUä¸‹çš„Losså€¼ã€‚

### The first parameter: data_iter

`run_epoch()`å‡½æ•°æ˜¯ä¸€ä¸ªforå¾ªç¯ï¼Œä»`data_iter`é‡Œé¢å–batchï¼Œå‰å‘ä¼ æ’­ï¼Œè®¡ç®—losså¹¶ç»Ÿè®¡ã€‚

åœ¨**å¤åˆ¶ä»»åŠ¡**å®ä¾‹ä¸­ï¼Œdata_iterç”¨çš„æ˜¯`data_gen(vocab_size, batch_size, nbatches)`å‡½æ•°ï¼Œè¿™ä¸ªå‡½æ•°é‡Œç”¨äº†ä¸ª`numpy.random.randint(low, high, size)`å‡½æ•°ç”Ÿæˆå‡çš„æ•°æ®ï¼Œè¯è¡¨å¤§å°å³highï¼ŒsizeäºŒç»´`[batch, 10]`ï¼Œå› ä¸ºæ˜¯å¤åˆ¶ä»»åŠ¡æ‰€ä»¥æºå’Œç›®æ ‡åºåˆ—ç›¸åŒï¼Œæœ€å`yeild Batch(src, tgt, 0)`ï¼Œæ‰€ä»¥ç»§ç»­çœ‹`Batch()`ã€‚

`Batch(src, trg, pad)`ä¸­å¯¹targetå¤„ç†ä¸ºtrgå’Œtrg_yï¼Œå‰è€…å»æ‰æœ€åä¸€ä¸ªï¼Œåè€…ä»ç¬¬äºŒä¸ªå¼€å§‹ã€‚è¿™ä¸ªç±»é‡Œæœ‰ä¸€ä¸ªæ–¹æ³•`make_std_mask`ï¼š

é¦–å…ˆçœ‹å…¶ä¸­çš„`subsequent_mask`ï¼š

```python
def subsequent_mask(size):
    """
    Mask out subsequent positions.
    """
    attn_shape = (1, size, size)
    mask = numpy.triu(numpy.ones(attn_shape), k=1).astype('uint8')
    return torch.from_numpy(mask) == 0
```

ä¸Šé¢ä»£ç ğŸ‘†ä¸­triuä¸º**upper triangle**ä¸Šä¸‰è§’é˜µï¼Œç¬¬kå¯¹è§’çº¿å¾€ä¸‹éƒ½æ˜¯0ï¼Œk=0æ—¶æ˜¯æ ‡å‡†çš„ä¸Šä¸‰è§’ï¼Œä¸»å¯¹è§’çº¿ä»¥ä¸‹çš„å…¨æ˜¯0ï¼Œ***è¿™é‡Œk=1çš„è¯ä¸»å¯¹è§’çº¿ä¸Šä¹Ÿéƒ½æ˜¯0***ã€‚è¿”å›å€¼æ˜¯maskçŸ©é˜µä¸­ä¸º0çš„ä½ç½®è¿”å›1ï¼Œå…¶ä»–ä½ç½®0ï¼Œè¿™ä¸¤è¡Œç»“åˆèµ·æ¥å°±***è¿”å›äº†ä¸€ä¸ªï¼ˆ1, size, sizeï¼‰ä¸‹ä¸‰è§’å…¨1çš„çŸ©é˜µï¼ŒåŒ…æ‹¬å¯¹è§’çº¿***ã€‚

å†çœ‹æ€»çš„ï¼š

```python
def make_std_mask(tgt, pad):
    """
    Create a mask to hide padding and future words.
    """
    tgt_mask = (tgt != pad).unsqueeze(-2)
    tgt_mask = tgt_mask & Variable(subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))
    return tgt_mask
```

ç¬¬ä¸€æ­¥targetä¸­padçš„ä½ç½®ä¸º0ï¼Œå…¶ä»–åœ°æ–¹éƒ½æ˜¯1ï¼Œå¾—åˆ°tgt_maskå½¢çŠ¶[30, 1, 9]ï¼Œvariableè¿”å›çš„å³ä¸Šé¢è¯´çš„[1, 9, 9]ï¼Œä½œ&è¿ç®—æ—¶broadcastï¼Œå˜æˆäº†[30, 9, 9]ï¼Œ***æœ¬è´¨å°±æ˜¯æœ‰30ä¸ª1x9çš„å¥å­ï¼Œæ¯ä¸ªå¥å­è‡ªèº«å¤åˆ¶ä¸º9x9ä¹‹åå†ä¸ä¸‹ä¸‰è§’çŸ©é˜µä½œä¸è¿ç®—ï¼Œç¬¬ä¸€è¡Œåªä¿ç•™ç¬¬ä¸€ä¸ªè¯ï¼ŒåŒç†ç›´åˆ°å¥å­ä¸­padä¹‹å‰çš„æœ€å¤§é•¿åº¦ã€‚***

ä¸Šé¢è¯´çš„æ˜¯subsequent_maskå‡½æ•°çš„è¾“å‡ºï¼Œå³Batchç±»ä¸­çš„trg_maskå˜é‡ã€‚å›åˆ°æœ€åˆè¯´çš„ï¼Œ`run_epoch()`å‡½æ•°å¾ªç¯è°ƒç”¨`data_iter`ï¼ˆå¤åˆ¶ä»»åŠ¡ä¸­å³data_genï¼‰ï¼Œyeildçš„ç”¨æ³•å°±æ˜¯æ¯æ¬¡ç”Ÿæˆä¸€æ¬¡æ–°çš„ï¼Œ***è¿™æ ·ä¸å ç”¨å†…å­˜***ã€‚

æ€»ç»“å‰å‘ä¼ æ’­çš„å››ä¸ªå‚æ•°ï¼š

1. `batch.src`æ˜¯æ²¡æœ‰ç»è¿‡embeddingçš„è¾“å…¥ï¼Œå½¢çŠ¶**(batch, time_steps)**ã€‚
2. `batch.trg`æ˜¯è¾“å‡ºåºåˆ—ï¼Œä»ç¬¬ä¸€ä¸ªåˆ°å€’æ•°ç¬¬äºŒä¸ªï¼Œå½¢çŠ¶(**batch, time_steps-1)**ã€‚
3. `batch.trg_y`ç”¨äºè®¡ç®—lossï¼Œæ˜¯ç¬¬äºŒä¸ªåˆ°æœ€åä¸€ä¸ªï¼Œå½¢çŠ¶(**batch, time_steps-1)**ã€‚
4. `batch.src_mask`æ˜¯æºåºåˆ—ä¸­ä¸æ˜¯padçš„ä½ç½®æ˜¯1ï¼Œæ˜¯padçš„ä½ç½®æ˜¯ï¼Œå½¢çŠ¶**(batch, 1, time_steps-1)**ã€‚
5. `batch.trg_mask`æ˜¯ä¸Šé¢æ‰€è¯´çš„ï¼Œé˜¶æ¢¯å½¢çŠ¶çš„äºŒå€¼çŸ©é˜µï¼Œå½¢çŠ¶**(batch, time_steps-1, time_steps-1)**ã€‚

### The third parameter: loss_compute

å¤åˆ¶ä»»åŠ¡ä¸­ç”¨çš„`SimpleLossCompute(model.generator, criterion, model_opt)`ï¼Œè´Ÿè´£å°†æ¨¡å‹è¾“å‡ºæ˜ å°„åˆ°ç›®æ ‡ç©ºé—´ï¼Œå¹¶è®¡ç®—losså€¼ï¼Œå¹¶åšå‡ºä¸€ä¸ªä¼˜åŒ–æ­¥ã€‚

å…¶ä¸­ç¬¬ä¸€ä¸ªå‚æ•°generatorä½œæ¨¡å‹ç»´åº¦åˆ°è¯è¡¨çš„å˜æ¢ç„¶åsoftmaxï¼Œç¬¬äºŒä¸ªå‚æ•°`LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)`ç”¨åˆ°äº†labelsmoothingï¼Œå…¶æœ¬è´¨å°±æ˜¯ä¸€ä¸ªloss_functionï¼ˆè¿™é‡Œç”¨çš„KLLossï¼‰çš„æ‰©å±•ã€‚ç¬¬ä¸‰ä¸ªå‚æ•°æ˜¯ä¼˜åŒ–å™¨optimizerï¼Œä¹Ÿæ˜¯ä¸€ä¸ªAdamä¼˜åŒ–å™¨æ‰©å±•ï¼Œä½¿ç”¨äº†Noamçš„æ–¹æ³•ï¼Œå³ç»“åˆäº†warmupå’Œdecayä¸¤ç§å­¦ä¹ ç‡è°ƒæ•´çš„æ–¹æ³•ã€‚

> The "noam" scheme is just a particular way how to put the warmup and decay together (linear warmup for a given number of steps followed by exponential decay).

æ‰€ä»¥run_epochè¿™ä¸ªå‡½æ•°ä¾æ¬¡åšäº†ï¼š

1. ä»data_iterä¸­å¾—åˆ°æºå’Œç›®æ ‡å¥å­ï¼Œå¹¶å®Œæˆç›®æ ‡ä½ç§»å’Œmaskç­‰å·¥ä½œã€‚
2. åˆ©ç”¨model.forward()è¿›è¡Œå‰å‘ä¼ æ’­å¾—åˆ°outã€‚
3. åˆ©ç”¨loss_compute()å‡½æ•°ä¾æ¬¡å°†outæ˜ å°„åˆ°vocabç©ºé—´ï¼Œå†å’Œä½ç§»è¿‡çš„targetä¹‹é—´çš„å·®è·ï¼Œå¹¶åå‘ä¼ æ’­ä¸€æ¬¡ã€‚

Realworldä»»åŠ¡ä¸­ç”¨çš„`MultiGPULossCompute(generator, criterion, devices, opt=None, chunk_size=5)`åšçš„äº‹æƒ…æ˜¯å·®ä¸å¤šçš„ï¼Œ

## Greedy Decode

é¢„æµ‹é˜¶æ®µswtich the model to evalue modeï¼Œç”¨greedy_decodeå‡½æ•°æ¥é¢„æµ‹è¾“å‡ºã€‚***åœ¨åªæœ‰æºå¥æ²¡æœ‰ç›®æ ‡å¥çš„çœŸå®é¢„æµ‹åœºæ™¯ä¸‹ï¼Œä¸å†èƒ½ä½¿ç”¨maskå‡ºçš„é˜¶æ¢¯çŸ©é˜µï¼Œå¿…é¡»one-by-oneé¢„æµ‹ã€‚***

```python
def greedy_decode(model, src, src_mask, max_len, start_symbol):
    memory = model.encode(src, src_mask)
    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)
    for i in range(max_len - 1):
        out = model.decode(memory, src_mask, ys, 
                           subsequent_mask(ys.size(1)).type_as(src.data))
        prob = model.generator(out[:, -1])
        _, next_word = torch.max(prob, dim=1)
        next_word = next_word.data[0]
        ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)
    return ys
```

ä¸€æ¬¡æ€§å°†srcå¥å­encodeå‡ºmemoryï¼ŒysæŒç»­è®°å½•æ–°çš„é¢„æµ‹è¯æ±‡ï¼Œé¦–å…ˆå®šä¹‰ä¸º1x1çš„sosï¼Œåˆ©ç”¨ä»–å»é¢„æµ‹ä¸‹ä¸€ä¸ªï¼Œç„¶åä¸æ–­çš„catåˆ°åé¢ï¼Œæ³¨æ„decoderçš„è¾“å‡ºå¹¶æ²¡æœ‰ä¸€ä¸ªæ˜ç¡®çš„é™åˆ¶ï¼Œ***ä½ 1x512çš„ä¹Ÿå¯ä»¥ç›´æ¥è¾“å…¥è¿›å»ï¼Œä½œä¸ºqueryå’Œ10x512çš„ä½œattentionæœ€åè¾“å‡º1x512çš„æ²¡é—®é¢˜ã€‚***è€Œä¸”batchç»´åº¦ä¹Ÿä¸æ˜¯å¿…é¡»çš„ï¼Œattentionè®¡ç®—åªåœ¨ä¹æœ€åä¸¤ç»´ã€‚

## ç»´åº¦å˜åŒ–

![](/img/flow2.png)


## Multi-GPU

æŒ‰ç…§ä½œè€…çš„è¯´æ³•ï¼Œä»–ä»¬å®ç°äº†å¤šGPUçš„word generationï¼Œå°±æ˜¯åœ¨è®­ç»ƒé˜¶æ®µå°†è¯çš„ç”Ÿæˆåˆ†è§£ä¸ºä¸åŒçš„å—åœ¨GPUä¸Šå¹¶è¡Œå®ç°ï¼Œç”¨çš„pytorchå¹¶è¡ŒåŸè¯­å®ç°çš„ã€‚

å®ç°ä¸º`MultiGPULossCompute(generator, criterion, devices, opt, chunk_size)`ï¼Œæœ¬è´¨è·Ÿä¸Šé¢æåˆ°çš„å¤åˆ¶ä»»åŠ¡çš„SimpleLossæ˜¯ä¸€æ ·çš„ï¼Œ**æ˜ å°„åˆ°vocabï¼Œæ±‚æŸå¤±å¹¶ä¼˜åŒ–ã€‚**

- nn.parallel.scatter(tensor, devices)è¿”å›ä¸€ä¸ªtupleï¼Œå…¶ä¸­å…ƒç´ æ˜¯æŒ‰ç¬¬ä¸€ä¸ªç»´åº¦åˆ†å‰²çš„tensorã€‚
- nn.parallel.replicate(model, devices)å°†ä¸€ä¸ªåœ¨primary gpuçš„modelå¤åˆ¶ä¸ºnä»½æ”¾åˆ°ä¸åŒdevicesä¸Šï¼Œè¿”å›ä¸€ä¸ªlistï¼Œå…ƒç´ æ˜¯ä¸åŒçš„modelã€‚
- nn.parallel.parallel_apply(models, tensors)å°±æ˜¯ä¸Šé¢ä¸¤ä¸ªè¯­å¥ç”Ÿæˆçš„ä¸œè¥¿ï¼Œéƒ½å¿…é¡»æ˜¯tuple/listå½¢å¼ï¼Œå°†ä¼šåœ¨ä¸åŒGPUä¸Šä¸€ä¸€å¯¹åº”çš„è¿›è¡Œå‰å‘ä¼ æ’­ï¼Œç„¶å***å°†outç»“æœä¿å­˜ä¸ºä¸€ä¸ªlistï¼Œå…¶ä¸­å…ƒç´ ä¸ºä½äºä¸åŒGPUä¸Šçš„å„ä¸ªè¾“å‡º***ã€‚ç¬¬äºŒä¸ªå‚æ•°å…¶ä¸­tensorå†è¢«ä¸€ä¸ªä¸­æ‹¬å·åŒ…å›´ä¹Ÿæ— æ‰€è°“ã€‚
- nn.parallel.gather(distributed_tensors, target_devices)ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯ä¸Šé¢ä¸€æ­¥ğŸ‘†ç”Ÿæˆçš„list of tensorï¼Œç¬¬äºŒä¸ªå‚æ•°æ˜¯ä¸€ä¸ª**æ•°å­—æˆ–è€…torch.deviceå˜é‡**ï¼Œ***è¿™ä¸€æ­¥è¿”å›ä¸€ä¸ªtensorï¼Œå°†listä¸­æ‰€æœ‰tensorçš„ç¬¬ä¸€ä¸ªç»´åº¦å‹ç¼©åœ¨ä¸€èµ·ã€‚***

targetçš„å½¢çŠ¶ä¼šå°‘ä¸€ç»´ï¼Œæ²¡ç»è¿‡embeddingã€‚

criterionæœ¬è´¨ä¸Šä¹Ÿæ˜¯ä¸€ä¸ªå‰å‘ç½‘ç»œï¼Œè¿™ä¸ªä»Label_smoothingçš„å†™æ³•ä¸Šä¹Ÿèƒ½çœ‹å‡ºæ¥ã€‚

1. ä¸€äº›å‡†å¤‡å·¥ä½œã€‚

```python
total = 0.0
generator = nn.parallel.replicate(self.generator, devices=self.devices)
out_scatter = nn.parallel.scatter(out, target_gpus=self.devices)
out_grad = [[] for _ in out_scatter]	# ä¸€ä¸ªlisté‡Œæœ‰å‡ ä¸ªç©ºçš„list
targets = nn.parallel.scatter(target, target_gpus=self.devices)
```
â€‹	æ³¨æ„è¿™ä¸ªout_gradçš„æ ¼å¼ä¸º[[], [], â€¦, []]ï¼Œå…ƒç´ æ•°ç›®å’ŒGPUæ•°ç›®ç›¸åŒã€‚
2. å°†è¯çš„ç”Ÿæˆå˜ä¸ºchunk-levelï¼Œåˆ†æˆäº†å‡ ä»½ã€‚

```python
for i in range(0, out_scatter[0].size(1), chunk_size):
	# æ¥ä¸‹æ¥è¿›è¡Œçš„éƒ½åœ¨è¿™ä¸ªå¾ªç¯ä¹‹å†…
```

3. åˆ†å¸ƒå¼é¢„æµ‹æ¯ä¸ªä½ç½®çš„è¯æ±‡ã€‚

```python
out_column = [[Variable(o[:, i:i + chunk_size].data, requires_grad=self.opt is not None)] 							for o in out_scatter]	# [[tensor], [tensor], ..., [tensor]]
gen = nn.parallel.parallel_apply(generator, out_column)
```

â€‹	ç¬¬ä¸€å¥è¯å¯¹åˆ†å¸ƒåœ¨ä¸åŒGPUä¸Šçš„out_scatterè¿›è¡Œåˆ‡ç‰‡ï¼Œå–å‰chunk_sizeè¡Œï¼ˆä¸ªä½ç½®ï¼‰ï¼Œå»é¢„æµ‹è¯¥ä½ç½®çš„è¯æ±‡ã€‚

â€‹	è¿™ä¸€æ­¥æ˜¯åˆ†å¸ƒåœ¨å„ä¸ªGPUä¸Šè¿›è¡Œçš„ï¼Œå› ä¸ºout_scatterä¸­çš„å…ƒç´ æœ¬æ¥å°±æ˜¯åœ¨ä¸åŒGPUä¸Šçš„ã€‚

***âš ï¸ç‰¹åˆ«æ³¨æ„è¿™é‡Œçš„.dataï¼Œä½¿å¾—è„±ç¦»äº†ä¹‹å‰çš„è®¡ç®—å›¾ç‹¬ç«‹ï¼å˜æˆäº†ä¸€ä¸ªæ–°çš„leaf_nodeï¼æ˜¯åé¢é€»è¾‘çš„èµ·ç‚¹ã€‚***

4. è®¡ç®—æŸå¤±ã€‚

```python
y = [(g.contiguous().view(-1, g.size(-1)), t[:, i:i + chunk_size].contiguous().view(-1)) 			 for g, t in zip(gen, targets)] # g: (batch/n, chunk_size, vocab), t: (batch/n, vocab)
loss = nn.parallel.parallel_apply(self.criterion, y) # criterion is also a pytroch module
```

â€‹	ç¬¬ä¸€å¥åšäº†ä¸€ä¸ªç»´åº¦reductionï¼Œä¿æŒæœ€åä¸€ä¸ªvocabç»´åº¦ä¸å˜ï¼Œå‹ç¼©äº†batch_newå’Œchunkä¸¤ä¸ªç»´åº¦ï¼›åŒæ—¶è®©targetå˜æˆä¸€æ’ï¼Œè¿™æ ·æ–¹ä¾¿ç¬¬äºŒæ­¥ä½œlossè®¡ç®—ã€‚

â€‹	ç¬¬äºŒæ­¥å°†y[(g1, t1), (g2, t2), .., (gn, tn)]ï¼Œè¾“å…¥åˆ°å¯¹åº”çš„criterionç½‘ç»œï¼Œ[c1, c2, â€¦, cn]ã€‚è¿”å›ä¸€ä¸ªlossçš„listã€‚æ³¨æ„è¿™é‡Œçš„criterionåœ¨1æ­¥ä¸­ä¹Ÿå·²ç»å¤åˆ¶åˆ°äº†å„å—GPUä¸Šã€‚**æ³¨æ„âš ï¸è¿™é‡Œçš„loss listä¸­çš„æ¯ä¸€ä¸ªå€¼éƒ½æ˜¯torch.size([])çš„0ç»´tensorã€‚**

5. ç´¯ç§¯lossï¼Œ**å¹¶normalize**

```python
l = nn.parallel.gather(loss, target_device=self.devices[0])
l = l.sum()
if l.shape == torch.Size([]):  # handle 1 GPU case
		total += l.item() / normalize
else:
		l = l[0] / normalize  
		total += l.data[0]
```

â€‹	é¦–å…ˆç¬¬ä¸€è¡Œç¬¬ä¸€ä¸ªå‚æ•°lossæ˜¯ä¸€ä¸ªlist of 0-dimensional tensorsï¼Œå…¶å…ƒç´ åˆ†å¸ƒåœ¨ä¸åŒçš„GPUä¸Šï¼Œç»è¿‡gatherä¹‹åç»´åº¦ä¼šå‘ç”Ÿå˜åŒ–ï¼Œæˆä¸º1ç»´tensorï¼Œå½¢çŠ¶ç”±***torch.size([])->torch.size([n])***ã€‚

â€‹	ç„¶ååšsum()è¿ç®—åˆå˜æˆ0ç»´tensorï¼Œ**è¿™é‡Œçš„æ¡ä»¶è¯­å¥åº”è¯¥æœ‰é—®é¢˜ï¼Œæˆ‘è§‰å¾—æ°¸è¿œéƒ½æ˜¯ç¬¬ä¸€ç§æƒ…å†µã€‚**totalå˜é‡***ç´¯ç§¯æœ¬æ¬¡å¾ªç¯çš„chunkçš„losså€¼***ï¼Œè¿™é‡Œä¼ çš„normalizeå‚æ•°æ˜¯batch.ntokensï¼Œæ˜¯ä¸€ä¸ªbatchä¸­æ‰€æœ‰çš„tokenæ•°ç›®ã€‚

6. åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦ï¼Œåœ¨trainæ—¶optæœ‰ï¼Œæ˜¯Noamï¼›evalæ—¶æ²¡æœ‰optã€‚

```python
if self.opt is not None:
		l.backward()
		for j, l in enumerate(loss):
				out_grad[j].append(out_column[j][0].grad.data.clone())
```

â€‹	æ³¨æ„è¿™é‡Œçš„çš„`l`çš„è®¡ç®—å›¾åˆ°out_columnä¸ºæ­¢ï¼Œå› ä¸ºåˆ›å»ºout_columnæ—¶è°ƒç”¨äº†`.data`ã€‚

â€‹	è¿™é‡Œçš„losså˜é‡æ˜¯4.æ­¥ä¸­æœ€åå¾—åˆ°çš„ï¼Œåˆ†å¸ƒåœ¨ä¸åŒGPUä¸Šçš„0ç»´tensorã€‚è¿™ä¸ªforå¾ªç¯ä¸»è¦æ˜¯ç”¨è¿™ä¸ªlossè°ƒå–GPUä¸ªæ•°å¯¹åº”çš„jï¼Œ***out_columnæ˜¯ä¸€ä¸ªä¸¤å±‚liståŒ…è£¹çš„tensorï¼Œæ‰€ä»¥è¦å–ä¸¤æ¬¡[]***ï¼Œå°†å¯¹åº”GPUä¸Šçš„out_columnçš„è¿™ä¸ªchunkçš„grad append åˆ° out_gradè¿™ä¸ªlistå¯¹åº”çš„2çº§listä½ç½®é‡Œï¼Œout_gradä¹Ÿæ˜¯ä¸€ä¸ªä¸¤å±‚çš„listã€‚

â€‹	æ‰€ä»¥out_gradè¿™ä¸ªlisté‡Œæœ‰nä¸ªlistï¼Œå…¶ä¸­æ¯ä¸ªlistæ˜¯ä¸€ä¸ª(batch/n, chunk_size, out_size)ä¸ºå…ƒç´ çš„listï¼Œå…¶å†…å®¹æ˜¯åˆšåˆšbackwardå¾—åˆ°çš„gradã€‚

â€‹	å¾ªç¯å°±åˆ°è¿™é‡Œç»“æŸã€‚

7. æ‰€æœ‰**å¾ªç¯ç»“æŸä»¥å**ï¼Œè¿›è¡Œä¸‹é¢çš„ï¼š

```python
if self.opt is not None:
		out_grad = [torch.cat(og, dim=1) for og in out_grad]
    o1 = out
    o2 = nn.parallel.gather(out_grad, target_device=self.devices[0])
    o1.backward(gradient=o2)
    self.opt.step()
    self.opt.optimizer.zero_grad()
return total * normalize
```

â€‹	ç¬¬ä¸€æ­¥æŠŠout_gradä¸­æ¯ä¸ªäºŒçº§list(å³æ¯ä¸ªGPUä¸Šçš„)ä¸åŒchunkçš„gradæŒ‰dim=1ï¼Œä¹Ÿå°±æ˜¯è¡Œ(å› ä¸ºè¿™é‡Œæ˜¯3ç»´ï¼Œç¬¬ä¸€ä¸ªç»´åº¦æ˜¯batchç¬¬äºŒä¸ªæ˜¯è¡Œç¬¬ä¸‰ä¸ªæ˜¯åˆ—)ï¼Œæ‹¼æ¥èµ·æ¥ï¼Œå›åˆ°æ²¡åˆ†chunkä¹‹å‰çš„å½¢çŠ¶ã€‚æ­¤æ—¶out_gradæ˜¯ä¸€ä¸ªå…ƒç´ ä¸ºtensorçš„listã€‚

â€‹	æ‰€ä»¥gatherä¸åŒGPUä¸Šçš„gradå°±ç†æ‰€å½“ç„¶äº†ï¼Œæ³¨æ„è¿™ä¸ªgradæ˜¯***loss wrt. out***ï¼Œæ ¹æ®é“¾å¼æ³•åˆ™è¦ä¹˜ä»¥***out wrt. Parameter***ï¼Œæ‰€ä»¥ä¼šæœ‰`o1.backward(gradient=o2)`è¿™å¥è¯ï¼Œå°†o2è®¤ä¸ºæ˜¯æƒé‡å³å¯ã€‚

â€‹	ä¹‹åè¿›è¡Œä¸€ä¸ªä¼˜åŒ–stepå¹¶æ¸…é›¶æ¢¯åº¦ã€‚

â€‹	æœ€åè¿”å›loss * normalizeï¼Œä¹‹å‰é™¤ä»¥normalizeè¿™é‡Œå†ä¹˜ã€‚



