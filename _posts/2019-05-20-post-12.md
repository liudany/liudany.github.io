---
layout: post
title: Hierarchical Neural Story Generation
date: 2019-05-20 09:28:09
tags: [NLP]
categories: [NLP]

---

A paper from Facebook AI Research.

## Abstract

1. Collected a 300k dataset of human-written **stories paired with writing prompts**.
2. Hierarchy generation: **first generate a writing prompt(sentence), then transform it to text**.
3. **Model fusion** improves **relevance** of prompt and story.
4. Add a **gated multi-scale self-attention** mechanism to **model long context**.

## Overview

Conventional language model generates on a **strictly-word-by-word basis**, which can not explicitly make a high-level plan.

First, we generate the prompt/premise using **a convolutional language model.**

Second, we use **a convolutional seq2seq model** to generate a story that follows the premise.

## Gated multi-scale self-attention

CNN can only model a **bounded context windows**. To model long-range context, authors supplyment the **decoder** with **a self-attention machanism**.

⚠️only **decoder** is supplymented with self-attention.

![](/img/convselfattn.png)

### GLU

GLU(Gated Linear Unit) is a new **activation function**:
$$
h _ { l } ( \mathbf { X } ) = ( \mathbf { X } * \mathbf { W } + \mathbf { b } ) \otimes \sigma ( \mathbf { X } * \mathbf { V } + \mathbf { c } )
$$
The second term on RHS of the equation is the **Gate Unit**. And do **element-wise** multiplication like in LSTM.

### Gated attention

Authors use multi-head attention and the (Q, K, V) are generated **by deep NN with GLU **(not just linear projection) where input is hidden state.

### Multi-Scale Attention

The input to each head is **downsampled** a different amount. The first head sees the full input, the second every other input timestep, the third every third input timestep. This is so-called **multi-scale.**

## Model fusion

Model fusion was first proposed in paper "On Using Monolingual Corpora in Neural Machine Translation" to improve the performance of translation system.

![](/img/modelfusion.png)

Seq2seq model and language model were **independently pretrained** in shallow/deep fusion. 

Cold fusion was proposed in "Cold Fusion: Training Seq2Seq Models Together with Language Models" to imporve deep fusion by **training a seq2seq model from scratch together with a fixed pretrained language model**.

In this paper, authors train a seq2seq model that has access to a pretrained seq2seq model(on the same dataset), which reduces the problem seq2seq degenerating into a LM.
$$
\begin{aligned} g _ { t } & = \sigma \left( W \left[ h _ { t } ^ { \text { Training } } ; h _ { t } ^ { \text { Pretrained } } \right] + b \right) \\ h _ { t } & = g _ { t } \circ \left[ h _ { t } ^ { \text { Training } } ; h _ { t } ^ { \text { Pretrained } } \right] \end{aligned}
$$
 Here follows a diagram of fusion model:

![](/img/fusion.png)

- To improve over the pre-trained model, the second model must focus on the link between the prompt and the story.
- Doing so can be seen as a type of **boosting or residual** learning that allows the second model to **focus on what the first model failed to learn**—such as conditioning on the prompt.

- Use model **ensemble** as a baseline, which simply compute a weighted sum over different models.

![](/img/ensemble.png)