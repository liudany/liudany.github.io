---
layout: post
title: CS224n Notes 1： Word Embedding
date: 2018-10-15 09:28:09
tags: [NLP]
categories: [NLP]
typora-root-url: ../../static
---

# word embedding

representations of words.

## word meaning

一个词想表征出来的想法/物体。

最初使用WordNet等taxonomy(分类学)的方法，是一种语义网络，有着hypernym上位词，同义词等等，是一种离散化的表征。这种方法存在以下问题：

1. 这种方法missing nuances（细微差别），缺乏词语之间的关联和细微差别，且缺少新词，很难keep up to date。
2. discrete representation的问题是vector space是one-hot encoding，太长了。
3. 没给出词之间的内在联系。例如搜索中，有时要搜索相近的词，one-hot很难表征similarity。

改进想法是利用dense vector to reprensente word，利用dot计算来求相似性，dot作为一种简单的计算方式很适合衡量similarity。bigger if more similar.

利用**distribution similatiry**来计算一个词义，利用分布相似性！**通过一个词上下分布的词语来构建词语的意义。**词汇的意义，依赖于他的上下文，也可以说由上下文来决定。

## word2vec

Neural word embedding，是通过训练得到的representation，想法是让**词向量可以预测周围的词语**，整个训练过程也是通过中心词预测窗口内词语来进行的。

### skip-gram

#### model

每一步取一个词作为center word. predict一定窗口大小内context中的词语，调整词向量使得最大化上下文词语所出现的概率。

![](/img/skip-gram.jpeg)

1. 注意矩阵维度，one-hot编码的V维向量是每个词语的标签，V表示词表大小。而词向量是一个d维的列向量，表征词语的语义。
2. center word和context是两个不同的矩阵，维度是相似的。
3. centor word的词向量和context做dot product来计算similarity，得到的维度是V维，每个位置是该位置对应的词在context中出现的概率。
4. 假设词向量维度100，词表大小20,000，那么参数矩阵共有2000000个，但是每次我们更新的其实只有一个窗口内的词对应部分的向量，若窗口为5则每次我们其实只更新500个参数，其他位置都是0，所以是非常sparse的一个matrix。所以每次只更新出现的词的参数 就可以了。
5. 用非常小的数字初始化。

#### Remark

- Loss_func：其中T为center word数量，m为窗口大小，v为词表大小。
  $$
  \begin{array} { l } { J ( \theta ) = - \frac { 1 } { T } \log L ( \theta ) = - \frac { 1 } { T } \sum _ { t = 1 } ^ { T } \sum _ { - m \leq j \leq m } \log P \left( w _ { t + j } | w _ { t } ; \theta \right) } \\\\ { P ( o | w ) = \frac { \exp \left( u _ { w } ^ { T } v _ { c } \right) } { \sum _ { w = 1 } ^ { v } \exp \left( u _ { W } ^ { T } v _ { C } \right) } } \end{array}
  $$


- Negative Log Likelyhood: Log：乘法变成加法。前面加个1/T平均一下。Negative：大家往往喜欢最小化一个东西，所以我们在log后面加个负号。

- NLL和Cross-entropy的关系：**多分类问题中，instance标签为one-hot encoding，输出经过softmax层后，再做NLL就和cross entropy是等价的。**利用one-hot编码，只预测真正出现的这个词，corss-entropy各项中就只剩下要预测的这个词的负对数概率。

- 关于softmax：**number -> prob.** 指数是因为都转化成正的。归一化。为什么叫softmax，因为**取指数操作基本上就是保留最大的那个了**，跟取最大值类似。但是是个软性的。 

- 这里的softmax分母部分是整个context矩阵与centor word的dot product，因为要预测词表中每一个词出现的概率，而不仅仅是窗口中的这几个词。

- **2 vectors for each word, centor and context.**

- 不关心词出现的位置，只要在窗口之内的算概率就完事儿了。

- Stochastic gradient descent：用SGD是避免整个epoch求梯度浪费太久，看起来不靠谱but works like a gem。

#### negative sampleing

上面的loss function在计算时分母运算量很大，每个window更新一次，所以下一次窗口移动后必须要重新计算所有的dot product的和，因此改为按P(w)来随机抽取k个负样本做运算。

$$
J _ { t } ( \theta ) = \log \sigma \left( u _ { o } ^ { T } v _ { c } \right) + \sum _ { j \sim P ( w ) } \left[ \log \sigma \left( - u _ { j } ^ { T } v _ { c } \right) \right]
$$


上面是第T个时间步或者说第T个窗口的损失，注意正负号，第一项为real outside word出现的概率，第二项为随机抽取的负样本出现的概率。注意正负号，最大化第一项，最小化第二项。

### The continous bag of words model(CBOW)

从周围**词向量的和**去预测中心词，而不是通过中心词去预测周围单个词出现的概率（Skig-gram）。

![](/img/cbow-skip-gram.png)

### Co-occurrence matrix based 基于共现矩阵

首先是一个对称阵，你出现在我旁边我就出现在你旁边。直接用这个矩阵的行列作为词向量也行，但是有不足之处：

- 新词的加入会对其他词产生影响，都要改变。
- 维度为词表大小，太大了。

所以需要进行降维，但是依然要保留重要信息，通常维持在25-10000维。降低维度的方法是**SVD分解**。

Hack we can do：

- 有一些词语产生一些噪声，比如the, he, has没有实际的含义，所以我们把他们在共现矩阵中的值设置一个上限，例如100。或者直接无视掉它们。
- 根据离中心词的距离，设置不同的权重。例如紧挨着中心词，在共现矩阵中记为1，离着3步以上记为0.5等。
- 计算词相关性(Pearson correlations)代替单纯的记数，设置副样本值为0。

### count-based vs window-based

- count-based更倾向于词的相似性，计数量大的词得到了不应有的重要性（有一些功能词）。
- window-based遍历语料效率低（scale with corpus size缩放语料），没有用到语料的全局统计信息。
- 实际证明skig-gram是比较稳的。

结合两者，得到了GloVe(Global Vector)。

### GloVe

结合计数和窗口的优点，将目光放在词语出现在全局的次数，并不像skip-gram一样走窗口的模式，将损失函数优化为：

$$
J ( \theta ) = \frac { 1 } { 2 } \sum _ { i , j = 1 } ^ { W } f \left( P _ { i j } \right) \left( u _ { i } ^ { T } v _ { j } - \log P _ { i j } \right) ^ { 2 }
$$


其中P是共现矩阵，u和v分别是词的行向量和列向量，W是词表大小。最终的词向量x=u+v，因为这两个向量都包含了信息。

即使在skig-gram中也会把context vector和centor vector加起来作为最终的词向量，这是因为**实践得出的效果好**。

## 多义词

一个词有很多意思，用很多意思的线性叠加作为最终的词向量。

使用sparse coding方法线性叠加context vectors，可以恢复出多义词不同的语义。

## evaluate word vectors

NLP的evalutaion一般分为两种，**intrinsic和extrinsic**。Intrinsic是着眼于某个中间目标，或者子任务的结果，比如vector differences or similarities。Extrinsic着眼于一个高层级的目标，直接看最终结果，比如词向量直接用于翻译任务中看BLEU。

### intrinsic evaluation

#### Word Vector Analogies

word2vec提出的**Word Vector Analogies**词向量类比方法，词向量之间做加减法，再用余弦相似度捕捉相似度最大的词看看准确与否。

Man和Woman的向量差，和King与Queen的向量差是类似的，Slow/Slower和Short/Shorter也是类似的等等。已经有数据集了，是四元组的形势（CITY1-COUNTRY1-CITY2-COUNTRY2），可以下载。

已有的结果表明，1000维度的向量并不会比300维的更能播捉相似性（在WVA任务中取得更高分）。但可以肯定的是数据越多表现越好！窗口大小为8比较适合GloVe。

#### Word Vector Correlation

人工评估了一组词汇的相似度，然后用cosine相似度去衡量你的词向量是否表现出相同的特征。也可以下载了，WordSim353。

### extrinsic evaluation

选择不同的下游任务(downstream task)，命名实体识别是个不错的选择。