---

layout: post
title: 《深度学习框架：PyTorch》学习笔记（4）nn
date: 2018-07-07 09:28:09
tags: [PyTorch]
categories: [PyTorch]

---
PyTorch的nn常用法为：继承nn.Module类，扩展为自己的网络/层。

# 常见的神经网络

## FC全连接层

到这还是满脑子线性回归……这里要把思路调整到正常的神经网络了，输入为**features \* m**的矩阵。

徒手写个全连接层试试：
```python
class Linear(nn.Module):
	def __init__(self, in_features, out_features):
		super(Linear, self).__init__()
		self.w = nn.Parameter(t.randn(out_features, in_features))
		self.b = nn.Parameter(t.randn(out_features, 1))

	def forward(self, x):
		return (self.w).mm(x) + self.b
```

这样是实现了某种网络层，再徒手写一个三层感知机：
```python
class Perceptron(nn.Module):
	def __init__(self, in_features, hidden_features, out_features):
		super(Perceptron, self).__init__()
		self.layer1 = Linear(in_features, hidden_features)
		self.layer2 = Linear(hidden_features, out_features)

	def forward(self, x):
		x = self.layer1(x)
		x = t.sigmoid(x)
		x = self.layer2(x)
		return x
```

实现一个(3, 4, 1)的感知机实例：
```python
perceptron = Perceptron(3, 4, 1)
x = V(t.randn(3, 5))
output = perceptron(x)
```

这里面要注意的地方：
1. 无论是实现某种层还是一个网络，定义该类的时候要**继承nn.Module**类。
2. 类内首先定义构造函数**def __init__(self, params)**，这里的参数是整个网络或者层的特性，并不包含输入。
3. 构造函数内首先执行父类构造函数，方法有**super(name, self).__init()**或者**nn.Module.__init__(self)**。
4. 构造函数内部，定义所有**可学习的参数**或**网络中包含的层**，注意变量前要加self，例如**self.w / self.layer1**。这里其实类似搭建静态图的过程，但不涉及运算。
5. 定义前向传播函数**def forward(self, x)**，参数为输入。注意构造函数的参数中并没有输入参数x。
6. 在forward中调用构造函数中的一些参数注意加**self.**。
7. 实现时，网络的输入要为Variable类型，requires_grad不必置位。
8. 前向传播时，用网络的实例直接加输入就可以，例如**perceptron(x)**，并不用指定调用其中的前向传播函数。
9. 在Module中定义参数的时候用**nn.Parameter(tensor)**。

## CNN和池化层

举例一个锐化卷积：
```python
input = to_tensor(img).unsqueeze(0)

kernel = t.ones(3, 3)/-9
kernel[1][1] = 1
conv = nn.Conv2d(1, 1, (3, 3), 1, bias = False)
conv.weight.data = kernel.view(1, 1, 3, 3)

out = conv(V(input))
to_pil = (out.data.squeeze(0))
```

1. 网络的输入input必须为batch，将一个样本用**unsqueeze(0)**的方法扩展为(1, 1, 200, 200)的维度，第一个维度为batch_size。PyTorch的网络直支持batch形式的输入。
2. 这个卷积核据说是个锐化卷积。
3. `nn.Conv2d(input_channels, output_channels, kernel_size, stride, padding, bias)`
4. kernel是conv的一个属性，是一个Tensor数据，维度为(out_channels, in_channels, (kernel_size))。不写也可以的，上一句中已经定义了这些参数。
5. 输出也是batch数据，要squeeze一下再画出来。

池化层的例子：
```python
pool = nn.AvgPool2d(2, 2)
list(pool.parameters())
out = pool(V(input))
to_pil(out.data.squeeze(0))
```

注意**.parameters()**方法将会返回网络中所有可学习的参数，这里平均池化层没有可学习参数。

## 其他

### 全连接层 Linear

这里要注意PyTorch里定义的输入形式与Ng讲的有出入：
```python
linear = nn.Linear(3, 4)
input = V(t.randn(2, 3))
output = linear(input)
```

1. `nn.Linear(input_features, output_features)`，第一个参数为上一层神经元数量，第二个为本层神经元数量。
2. 特别注意input数据的形状为**(batch_size, in_features)**，每一行为一个数据，适应一下。

## BatchNormalization

先回顾一下批标准化的意义，每层的输出在经过激活函数之后，例如tanh，会映射到(-1, 1)的分布上，这样20和20000的输出在经过tanh后的差异其实并不大的，为了加强对过大过小数据的敏感性，所以我们在每层网络和激活函数之间加上批标准化层。
## DropOut

随机失活，加快训练，参数为失活的概率。
```python
dropout = nn.Dropout(0.5)
input = dropout(input)
```

## Activation

PyTorch将常见的激活函数例如ReLU等也定义为层，可选一个`inplace = True`的参数，节省显存，不会丧失梯度，因为可以根据输出来求出。

# 复杂网络的其他定义方式

除了继承Module类然后实现forward方法之外，还有其他方法来定义一个网络，比如`sequential`和`modulelist`。
## Sequential

举个例子：
```python
net1 = nn.Sequential()
net1.add_module('conv', nn.Conv2d(3, 3, 3))
net1.add_module('bn', nn.BatchNorm2d(3))
net1.add_module('activation', nn.ReLU())

net2 = nn.Sequential(
	nn.Conv2d(3, 3, 3),
	nn.BatchNorm1d(3),
	nn.ReLU()
	)

input = V(t.randn(1, 3, 4, 4))
output = net1(input)
```
注意net已经是Sequential的一个实例，这里不需要像继承Module类一样在此进行实例化，直接net1(input)即可。

## ModuleList

可以像List一样使用Modulist，用法为`nn.ModuleList([nn.Conv2d(3, 3, 3), nn.ReLU()])`，这个用法有点迷。留个坑后面填！

# Loss Function

PyTorch将损失函数也实现为nn.Module的子类，使用时也是先实例化，一个交叉熵的例子：
```python
score = V(t.randn(3, 2))
label = V(t.Tensor([1, 0, 1])).long()
criterion = nn.CrossEntropyLoss()
loss = criterion(score, label)
```
要注意**label必须是LongTensor类型**。

# Optimizer

这里先用之前讲的Sequential方法实现一个第一章中实现过的LeNET网络：
```python
class Net(nn.Module):
	def __init__(self):
		super(Net, self).__init__()
		self.features = nn.Sequential(
			nn.Conv2d(3, 6, 5),
			nn.ReLU(),
			nn.MaxPool2d(2, 2),
			nn.Conv2d(6, 16, 6),
			nn.ReLU(),
			nn.MaxPool2d(2, 2)
		)
		self.classifier = nn.Sequential(
			nn.Linear(16 * 5 * 5, 120),
			nn.ReLU(),
			nn.Linear(120, 84),
			nn.ReLU(),
			nn.Linear(84, 10)
		)
	def forward(self, x):
		x = self.features(x)
		x = x.view(-1, 16 * 5 * 5)
		x = self.classifier(x)
		return x

net = Net()
```

注意Sequential方法会依次将数据通过所定义的各层网络，然后定义优化器：
```python
from torch import optim
optimizer = optim.SGD(params = net.parameters(), lr =1)
optimizer.zero_grad()

input = V(t.randn(1, 3, 32, 32))
output = net(input)
output.backward(output) # fake backward

optimizer.step()
```
一般来说优化器配合损失函数后使用，先将损失函数反向传播，使得每个叶子结点梯度保存，然后再进行一次优化步。若要对不同的子模块指定不同的参数，方法为：
```python
optimizer = optim.SGD([{'params': 'net.features.parameters()'}, {'params': 'net.classifier.parameters()', 'lr': 0.01}], lr = 0.001)
```
即将第优化器的第一个参数改为一个tuple，其中的元素为dict，不特别指定lr的用默认学习率。

想要调整学习率只要新建一个optimizer就可以了，因为optimizer十分轻量级，构建开销很小。

# nn.functional

这里其实和nn.Module重合了很多部分，Module可以自动提取可以学习的参数，而Function更像是纯函数。这本书的作者建议在有可学习参数的场合用Module，例如卷积、全联接等，而类似池化层和激活层就用Functional。目前我个人觉得都用Module好像更直白一点。

# 初始化策略 nn.init

在我们执行`nn.Linear(3, 6)`时已经进行了较为合理的参数初始化，一般是不用我们自己考虑的。当我们想自己来定义一些参数初始化方式时，注意不要直接用`rand`类函数，极大值可能会导致梯度爆炸和梯度消失。

`nn.init`模块专门为初始化设计，方法为先实例化网络，再用init初始化其参数：
```python
linear = nn.Linear(3, 4)
nn.init.xavier_normal(linear.weight)
```
# 深入讨论

这里作者讲了很多Module类的构造函数的原理，挑几个重要的记下来：
1. 有很多在训练和测试阶段行为差距较大的层，如Dropout，BatchNorm，训练的时候应该是开启的，但是测试时应该关闭，所以要用`model.train()`函数和`model.eval()`函数来设置training属性，前者全True，后者全False。
2. 保存模型的方法为：`t.save(net.state_dict(), 'net.pth')`，载入模型的方法为：先实例化一个网络，然后导入参数`net2.load_state_dict(t.load('net.pth'))`。
3. 模型搬到GPU运行的方法：`model = model.cuda(), input = input.cuda()`。
4. GPU并行计算的方法：`nn.parallel.data_parallel(module, inputs, device_ids, output_device, dim, module_kwargs)`。