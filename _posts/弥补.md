# 弥补VAE

## CNN based VAE

## RNN based VAE

## Transformer based VAE





### 回家前

代码工作开始，看一些vae工作

ucl和imperial的rp修改好

给老师们发邮件

瘦 早睡早起好好吃饭

你要买什么东西

# 申请

## CNRS

### **项目介绍：**

文本输出的可解释模型。这个phd论文的宽泛目标是，提供一种可解释的文本生成模型，其允许识别输入和输出中错误匹配的部分。两种Text Production可选：知识库的语言化，和文本摘要。对于这二者而言，输出文本应该与输入相匹配。对摘要而言，应该保留输入中的关键信息。所以，有很多不同的问题待解决，例如如何分析语义是否妥当，以及如何解释一个生成系统的行为。

###**Expected Results:**

1. 文本生成模型，其可以解析那些失败的例子中，是为什么没有达成语义的妥当性的；第二，可以区分出那些，由于数据的偏差而失败的例子，和那些由于模型本身存在问题的例子。

2. 在标准的KBV和摘要数据集上对其进行测验。

3. 一个解释模型，其基于：将端到端的解码过程分为一系列可解释的子步骤，并基于中间子步骤的输出结果和原始的输入，一起决定最后的输出。第二是，一些评估指标，用于测试这些中间的步骤到底表示什么意思，并且他们如何在模型中作出贡献。

挑战在于如何确认相关的子步骤，和为这些步骤在文本生成中的贡献设计评估指标。我们会解码自然语言生成到很多传统NLG模块，其允许一个更加犀利度的评估，评估这些NLG系统面对文本生成任务重的多种选择时如何抉择。

 ### 实验室信息

PhD将在法国Nancy的LORIA实验室，洛林大学。LORIA是CNRA，洛林大学和INRIA共同创办的实验室，聚集了大概500人和27个团队，分为5个部分，同时注重计算机的基础和应用研究。NLKP部分包括5个团队，其中Synalp组就是对口这个phd职位的，其专业搞统计和符号语言处理，注重神经方法的语言生成。SYNALP在本地和国际化研究都有重要地位，ClaireGardent活跃在顶级的国际组织，例如ACL成员，EACL的主席，同时也是SIGGEN的主席（ACL在生成领域的分会）。她指导过16个博士学生，5个博士后和8个工程师，她已经是10个重大项目的参与者。她曾做过所有主要NLP会议的主席和审稿人，也是NLP主要期刊的审稿人或者是主席之一。

## 哥本哈根 2/13

- Cover/motivation letter.
- Project description, incl. ethics assessment form.
- CV, incl. publications (and the names and contact details of two referees: one should be from the master thesis supervisor (if applicable), or other senior scientist that the applicant has worked closely with).
- Declaration of mobility.
- Documentation of academic degrees obtained: transcripts and diplomas must be in English or an officially approved translation must be included.
- Copy of the required score from one of the following English tests (for non-native English speakers, cf. requirement regarding English skills). All pages of the test must be submitted.

模板下载下来了，填写。

网站https://employment.ku.dk/phd/?show=150920

## CNRS 2/14

玛丽居里，网址 https://nl4xai.eu/vacancies/esr6/

起始月份4/1，deadline 2.14号

前辈 Chunyang https://www.linkedin.com/in/chunyang-xiao-2550382b/

## McGill 3/1 

说financial assistant已经过了1/1？

https://www.cs.mcgill.ca/academic/graduate/applying/

## Edinburgh

Scholarship申请

发邮件问Frank Edinburgh面试准备什么以提高成功率，另外Data Science & AI[这个项目](https://www.ed.ac.uk/studying/postgraduate/degrees/index.php?r=site/view&id=858)是否也可以申请

问commitee scholarship是否需要另外单独申请，以及后续时间大致安排✅

International Global Phd Scholarships deadline 2/7 notification 3/27

写一个自我陈述，为什么要这个奖金

RA

https://www.vacancies.ed.ac.uk/pls/corehrrecruit/erq_jobspec_version_4.display_form

## UCL & Imperial

UCL没消息就重新申请一遍吧。

这俩的推荐信

Generally speaking, This model extends the BERT model to multimodality setting. The core ideas of this model is cross-modality attention and pre-train strategy. I think in this paper, the cross-modality layer was inspired by third sublayer of transformer decoder. As we know, there are three sublayers in a transformer decoder, self-attention layer, encoder-deocder attention layer and the ff layer. In this paper the cross modality layer works like the encoder-decoder layer which connects the source language and the target langauge. The cross-modality layer extends this setting to connect the vision side and language side.

To be honest, this idea looks like some incremental ideas. I have ever thought about this idead before.

And the second contribution of this paper is to design a lot of task to pre-train the multimodal encoder. This also works like the BERT pre-train but it was extended to not only language tasks but also vision and multimodal tasks such like object prediction and vision-question answering.

I think one of the potential direction will be extending the multimodal representation to image sequence setting. 

This is an unexplored research area in multimodal machine learning. We could proposed new method, new dataset and new task.

For example, in visual storytelling model, exsiting methods all use representation for single image and generate a sequence of sentences. I think we should consider the representation for image sequence , which not only contains the single image features, but also contains the sequence information of a series of  images.



About two year ago, I started following your webpage of sheffield and imperial. Being your phd student and working with you is one of my dreams. So it's really an honor for me to have this interview and I'm really excited now.

## Amsterdam

发邮件继续问问，加上自己新发的论文。

有ILLC的职位了

有hybrid AI的职位了

### Monz

 In interactive explanation generation human agents can ask not just for explanations but also pose scenario-based questions, such as `what if’-scenarios? By how much could the input be modified without the final result changing (scenario robustness)? What would, be the best option if the proposed option is not possible due to circumstances not known to the system (alternative scenarios)?

在交互可解释行生成人类代理可以提问不只是可解释行，同时还有基于场景的问题，例如假设场景。

输入可以修改多少，而输出却不改变？这叫场景鲁棒性。



![image-20200203184338083](/Users/danyliu/Library/Application Support/typora-user-images/image-20200203184338083.png)

代理之间的交流服从于可解释性。我们早起的研究在多源翻译，展示了编码信息的丰富性直接影响语言生成质量。在早期工作中，我们首先追踪翻译模型中出现的错误，通过分析注意力分布可视化句子翻译之间的关联性。虽然这种方法可以辨认出错误的来源，但是这只可以确定哪些单词出错了，而不能解释为什么这些词语出错了活着这种出错的内在原因。为了解决这个问题，我们将建立sharchilev的方法，并且开发一个新的框架来产生自然语言解释，用来更加详细的描述产生当下预测的步骤。

## Melbourne

Ongoing 2.17出结果

## KU LEUVEN

重新申请

## Uppsala

https://www.uu.se/en/about-uu/join-us/details/?positionId=315943

# 待办事项

发邮件问Frank Edinburgh的时间安排和自己的准备。

发邮件问Amsterdam的事宜。

UCL和Imperial材料修改，deadline和严老师推荐信。

准备面试和英语。

哥本哈根材料书写。

AAAI Poster。

AAAI 酒店机票，开会日期2.7-12号。 2.10 9:30-10:45

Transformer-based VAE 练coding。

Character-centric 可以拖，就说holiday leave。

