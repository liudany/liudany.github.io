---
layout: post
title: Language Modeling with Gated Convolutional Networks
date: 2019-05-21 09:28:09
tags: [NLP]
categories: [NLP]
typora-root-url: ../../static
---

## Abstract

1. **Finite context** approach through **stacked convolutions**, which allows **parallelization**.
2. **Gating mechanism**.

## Approach

Conv has no temporal dependecies(时间依赖性), making parallel possible. But the context size is **finite**.

1. Before convolving inputs, we **shift the input sequence by zero-padding** the begging with k-1 elements(kernel size is k). Therefore hidden state(hi) does not contain information from future words.
2. Convolve
3. The output of each layer is modulated(调节)(**element-wise multiply**) by gates $\sigma ( \mathbf { X } * \mathbf { V } + \mathbf { c } )$, which looks like what LSTM does. **This is dubbed GLU.**
4. Warp conv and GLU win a **residual block**.
5. Choose an improvement of hierarchical softmax known as **adaptive softmax**.


![](/img/convlm.png)

## Gated Mechanism

Compared with RNN, **there is no gradient vanishing problem**. Therefore we can use only the output gate to control what information should be propagated through layers.

## Setup

- Using gradient clipping(and argue that GC is not only for RNN)
- Weight normalization
- Kaiming initialization
- Learning rate is sampled uniformly in [1, 2]

## Context Size

40 is a magic number.

Larger contexts improve accuracy but **returns(回报) drastically diminish** with windows larger than 40 words.

This is congruent(一致) with the fact that RNN obtains good performance by **truncating gradients** after only 40 timesteps using **truncated BPTT(截断式BPTT)**.