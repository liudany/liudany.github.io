---
layout: post
title: Generating Sentences from a Continuous Space
date: 2019-05-16 09:28:09
tags: [NLP]
categories: [NLP]
---

## The variational auto encoder

作者认为VAE是一个**带正则的普通DAE**，这个正则就是将hidden code z强制到一个确定的先验分布上，即损失中的KL损失项。

直观来讲，对于DAE来讲把X映射到一个确定的点，而直观来讲**VAE是把X映射到一个光滑连续的椭球面区域（通过一个点和方差来实现），之后通过KL项让这些区域充满整个球。**

如果在VAE中去掉Loss中的KL项只保留重构误差，那么模型会迫使方差到0（偷懒），然后又变成了一个DAE。

⚠️**这里顺便比较一下VAE和DAE结构（实现）上的不同：只有两点，一个是encoder之后的sample操作，另一个是损失函数中多了KL项，decoder的地方是并没有不同的。**

## VAE for sentences

结合LSTM，这时VAE的decoder可以认为是一个**conditional language model**，其条件就是hidden code。

**所以在hidden code没有有效的incorporate information的时，模型degenerate为普通LM。**

作者尝试过很多种利用z的不同方法，例如：

- 在decoder的每一个时间步上concat z
- 在encoder和z和decoder之间都加入深度NN
- softplus parametrization for variance。

发现对于**结果并无显著的差别**，但是作者选择在第二种情况下，各使用4层highway network。注意第三种说的softplus可使结果非负，公式为：
$$
\zeta ( x ) = \log ( 1 + \exp ( x ) )
$$
作者们也尝试了把encoder处变得更加精密复杂，例如多步采样等，发现也**不能提高效果**。

### 优化中的问题

我们的模型目的在于学习句子的全局表征，学的怎么样可以通过变分下界/ELBO/Loss（同一个东西）来看。

能编码有效信息的模型要满足：

- KL项非零
- 相对较小的重构误差

但是LSTM-VAE不能直接学到这些特点，**大部分的训练中出现KL项都一直为0的现象。**这意味着模型退化为普通LM，即z中没有encode有效信息。

更麻烦的是LSTM decoder对于z的方差变化的敏感性，因为随机的方差**导致z一直在变化**，所以干脆忽视z，使得任务退化为简单的LM，这样优化任务会变得更加简单。此时Decoder和Encoder之间的反向传播信噪比非常低，不能有效的更新encoder，使得模型进入一种我们不愿看到的稳定状态，即KL坍塌到0。

作者提到了在图像领域的vae中，有人**用weaker一些的decoder，迫使decoder必须要利用hidden code的信息。**

作者提出了两种方法来改进：

- KL cost annealing：退火算法，给损失中的KL项加一个系数，从0开始慢慢增大到1，这使得模型在开始阶段更关注于怎么在z中编码更多的有效信息，之后再将分散的面强制到先验空间内。
- Word drop out：对decoder而言的，一般而言都是teacher forcing的，用ground truth，为了weaker这个decoder我们把这个ground truth中都drop out一些，丢失一些信息量，这样可以decoder更依赖于z。



# KL-vanishing

下面是关于KL vanishing的一个[讨论](https://weiwenku.net/d/104863198#tuit)。

## 产生原因

最大化ELBO的目标，等价于**最大化重构并最小化KL项**，分开来看：

- 使得data x和latent z相互独立，并让P(z|X)等于先验P(z)，这样KL就等于0了。
- 当decoder足够强大，可以不利用隐向量z中的信息，本身就足够model样本X的分布，可以不依赖z。

所以解决方法也分为针对KL项和针对重构过程这两部分。

## 针对KL

这一部分也可以说，怎么让z中包括更多x的信息，二者更加相关。

- KL cost annealing：即上面讲到的，刚开始的时候系数比较小，encoder更关注于把x的信息encode到z中。通常搭配word-drop-out使用。
- Free Bites：针对维度来做的，如果这一维度KL值太小，不管它，等他超过一定阈值之后再计入loss优化。
- Normalizing flow：《Variational lossy autoencoder》. ICLR 2017.
- Auxiliary Autoencoder：《Improving Variational Encoder-Decoders in Dialogue Generation》. AAAI 2018.

## 针对Reconstruction

- word drop out
- CNN decoder：避免autoregreesive结构。
- Additional Loss：《Learning discourse-level diversity for neural dialog models using conditional variational autoencoders》. ACL 2017.

## VAE, DAE, LM, CLM

VAE与DAE结构上相比，只在hidden code生成上有所不同，一个是deterministirc的一个是sample。

都针对LSTM结构文本任务而言，LM的训练过程是预测下一词；文本任务的DAE训练过程是根据一个确定的latent code来一步一步预测下一词，恢复原句，是一种条件语言模型CLM；VAE的训练过程也是根据一个latent code来恢复原句，与DAE相比只是这个latent code的生成方法不一样而已（损失也不同），也是CLM。

综上，基于LSTM网络文本任务的AE都是一种conditional language model，只是z生成方式不同。