---

layout: post
title: Ganerative Adversarial Network Tutorial
date: 2019-05-26 10:28:09
tags: [NLP]
categories: [NLP]
typora-root-url: ../../static
---

From Hung-yi Lee 2018.

# 1 Introduction

## Basic Idea

GAN的目标也是训练出一个Generator，输入一个随机的向量（特征向量），生成一个样本。

但是也有一个Discriminator，输入生成的样本，输出一个scalar，代表quality，越大越真实。

二者是一起进步的：生成器生成了假样本，判别器根据生成的样本和dataset之间的样本比较相似度。

## Algorithm

![](/img/ganalgo.png)

不是一步接一步的，可以固定G训练N次D，然后固定D训练N次G。

## 为什么不让Generator直接训练

如果要这么做，首先要给dataset中每个样本assign一个code，然后G根据code生成对应的样本。

这种做法是类似普通AE的，问题在于对于生成新样本而言，采样的空间不是连续的，不可以随意从一个先验里面采样。这也是VAE出现的原因，在code上加上一些噪音的影响，使得code布满一个prior空间。

如果不用Discriminator来判断相似度，而用简单的欧拉距离来判断，是不准确的（例如平移）。

且Generator自己很难在有限的层数判断不同维度之间的相关度。

## 为什么不用Discriminator生成

这种打分的机制是可以用来生成新图片的，随机生成一个样本，用D打分，找高分就可以了。

但是怎么训练它呢，我们手里只有正面样本，这样的问题是D在训练过程中一直打高分，所以后面无论输入什么都倾向于打高分。

所以问题在于怎么找假样本，如果用随机噪声的话，这个样本也太差了，会使得D的要求特别低。只有用质量非常高的negative example才能让D变得更加强大。

那可以从随机噪声开始，用穷举的方法迭代D本身，用上一轮D认为是好的样本，做下一轮的假样本。

![](/img/gandis.png)

在Graphical Model中，采用的训练思想跟上面是一样的，整个graph就是一个discriminator。

## 比较G和D

![](/img/gvsd.png)

Generator可以生成，但考虑不到大局。D可以从大局考虑图片质量，但是生成很累。

将D单独训练中的找negative example这个过程，变成了用G来生成当前D认为的positive example。

![](/img/benifit.png) 

G使得采假样本这个过程变得简单，D使得在计算loss时可以更加从大局的角度入手。

# Conditional Generation

![](/img/cgan.png)

注意D的输入也要有类别c的信息，不然G会偷懒只生成一个高质量的图片，而不管是什么分类，因为这样就足够从D那里取得高分了。

负样本的输入也与普通GAN不同，不能只有假图片，同时还要有不同的标签来表征类别之间的不同。因为D是要全局的学习condition和example之间的关系的，判断两件事：图片真不真，图片符合不符合分类。

算法流程：

![](/img/cganalgo.png)

注意D中损失函数的变化，第二项是图片不清楚的，第三项是图文不符的。

CGAN的网络结构：

![](/img/cgandis.png)

第一种方法是结合两种evaluation的综合分数，第二个是两种分数分开叙述。看起来第二种更具体一些。

第一种会在低分时confuse到底是图文不符，还是图片不清晰。

但是还是第一种用的多，算是一种端到端的简化。

## Stack GAN

先生成小的图，再产生大的图，还用了点variational的东西。

![](/img/stackgan.png)

## Patch GAN

一种image-to-image的变化，以图为条件，去改变风格质，这里的Discriminator每次只检查一个区域。

![](/img/imagetoimage.png)

这里最后一个加close的意思是，我们之前使用discriminator的时候会忽略了生成的假样本和目标图片之间的直接关系，而是以(c, x)(c, x假)(c假, x)这样的形式去让D判断，丧失了x和(x假)之间的关系。

## 总结

所以看起来，所有端到端的supervise训练过程都可以用CGAN来实现，让生成的东西更加精确清晰。



## GAN for Sequence

为什么在Seq2seq里面要用GAN，因为**评价标准**，例如chat-bot，how are you，标准答案I'm fine。

如果按照Maximize likelihood的角度看，I'm John是个比Not bad更好的答案，与gold相似的地方更多。

这就是问题。

![image-20190706104203448](/../../Library/Application Support/typora-user-images/image-20190706104203448.png)



![image-20190706105401126](/../../Library/Application Support/typora-user-images/image-20190706105401126.png)

Update参数这一步很重要。

![image-20190706105650943](/../../Library/Application Support/typora-user-images/image-20190706105650943.png)

与普通的最大化ground truth样本出现的可能的区别是，每个生成样本都给一个reward，weighted sum。

### 引入GAN

在RL里面，feedback 也就是这个reward是人给的（游戏场景下比较明显）。

但是在GAN里面，feedback是discriminator给的。



问题是，取词的过程有sample的过程，这个阶段是不可微分的。

![image-20190706115350619](/../../Library/Application Support/typora-user-images/image-20190706115350619.png)

三种办法解决，用一个特殊的softmax，或者使用sample前的连续vector，或者RL。

第二种方法问题是gold样本标签都是one-hot，我们的distribution都不是，这样D很容易作弊，G也会学坏。

所以RL。

### RL

把R(c, x)换成用discriminator来算，D(c, x)。

![image-20190706115728841](/../../Library/Application Support/typora-user-images/image-20190706115728841.png)

也要G，D分步交替反复进行。



给Reward的时候总的给是不合适的，因为可能第一个词是对的。

how are you? i am John 第一个词是对的。

解决方法是给每一步都打分，可用蒙特卡洛搜索，但是计算量很大。



所以应该是把Policy Gradient用在GAN方式的训练之中。