# 准备阶段

## 完成毕业论文

做实验和写论文怎么均衡。

## 目前问题

### 这个一个Language Model问题，还是Encoder-Deocder问题

首先是一个**Autoencoder**，是一个**重构**问题，所以是Encoder-Decoder问题的。VAE可以用LM的数据集来训练，因为不需要平行语料库。说Language Model主要原因是**Decoder without attention很像Language Model**，以及**test阶段没有encode过程，sample直接decode也类似LM**。

### 找Seq2seq Framework和Transformer Decoder (Encoder)

主要是区别是**decoder中没有attention**（如果有会怎样？），这一点可以**从Tranformer LM中去找**。Test阶段没有encode的一步。而为了实验的完整性，要找一个框架性非常好的Seq2seq框架，方便更换各种encoder/decoder结构。

#### Transformer Decoder

1. 看一下timo BERT的transformer用法，因为它基于Harvard的写的，这个之前研究过。
2. Huggingface的有点太庞大了，不方便看，留在后面去读。

#### Sequence2sequence



## 论文写作

### 词向量

### 神经元以及感知机✅

### RNN/LSTM ✅

补充GRU

### CNN图

### Seq2seq✅

### Transformer ✅5页

### 自动编码器✅

优势可以整理一下，预训练，降维。

### VAE✅ 6页

### RNN-VAE

优化器，teacher forcing，

### Transformer-VAE 5页

### 实验中





### 周日 15

今天写完Seq2Seq的介绍，捋好autoencoder的效果。达到30页。

### 周一 16

开始VAE for Sentence，从RNN到Transformer讲清楚原理，实验和结果。最好达到40页。

### 周二 17

Shelly文章翻译并全部放上来，最好达到50页。

### 周三 18

总结与展望，CNN等最后的小修改，完成最终版。

北大报名截止。

### 周四 19

看医生，拔牙。

### 周五 20

下周一演讲的PPT做好了，还有讲稿。

### 周六 21

### 周日 22

### 周一 23

讲论文PPT。







## 论文问题

### 结构

第三章题目，体现draft edit。✅

标题和第一节加文字。✅

实验与分析。✅

展望加一些，写满最后一页。✅

### 扉页

学院，电子信息与电气工程学院。✅

答辩日期，2020年1月。✅

### 摘要

汉语保持在一页。英语可以多点。✅

领域->任务，新闻稿件生成去掉，换成小说生成。聊天机器人换掉。✅

初稿模型和修改模型，改成一个模型。体现出Transformer模型的修改的地方和VAE。说出原有的无法结合。✅

第二段应该最丰满，模型和其中最精髓点，都写进去。第三段压缩一些。提升明显的话数据可以写进去。✅

### 主要符号对照表

删掉✅

### 绪论

研究内容和意义，删掉。✅

本文组织结构，补充完整。✅

### 第二章

和2.1之间写一个承接段。✅

sigmoid和relu，并在一起。✅

图中字号要比正文的小一些。✅

图都压一压。✅

### 第三章

承接段提出原来的结构不能做vae句子生成，本文做到了。✅

### 第四章

承接段。✅

是条件变分，这个名字全都改一下。✅

初稿-修改机制，全部改成多轮修改机制。✅

### 第五章

承接段，标题之间加点字。✅

大表分开，标题应该在上面。✅

表没分开，就这样吧

### 学位论文

参考文献13，15，20，凑参考文献，总共50！✅

### 全局

中英文标题✅

GRU写不写

绪论里面加当前生成领域文章多的现状。

英文摘要

表格位置调整

英文封面✅





第一章加上本文主要工作

国内外现状这一节再扩一下，第一章到6-8页

第四章扩一下

首次两个字✅

参考文献作主语✅



第四章：GRU 故事 加的图 都再介绍一下 加小标题！



等间隔！这个词 等距离



### 互读修改

统一Tranformer网络✅

修改一下翻译的abstract❌

插图标题[]{}简单复杂✅

图片中的文字都用中文❌

我们-模型 或者删掉✅

所有的公式后面的“其中”都不要重新开始一行✅



扩充每一章节的本章小结 【这个最后来】✅

### 第二章修改

扩充Transformer-based vae中自己的内容，好好介绍为什么删掉第二层，引入BERT✅

2 3 章结构调整

### 第三章修改

问题构成这里，重新写一遍，从不同的视角，从他们的insight来写✅

GRU与与长短期记忆网络的比较 多写一点！✅

首先加一段标题和小标题之间的介绍。✅

然后介绍general的层级网络。✅

然后介绍具体的实现过程。✅

整个第四章的图片，文字，图片label，全都改一下✅