# 准备阶段

## 完成毕业论文

做实验和写论文怎么均衡。

## 目前问题

### 这个一个Language Model问题，还是Encoder-Deocder问题

首先是一个**Autoencoder**，是一个**重构**问题，所以是Encoder-Decoder问题的。VAE可以用LM的数据集来训练，因为不需要平行语料库。说Language Model主要原因是**Decoder without attention很像Language Model**，以及**test阶段没有encode过程，sample直接decode也类似LM**。

### 找Seq2seq Framework和Transformer Decoder (Encoder)

主要是区别是**decoder中没有attention**（如果有会怎样？），这一点可以**从Tranformer LM中去找**。Test阶段没有encode的一步。而为了实验的完整性，要找一个框架性非常好的Seq2seq框架，方便更换各种encoder/decoder结构。

#### Transformer Decoder

1. 看一下timo BERT的transformer用法，因为它基于Harvard的写的，这个之前研究过。
2. Huggingface的有点太庞大了，不方便看，留在后面去读。

#### Sequence2sequence



## 论文写作

### 词向量

### RNN/LSTM

### Seq2seq

### Transformer

### 自动编码器

### VAE

### RNN-VAE

### Transformer-VAE

收藏夹吴恩达笔记，根据这个来写