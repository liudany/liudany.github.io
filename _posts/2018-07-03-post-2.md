---

layout: post
title: 《深度学习框架：PyTorch》学习笔记（2）Tensor
date: 2018-07-03 09:28:09
tags: [PyTorch]
categories: [PyTorch]

---
# 基础操作

一个支持GPU加速的高维数组。
## 新建一个Tensor

|方法|含义|
|---|---|
|t.ones(size)|全1矩阵|
|t.zeros(size)|全0矩阵|
|t.arange(start, end, steps)|从start开始，按步长往后加，不含end|
|t.linspace(start, end, steps)|以start为首，以end为结尾，包含首尾平分为steps份|
|t.randn(size)|随机|
|t.randperm(n)|生成0到n-1的随机排列|
|t.eye(n)|生成n维对角阵|

## 形状和维度

|命令|含义|
|---|---|
|a.view(-1, 3)|将Tensor调整为size形状，前后必须**元素数量一致**且**共享内存**，-1表示该维度自适应|
|a.shape & a.size()|shape是Tensor的一个变量而size()为一个函数|
|a.unsqueeze(n)|在a的第n个维度处增加1|
|a.squeeze()|缺省时为压缩矩阵所有为1的维度|
|a.size_()|resize函数的inplace用法可以无视Tensor维度，增加0或减少多余|
|a.t()|转置，**二维数组才有转置**|

+ 关于Tensor的维度，例如：
```
tensor([[[ 0.,  1.,  2.]],
        [[ 3.,  4.,  5.]]])
```
  这是一个3维矩阵，维度为 2x1x3，开头几个中括号就有多少维度。
+ 注意所有操作都是**返回一个新的Tensor**，如果想用inplace的操作需要在函数后加下划线_，且部分函数没有inplace的用法。
+ non-inplace的resize函数和view基本一致，不可以改变维度。

## 索引
a = t.randn(3, 4)

|命令|含义|
|---|---|
|a[0] / a[:, 0]|第1行 / 第一列|
|a[0][-1] & a[0, -1]|第一行最后一个元素|
|a[:2] / a[:, :2]|即a[0:2]前两行 / 前两列|
|a > 1|判断每个位置元素是和否大于1|
|a[a > 1]|将上行中矩阵作为mask，输出大于1的所有元素|
|a.gather(dim, index)|dim分为**0（列）和1（行）**，按index取出元素|
|a.scatter(dim, index, input)|gather的逆操作，改变input矩阵中index与dim所规定位置元素|

+ a[0:1, :2]与a[0, :2]不同，前者是二维Tensor，后者一维。带冒号一般是二维数组。
+ gather函数中，index若为二维行向量，则dim应该为0（列），表示从每列中选取index位置元素，生成一个形状与index相同的向量；二维也是一样，index与input行对齐则dim应该为0。
+ 想生成一个列有规律的向量，往往先按照[[xxx][yyy]]的形式生成行向量，再.t()转置得到。
+ 三维数组理解为n个2维数组。

## Tensor数据类型

默认的Tensor数据类型为`FloatTensor`。

|命令|含义|
|---|---|
|t.set_default_tensor_type()|改变默认Tensor类型|
|a.type(type)|改变a的类型|
|a.new(size)|生成与a同类型的Tensor|

## Element-wise 逐元素操作

1. 很多运算符PyTorch已经进行重载，例如 a ** 2， a * 2 等。
2. 注意`t.clamp(x, min, max)`函数，小于min和大于max的部分都会被截断。

## 归并操作 & max

这里牵扯到维度，特别是1的维度保留与否，类似Numpy中axis的用法，很绕。a = t.ones(2, 3)：

1. 以sum为例，a.sum(dim = 0, keepdim = True)，则按列求和，生成一个维度为（1, 3）的**二维数组**。若keepdim=False，则结果维度为（，3）一维数组。
2. 也有说法，从结果来看，dim = n，则输出向量的第n个维度就是1，至于1会不会保留，大部分时候看`keepdim`是否为True。

max的操作分为三种：

1. t.max(a)返回tensor中最大的一个数。
2. t.max(a, dim)在指定维度上最大的数，**返回两个tensor，第一个为最大的数值，第二个为所在的坐标**。
3. t.max(a, b)返回每个位置中ab较大的那个。

## 线性代数

1. 转置运算`.t()`会使得存储空间不连续，这一点可以通过`a.is_contiguous()`来判断。Tensor的`.contiguous()`方法可使其变得连续。
2. 注意`a*b`为Element-wise操作，矩阵乘法为`t.mm(a,b)`。
3. 逆运算`inverse`，奇异值分解`svd`，迹`trace`等等，现用现查。

# Tensor与Numpy对比

## 关于内存共享
前面提到过二者可相互转换且共享内存，Numpy支持的操作更丰富，可以Tenor->Numpy—>....->Tensor，且这样操作开销很小。这里注意共享内存的前提是：**Numpy和Tensor的数据类型相同**，由于Tensor默认为FloatTensor，所以在新建Numpy的时候注意设定`dtype = float32`，不然数据只会被复制而不会共享内存。

## Broadcast
说起这个突然想起了吴恩达……orz，这里书的作者建议手动实现一下广播法则，举个例子实现形状为a(3,2)和b(2, 3, 1)的Tensor相加：
```python
a.unsqueeze(0).expand(2, 3, 2) + b.expand(2, 3, 2)
```
注意`unsqueeze()`和`expand()`两个函数参数用法不同，前者只需要指定增加维度的地方，后者要指明扩展之后的向量具体维度，注意expand只可以扩展原来为1的维度，不可2->3，扩展方法为复制。
平时使用还是自动广播就好了。

# 内部结构

分为`头部信息区 + 数据存储区`，所谓共享内存就是指使用共同的数据存储区，但是头部信息区的信息不同（例如形状）。当某些操作导致内存不连续时，使用`.contiguous`可以使连续，但是数据会被复制到新的内存空间，这样前后Tensor的内存不再共享。

# 其他

写基础的Tensor运算模块时，很重要的一点是注意向量化运算（此处又想起吴恩达……），尽量避免Python原生的for循环，非常低效。PyTorh中Builtin的向量运算，或者说矩阵运算这些底层的函数都是C/C++实现的，能通过执行底层优化实现高效的计算。

# 实现线性回归

这里我混淆一个地方，**Loss Function**和**Cost Function**，根据Ng的课来说，前者是**单个样本的偏差**，后者是**所有样本或者一个Batch上的偏差**。现在很多地方已经不区分这两者，但是分的清楚点很多时候讲的清楚。我们的优化目标是**整个样本空间内或当前Batch的总体损失最小**，要的是全局最优解。反向传播求导的时候是从Cost Function开始来求的，牵扯到一个平均的问题。

$$
Cost = \lambda \frac{1}{m} \sum_{i=1}^{m}(\hat{y}\_{i} - y\_{i})^2
$$

反向传播后得到：

$$
\frac{dCost}{dw} = \lambda \frac{1}{m} \sum_{i=1}^{m}(\hat{y}\_{i} - y\_{i})\frac{d\hat{y}\_{i}}{dw}
$$

注意到前面是有m累加的，或者说这里也可以通过**Vectorize**的方法实现：

$$
\frac{dCost}{dW} = \lambda (\hat{Y} - Y)^{T}(\frac{d\hat{Y}}{dW})
$$

这里又说到一个老问题，复习下梯度下降的不同方式：

1. Batch Gradient Descent: 每次更新参数时使用所有的参数，易于全局最优但是慢。
2. Stochastic Gradient Descent: 每次只用一个样本来下降，训练速度很快，但是盲目，需要的迭代次数多。
3. Mini-batch Gradient Descent: 中和上面二者的方法。

## plt工具

总的来说plt的工作模式分为**阻塞模式**和**交互模式**，前者plot后必须show()才会显示图像，且代码停止在此；后者持续运行，图像可能会一闪而过，要配合pause()食用。

|命令|含义|
|---|---|
|from matplotlib import pyplot as plt|引入包|
|plt.scatter(x.numpy(), y.numpy())|散点图，两个参数为同维度的numpy型|
|plt.plot(x.numpy(), y.numpy())|直线，参数同上，还有**color, linewidth, linestyle等参数**|
|plt.xlim(0, 20)|设置x轴坐标范围|
|plt.ylabel('I am y')|设置y轴坐标轴名称|
|plt.show()|显示图像，在jupyter中经常自动显示了|
|plt.pause(0.5)|暂停供观察|
|plt.ion()|打开交互模式|
|plt.ioff()|关闭交互模式|
