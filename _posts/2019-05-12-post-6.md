---
layout: post
title: Language Model Pytorch Implementation
date: 2019-05-12 09:28:09
tags: [NLP]
categories: [NLP]

---

From [pytorchExample](https://github.com/pytorch/examples/).

## Model 

- init_hidden()

  Usually we implement the init_hidden() function ***inside our model class.*** Because we can use some `self.` variable of the class such like `nlayer, batch_size and hidden_size`.

- Init_weights()

  The default init method is kaiming_uniform_.

- repackage_hidden()

  If we do not detach the hidden state from previous graph at the beginning of each epoch, it will raise error:

  ```python
  RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.
  ```

  When we do `loss.backward()`, loss is dependent of hidden while hidden is dependent of previous hidden. If we do not detach it, the backward() function will **go through all the way to start of the dataset.** But the calculation based on previous dataset has already been bp once and the **graph has been freed.**

- dropout layer

  Do it twice, **AFTER embedding and AFTER the forward.** 

## Main

1. About hidden state

   ```python
   def run_train():
     hidden = model.init_hidden()	# return zeros tensor
     for i, batch in enumerate(train_iter):
       hidden = hidden.detach()	# detach hidden from last iteration 
       output, hidden = model(data, hidden) # use the last hidden from last iter
   ```

   In this example, the batch data is consecutive from the same source text, so the hidden state can be reused in next iteration. **But it has to be detached from previous graph.**

2. Difference between eval / train / test 

   When eval you do not need to do `loss.backward()` and `optimizer.step()`. Just return `eval_loss` at the end of the function.

   ***Wrap eval&test with torch.no_grad, calc loss but no bp.***

3. Framework

   a general framework:

   ```python
   for epoch in range(n):
     model.train()
     run_train(train)	# 每个epoch相关的都写在run_epoch里面，包括读数据/前向/损失/优化
     model.eval()	# close drop, norm etc.
     run_eval(eval)	# wrapped with torch.no_grad, calc loss but no bp.
     
     calc(loss)
     
   run_eval(test)	# 这里和eval用相同的函数即可
   ```

4. Loss calculation

   At the beginning of each epoch, define `total_loss=0`. Accumulate loss of every batch until interval and print average loss(`total_loss/interval`)  and define `total_loss=0` to restart accumulate.

   Besides, use `time.time()` to record elapsed time, like `elapsed = time.time() - start_time`.

5. save model and checkpoint

## Generator

1. Prepare.

```python
model.eval()
hidden = model.initHidden(1)	# bsz = 1, hidden (1(num_layer), 1(bsz), hidden_size)
```

1. Pick a start word randomly.

```python
input = torch.randint(len(field.vocab), (1, 1), dtype=torch.long).to(device)
```

2. Output to file and no grad.

```python
with open('generated.txt', 'w') as outf:
    with torch.no_grad():
```

3. Feed forward, note that output size: `(bsz=1, ts=1, len(vocab))`.

```python
        for i in range(1000):	# total 1000 words
            output, hidden = model(input, hidden)	# feed forward
```

4. 1.0 is the temperature, **higher will increase diversity **(like label smoothing). **The exp() is used for convert weights to positive numbers for next multinomial function.**

```python
            word_weights = output.squeeze().div(1.0).exp().cpu()
  					# use multinomial function to pick word by weight, [0] converts 1 dim to 0 dim
            word_idx = torch.multinomial(word_weights, 1)[0]
            input.fill_(word_idx)	# update input, we dont need to record history.
            word = field.vocab.itos[word_idx]
            outf.write(word + ('\n' if i % 20 == 19 else ' '))	# print \n every 20 steps
```

Here `input.fill(word_idx)` updates input with generated word_idx. We don't need to record all the previous words because they were encoded in hidden state.

5. Print log.

```python
            if i % 10 == 0:
                print('| Generated {}/{} words'.format(i, 1000))
```

