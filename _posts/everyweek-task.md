---
title: 待办事项⚠️
date: 2020-10-15 09:28:09
tags: [NLP]
categories: [NLP]
draft: true
---



**少把时间浪费在花里胡哨的上面，脚踏实地做事情！**



看完《白夜行》。❌

## VAE路线

- UvA Wilker VAE tutorial，通过这个决定要不要再看PGM。
- RNN和transformer-based的VAE。
- CVAE的动机和方法。
- 层次级别的文本autoencoder。
- Miao 2016论文，BOW文档编码。


## GAN路线

- [TorchGAN](https://github.com/torchgan/torchgan)
- [GAN](https://www.youtube.com/watch?v=DQNNMiAP5lw&list=PLJV_el3uVTsMq6JEFPW35BCiOQTsoqwNw)
- [OpenAI Intro](https://blog.openai.com/generative-models/ )
- GAN用于文本生成，[知乎网址](https://zhuanlan.zhihu.com/p/36880287) 
- GAN路线的深入，[CVAE-GAN](https://zhuanlan.zhihu.com/p/27966420)
- 杨敏介绍[知乎](https://zhuanlan.zhihu.com/p/25168509)

## RL

- 知乎一个[好问题](Role of RL in Text Generation by GAN(强化学习在生成对抗网络文本生成中扮演的角色))

## 基础方面

- SGD和Adam的区别，花书和知乎的文章[1](https://zhuanlan.zhihu.com/p/32338983)和[2](https://zhuanlan.zhihu.com/p/46145843)
- [BERT](https://github.com/huggingface/pytorch-pretrained-BERT)
- RNN [Incremental Decoding](https://fairseq.readthedocs.io/en/latest/models.html#incremental-decoding)
- [pytorch examples](https://github.com/pytorch/examples)其中的LM/VAE/DCGAN/RL。
- BERT/ELMO/GPT2的应用。

## Transformer

(x, m, m)这种运算是为了mimic seq2seq attention，那看看普通的seq2seq有哪些对attention的改进。