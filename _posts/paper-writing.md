---
title: paper里一些句子的表达
date: 2018-12-20 09:28:09
tags: [Paper]
categories: [Paper]
draft: true
---

1. 搞清楚文章要解决什么问题。
2. 当前这个问题存在的现状是什么样的，为什么会存在，现有的解决方法是什么。
3. 现有解决方法的不足是什么。
4. 作者提出的解决方法是基于什么思想，这一步非常重要。
5. 作者的模型是怎么做的。





we parametrize(spell!) the model as a feedforward neural network.

对比

The usual RNN, described in .... However, in the proposed scheme, we would like the ...

所以

Hence

列公式

The ... is computed 

........

关于无监督的一些表述

annotated data is hard to obtain标注数据

描述生成模型

it estimates the Probability Density Function (PDF) of the training data

The VAE model can also sample examples from the learned PDF,

It turns out every distribution can be generated by applying a sufficiently complicated function over a standard multivariate Gaussian.

分为两步

be broken to two phases

估计

we’ll approximate it using Monte Carlo method:

反向传播

the gradients won’t propagate.

有作用

Most sampled z’s won’t contribute anything to  P(x)

替换

We can substitute Q with a 

在decoder的每一个时间步

concatenating the sampled ⃗z to the decoder input at every time step,

介绍变量

We refer to T as the transform gate.

为了简化

For simplicity

数值表示

numeric representation

展开的LSTM
 unrolled-LSTM step 

GPT没有encoder，没有e-d attn
Since there is no encoder in this set up, these decoder layers would not have the encoder-decoder attention sublayer that vanilla transformer decoder layers have.

我们使用了参数为x的xx训练方法

During training, we employed label smoothing of value ϵls=0.1.

为了避开这种问题

In order to circumvent this issue.

担当..角色，起..作用

serve as a ...

同时

Simultaneously

本文解决了..问题。

In this work, we propose a variational attention mechanism to address this problem.

我们模型的效果由…来体现

The effectiveness of the model is demonstrated by 

因此

Hence 可在句号后，另起一句

等式左边第一项代表

The first term in Equation 4.10 corresponds to/ refers to

正态分布变量

Normally-distributed z 

计算什么什么

compute ...

稳定状态

undesirable stable equilibrium

由于..而恶化

be compounded by，组成的话是of

考虑到..的极限情况

Taken to the extreme of 

与…不同 有差异

contrast with

给..增加了...

supplement the decoder with a … mechanism

使用了...

is equipped with GLU

配备一个语言模型

is augmented with a LM

结合...方法

a standard way to integrate LM is to ...

we present a way to integrete LM into an NMT system.

结合….

incorporating monolingual corpora can ….

我们的方法/模型

approach

我们交替使用..的表达方式

we refer to .. and … interchangeably.

这很像

it resembles ...

… 接下来 ...

train a … and subsequently trains ..

单独

Solely

达到了..的准确率

we reach 7% WER

调整参数

Tune the parameters that were used...

从头开始

from scratch

减轻了..问题

reduce the problem of 

mitigate the problem

与…不同的事	指定作为

Distinct form existing VAE, which…, out model specify … as

由..初始化，用一个神经网络表示

parametrized by a neural network

用于赋予

applied to endow .. with ...

因此

accordingly

等号成立是因为

equation holds because

在…上的分布

distribution over words

现存模型存在的问题

Existing … model

最终目标

achieve the ultimate goal

实现目标

accomplishing another objective

有说服力的结果

promising results

用…符号表示...

denote .. as ..

关于..的函数

a function related to ., ., and .

a vector associated with..

表示，指的是

refers to 

可能的取值

possible value

包含

consis of

引入了..方法

Introduce

最经典的方法

the canonical approach

没有得到解决

problem is largely under-addressed in the literature.

进行了复杂实验

conduct comprehensive experiments to evaluate the performance of 

从...角度

From … perspective

处理这个问题

tackle the problem

根据…的做法

follow …. to re-iterpret

利用了…的思想

we propose a algorithm which draws on the idea of.

对..的重要性

importance of .. to ...

所以

thus

扩展了….模型到

SentiVAE extends the original VAE model to combine….

## From paperweekly

Notice that the indefinite article (不定冠词) 'a' should NOT be capitalized: "Taking a Closer Look..." & "Searching for a Robust..."

## VAE

Neural variational models can be used to encode input data into latent variables. It is further possible
to sample multiple points from the latent space in order to generate diverse outputs.



## Story

Jiwei Li



## Abstract

研究任务

已有方法

面临挑战

创新思路，与挑战一一对应

实验结论

Although the Transformer model has outperformed traditional sequence-to-sequence model in a variety of natural language processing (NLP) tasks, it still suffers from semantic irrelevance and repetition for abstractive text summarization. The main reason is that the long text to be summarized is usually composed of multi-sentences and has much redundant information. To tackle this problem, we propose a selective and coverage multi-head attention framework based on the original Transformer. It contains a Convolutional Neural Network (CNN) selective gate, which combines n-gram features with whole semantic representation to obtain core information from the long input sentence. Besides, we use a coverage mechanism in the multi-head attention to keep track of the words which have been summarized. The evaluations on Chinese and English text summarization datasets both demonstrate that the proposed selective and coverage multi-head attention model outperforms the baseline models by 4.6 and 0.3 ROUGE-2 points respectively. And the analysis shows that the proposed model generates the summary with higher quality and less repetition.