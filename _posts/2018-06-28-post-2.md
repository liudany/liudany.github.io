---
layout: post
title: Mosedecoder汉英翻译中的数据预处理
date: 2018-06-28 20:50:09
tags: [Machine Tranlation]
categories: [NMT]

---

不管什么模型在翻译之前都需要对两种语言的语料进行清洗，达到一种比较清爽的状态，很多都是重复工作，记录下来。

# Mosedecoder中英语料预处理

## Jieba Parallel
平常处理的语料库动辄百万级，Jieba提供了并行分词的接口：

```python
python ~/jieba/test/parallel/test_file.py $TEXT/zh
```
这是针对文件的分词，第二个参数为汉语文件地址。其中用到函数`jieba.enable_parallel()`来开启并行分词，参数为并行进程数，没有参数时会调用所有CPU一起工作。

## Tokenize
英文分词，主要针对`I'm`这种缩写分为`I`和`'m`两个token。
```python
~/mosesdecoder/scripts/tokenizer/tokenizer.perl -l en \
   < $TEXT/train.en    \
   > $TEXT/train.tok.en -threads N
```
这里使用的Mosedecoder的tokenizer只可以对英语、法语和德语等使用。`-l`后为目标语言，第二行为输入文件，第三行为输出文件。针对大型语料库可以使用多线程参数`-threads`来指定线程数。

## Truecasing
对词汇的大小写进行调整，这里并不是把所有的单词都变成小写，而是首先训练一个模型，统计出每个单词的大小写最常出现的频率，以此为根据来进行调整为所谓的Truecase。

训练统计模型：
```python
~/mosesdecoder/scripts/recaser/train-truecaser.perl \
     --model $TEXT/truecase-model.en  \
     --corpus $TEXT/train.tok.en
```
利用这个模型来进行大小写转换：
```python
~/mosesdecoder/scripts/recaser/truecase.perl \
   --model $TEXT/truecase-model.en         \
   < $TEXT/train.tok.en \
   > $TEXT/train.true.en
```

## Cleaning
处理语料中过长和过短的句子，将语料中句子长度范围限制在(MIN, MAX)。
```python
 ~/mosesdecoder/scripts/training/clean-corpus-n.perl \
    $TEXT/train.true en zh \
    $TEXT/train.clean MIN MAX
```
这里可以一次性处理源-目标两个文件，生成以.en和.zh结尾的两个结果。

## 整合
把上面4个步骤整合为一个脚本，注意文件操作的一些坑，比如：
```bash
TEXT=/data/ldy/dev/NJU-newsdev2018
```
这里最后不要加斜杠，这个变量的用法一般为`$TEXT/file`。