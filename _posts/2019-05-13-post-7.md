---
layout: post
title: Reading notes about BERT, ELMo and GPT
date: 2019-05-13 09:28:09
tags: [NLP]
categories: [NLP]
typora-root-url: ../../static
---

本文大体为Jay Alammar[相关博客](https://jalammar.github.io/illustrated-bert/)和知乎[好文1](https://zhuanlan.zhihu.com/p/49271699)和[好文2](https://zhuanlan.zhihu.com/p/56865533)的翻译和笔记。



# Word Embbedding Recap

目的是将词转化为数值的形式以进行计算，现有的Word2Vec等方法都是捕捉语义/词的关系/相似度。

很快大家就发现使用***在大语料上预训练好的词向量***是很棒的，而不是在随着模型训练阶段同时训练词向量，因为训练阶段往往是小数据集的。这其实就是最早的预训练方法。

使用词向量的方法有两种，Frozen即固定Embedding层参数不动，另一种是Fine-Tuning，就是用预训练的词向量初始化Embedding阵之后，随着训练过程仍然更新这层的参数。

但是这两种无论哪种方式，预训练的词向量，无论上下文如何，词向量是不变化的。

# ELMo

## Context matters

一个词的意思是随着语境变化的，或者说存在多义词问题。***ELMo就是一种contextualized word-embedding***，全名***Embedding from Language Models***。它可以根据一个词所存在的上下文来给予它不同的词向量。

## Architecture

结构上，ELMo基于双向LSTM网络，即两个双层的LSTM，分别以正反向处理句子，这样就能关注前面的词，又蕴含其后面的词的信息。

## Pre-train and downstream

预训练时，跟普通的LM类似，**使用大量的无监督语料，做预测下一词任务**，更新整个网络和Embedding，最终我们除了得到Embedding(与普通的LM得到的词向量相同)，**还得到了一个训练好的LSTM网络**。

用于下游任务时，我们将现实的句子输入训练好的网络，第一层是普通Embedding(预训练中训练好的)，后面还有两层hidden state。

⚠️***注意这时候我们不用再去预测了，直接拿这三个状态做后续任务。***

![ELMo bi-directional LSTM](/img/elmo.png)

最终每个单词的词向量，如上图所示是concatenate之后在weighted sum，得到最终ELMo提供的embedding，以供下游任务使用。

## 缺点

- LSTM特征抽取的能力远远弱于Transformer。
- ELMo只能用于feature extraction即得到新的embedding，并不能fine-tuning。



# ULM-FiT

三阶段模式，比ELMo添加了一个中间阶段，针对领域语言模型的预训练。

# OpenAI Transformer

## Architecture

Transformer在捕捉长距离关系上优于LSTM，其结果也自然而然的适用于翻译任务。但是如何才能将其运用到语言模型上，并生成预训练模型用于下游任务呢。

⚠️**OpenAI Transformer is made up of Transformer *decoder* stack.**⚠️

OpenAI使用了12层transformer的decoder（因为decoder本来就用于预测下一词），没有使用encoder。所以在这种结构下，***decoder需要去除coder-decoder attention layer***，仅剩带有mask的self-attention和前向网络。训练集是7000本书，因为书中可以蕴含分隔很远的词语之间的关系，这是短文和微博等所不具备的。

## Transfer to Downstream Tasks

![gpt](/img/gpt.png)

利用之前预训练好的模型，***针对不同任务，使用不同的输入方式***，如上图所示。之后会***训练整个网络***的参数，之前预训练只是初始化，之后的更新成为Fine-tune，使得网络更适合当前任务。

***⚠️从论文中看来，是取最后一层transformer的最后一个位置的向量，过Linear层。***

## 问题

Transformer Decoder是一个单向预测的语言模型，没利用后文信息。

# BERT

OpenAI所使用的transformer训练语言模型的方法，有一些不足的。在ELMo中使用了***双向***的LSTM，但是OpenAI只使用了单向的，我们期望得到一种***既是基于transformer，又能结合前后文语义***的语言模型。

**因为decoder的训练方式是根据前n-1个词预测第n个，这注定了它不能看到后面的词，否则就提前知道答案了。**

所以BERT使用了Transformer的encoder，全名***Bidirectional Encoder Representations from Transformers***。

## Arichitecture

⚠️***BERT is basically a trained Transformer Encoder stack.***⚠️

注意这里的几个词，trained表示BERT是一个**经过预训练**的模型，且是**由Transformer Encoder堆叠**而来。

- 原始的Transformer模型有6层encoder，512维前向网络，8 attention heads。
- BERT Base Version有12层Transformer，前向网络有768维，12 attention heads。
- BERT Large Version有24层Transformer，前向网络有1024维，16 attention heads。

## Input/Output

使用时，第一个词的位置会输入一个[CLS]标志，stands for Classification。之后与vanilla transformer相同。

在处理分类任务时，我们***仅使用[CLS]对应的第一个位置的output vector***输入classifier，论文中仅用一层FFN做分类器就取得了很好的效果！

输入的embedding分为三层，位置信息，句子信息和原词的embeeding，叠加得到最终的输入embedding。

![](/img/bert-input.png)

## 训练方法

BERT***使用了mask language model的训练方法***。随机mask掉输入句子中15%的词汇，具体来讲，对于这15%的词汇，其中80%被mask替代，10%被一个随机的token替代，另外10%保持原样。这个任务其实跟CBOW有点像，由外面的词去猜中间的一个词。

同时针对Two-sentence Tasks(评价两个句子关系的任务)，为了提高BERT处理句子间关系的能力，预训练过程还包括***猜测是否为下一句***的任务，如下图。

![](/img/bert-next-sentence-prediction.png)

## Task specific-Models(fine-tune)

与GPT一样，针对不同的任务，BERT有不同的使用方法。

![](/img/bert-tasks.png)

## for feature extraction(contextualized embedding)

BERT不只有fine-tuning一种使用方法，也可以跟ELMo一样，使用预训练好的BERT来***生成contextualized word embeddings***。但是这么多层encoder，我们使用哪一层输出的vector作为embedding比较好呢，看下图。

![](/img/bert-feature-extraction-contextualized-embeddings.png)

图中提到了使用最后一层，使用全部12层的和，仅使用倒数第二(second-to-last)层，使用最后四层的和，和concat最后四层，在NER任务上的表现来看，***不一定最后一层就是最好的***，相反倒数第二层的结果反而比最后一层好。

# GPT 2

较于GPT1，做了如下改动：

- Transformer stack由12层decoder增加到**48层decoder**，参数15亿，但依然选择单向语言模型。这一点也是为了**适应更大的更高质量的语料库**。

- 语料使用WebText，大概800万网页，主题广数量大，且经过了人工的筛选，适用于任何主题的下游任务。

- 尝试***无监督***的下游任务，不做任何fine-tune，**以语言模型的形式完成摘要/翻译等任务**，实现方法是在输入中增加**TL:DR**引导，类似google的多语言翻译系统，通过标志符来引导。不管啥任务都一个一个往外蹦字。估计翻译任务是通过BPE在相似语言内共享词表。

综上，**通过提高模型的层数，提高预训练数据的数量/质量/广度**，使得预训练模型有着更广泛的适用性。

# MT-DNN

出了与BERT类似的无监督LM预训练任务相比，在高层***增加了multi-task的多任务有监督训练***，且更新底层参数。

![](/img/mt-dnn.png)

底层Shared Layers即BERT，预训练任务也相同。不同之处是在第二阶段的Fine-tune中，通过各种不同的有监督任务，更新所有（包括预训练的底层）参数，使得模型对多任务更具有适应性。