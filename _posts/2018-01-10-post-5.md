---
layout: post
title: All about AllenNLP
date: 2018-01-10 15:01:00
description: An apt description of your post
categories: [rnn, regularization]
---

# Tutorial on EMNLP

## How to code
1. Meaningful names
2. Shape comments on tensors
3. Comments describing non-obvious logic


4. Don’t start from scratch! Use someone else’s components.

**write code for people, not for machine.**

5. minimal test. 一次性检查所有的实验是浪费时间，但是有些minimal单元不可测试，make sure data processing works consistently, taht tensor. operations run, gradients are non-zero. Run on small test fixtures.

6. no hard-code. 不要把参数直接写在函数里，例如 self.input_embedding = embedding(100)

7. Use EXCEL to keep track of what you ran. Note down the experimenter, background and any other detailed information.

8. 控制变量， it's better to use a configuration files.

9. Use Tensorboard NOW.

10. Source control 版本控制，如果事情不对，赶紧revert回原来的版本

11. unit test, check that a small part of your code works correctly.


## AllenNLP

DatasetReader file->instance

TokenIndexer(character-level? bpe? pos?....)

Field -> Instance -> Vocabulary

DataIterator 迭代batch


### TokenIndexers & ToeknEmbedder

words -> TokenIndexers -> ids -> TokenEmbedder -> tensors

### Seq2VecEncoder

(batch_size, sequence_length, embedding_dim) -> (batch_size, embedding_dim), such as BOW, LSTM(last output), CNN + Pooling

### Seq2SeqEncoder

(batch_size, sequence_length, embedding_dim) -> (batch_size, sequence_length, embedding_dim), RNNs, self-attention, do-nothing(lol)

### Attention

(batch_size, sequence_length, embedding_dim)（很长的输入）, (batch_size, embedding_dim) （一个词）-> (batch_size, sequence_length)（算出这个词和每个词的attention）

### MatrixAttention

扩展到一句话的每个词，对另一句话的attention。

### SpanExtractor

提出一段文本的embedding.

### Flowchart

file -> DatasetReader -> instance -> Model -> predictions/loss 
use a JSONN config and AllenNLP training code

### Declarative syntax

Using JSON to specify an entire experiment, including changing architectures without changing code


# Github tutorial

## Preliminary

讲基于Json config文件实现模型的机理。

## Dataset, Instance and Field

We train and evaluate our models on `Dataset`. 

A `Dataset` is a collection of `Instance`s.

An `Instance` consists of `Field`s, such as `TextField` and `SequenceLabelField`.

注意`TextField`里面是words/tokens，这里还没有变成Embedding。

有什么用？？？？？？？

## DatasetReader

The first section we define in configuration file.
```json
  "dataset_reader": {
    "type": "sequence_tagging",
    "word_tag_delimiter": "/",
    "token_indexers": {
      "tokens": {
        "type": "single_id",
        "lowercase_tokens": true
      },
      "token_characters": {
        "type": "characters"
      }
    }
  },
```
`word_tag_delimiter` specify the symbol between a word the corresponding tag.
There is also `token_delimiter`. The deafult is split-on-whitespace.

Note that we should convert tokens/words into arrays(not embedding yet) by `token_indexers`. Here we specify two methods, `toekns` and `toekn_characters` which indicates character-level encoding.
这里tokens和token_characters名字可以自己乱起吗？？？重在type的值来重构子类，好像和每个type上面的名字无关，

## Training and Validation Data

```json
  "train_data_path": "https://allennlp.s3.amazonaws.com/datasets/getting-started/sentences.small.train",
  "validation_data_path": "https://allennlp.s3.amazonaws.com/datasets/getting-started/sentences.small.dev",
```

They can be specified either as local paths or URLs.

## Model

### type

```json
  "model": {
    "type": "simple_tagger",
```

Firstly we specify the subclass of our model. A `,` indicates the end of a property.


### text_field_embedder

```json
  "text_field_embedder": {
    "tokens": {
      "type": "embedding",
      "embedding_dim": 50
    },
    "token_characters": {
      "type": "character_encoding",
      "embedding": {
        "embedding_dim": 8
      },
      "encoder": {
        "type": "cnn",
        "embedding_dim": 8,
        "num_filters": 50,
        "ngram_filter_sizes": [
          5
        ]
      },
      "dropout": 0.2
    }
  },
```
It has an entry for each of the `token_indexers` methods.

The "tokens" input gets fed into an Embedding module that embeds the vocabulary words in a 50-dimensional space, as specified by the `embedding_dim` parameter.

The output of `text_field_embedder` is a 50-dimensional vector for "tokens" **concatenated** with a 50-dimensional vector for "token_characters"; that is, a 100-dimensional vector.

一个type后面跟着的，都是这个子类的属性，直到逗号结束。
一个大括号开始的，必然是一个type和type的值。选择两种embedding方法那里除外，也可以说如果没有type，就是一种 concatenate的方法？？？

### Seq2SeqEncoder

```json
    "encoder": {
      "type": "lstm",
      "input_size": 100,
      "hidden_size": 100,
      "num_layers": 2,
      "dropout": 0.5,
      "bidirectional": true
    }		//注意最后一个属性的结束可以不加逗号
```
The LSTM layer is just a thin wrapper around `torch.nn.LSTM`, and its parameters are passed through to the PyTorch constructor. Its input size should **match** the 100-dimensional output size of the previous embedding layer.

The output of this lstm layer gets passed to a linear layer that **doesn't need any configuration**.
这里线性层的输出维度在datasetreader时候就决定了吗？？？？

## Train your model

The rest of config file is dedicated to the training process.

```json
  "iterator": {"type": "basic", "batch_size": 32},
```
The `BasicIterator` pads our data and processes it in batches of size 32.

```json
  "trainer": {
    "optimizer": "adam",
    "num_epochs": 40,
    "patience": 10,
    "cuda_device": -1
  }
}
```

The first line indicates Adam optimizer is used with default parameters.

`patience` stop prematurely if we get no improvement for 10 epochs.

Change `cuda_device` to your GPU device id. '-1' indicates CPU.

The training configuration is always saved as part of the model archive, which means that you can always see how a saved model was trained.

## Conclusion

The configuration file consists of 5 parts: `data_path`, `dataset_reader`, `model`, `iterator` and `trainer`.

# Creating Your Own Models










logits(未经过归一化的概率，一般是softmax的输入) 