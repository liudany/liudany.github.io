---
layout: post
title: Word Embedding & Language Model 词嵌入和语言模型
date: 2018-07-17 09:28:09
tags: [Word Embedding]
categories: [NLP]

---

今天读一下PyTorch Tutorials里[关于Word Embedding的部分](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html#getting-dense-word-embeddings)。

# 概念

1. 词嵌入是「Encoding Lexical Semantics」，是对单词的语义进行了编码。普通的ASCII码或是One-hot编码都是将词汇视为independent entities而忽视了词汇之间的similarity或者说semantic relation。
2. Sematic similarity常基于distributional hypothesis：认为具有相似的上下文的词语，在语义上也是相似的。
3. 词嵌入向量的每一个维度是一个特征，但是网络学习到的向量的特征往往是不可解释的。

# 基础用法

词嵌入以一个V\*D的矩阵的矩阵形式，每行是一个词的嵌入，D是向量维度。这里也需要一个索引号来对应词汇和向量，用法与前面词袋模型中的**word_to_ix**相同，以单词为索引，对应的索引号为value。

在PyTorch中，Embedding与其他层的用法相同，先实例化一个固定维度的Embedding，这时候其中的词向量都是随机初始化的，可以对实例传入index参数**（不需要ont-hot直接index）**来取出对应位置（词汇）的词向量。之后再建立神经网络，**通过learning的方法来不断更新Embedding中的参数**，希望让这些参数能够蕴含单词的语意信息。

```python
word_to_ix = {"hello":0, "world":1}
embeds = nn.Embedding(2, 5)
lookup_tensor = torch.tensor([word_to_ix["hello"]], dtype=torch.long)
hello_embed = embeds(lookup_tensor)
print(hello_embed)
```

要注意：
1. Embedding要实例化，定义时参数为(vocab_size, embedding_dim)。
2. 之后在实例中索引时注意**索引必须是LongTensor类型**。
3. 得到的hello_embed是每行一个词向量。

# N-Gram Language Model

上面说到词向量是训练得来的，既然是训练，就要有目标要有损失。用Language Model训练词向量是一种方法，即从上下文的角度去定义语义。这一点与分布假说是符合的。

## 预处理

将语料切分为N-Gram的形式，这里具体使用tri-gram。并构建词典，并分配索引。
```python
CONTEXT_SIZE = 2
EMBEDDING_DIM = 10
#语料使用Shakespeare的Sonnet2，这里意思一下就行
test_sentence = """When forty winters shall besiege thy brow,
And dig deep trenches in thy beauty's field...""".split()
trigrams = [([test_sentence[i], test_sentence[i+1]], test_sentence[i+2]) for i in range(len(test_sentence) - 2)]
vocab = set(test_sentence)
word_to_ix = {word: i for i, word in enumerate(vocab)}
```

## 定义网络

注意Embedding层的定义方式和其他层一样。
```python
class NGramLanguageModeler(nn.Module):
	def __init__(self, vocab_size, embedding_dim, context_size):
		super(NGramLanguageModeler, self).__init__()
		self.embeddings = nn.Embedding(vocab_size, embedding_dim)
		self.linear1 = nn.Linear(embedding_size * context_size, 128)
		self.linear2 = nn.Linear(128, vocab_size)

	def forward(self, inputs):
		embeds = self.embeddings(inputs).view((1, -1))
		out = F.relu(self.linear1(embeds))
		out = self.linear2(out)
		log_probs = F.log_softmax(out, dim = 1)
		return log_probs
```

注意的地方：
1. 第一层FC层，输入维度为**embedding_size * context_size**，因为每次根据context_size个词汇作为输入，来预测下一个。
2. 第二层FC层，输出维度为**vocab_size**，因为语言模型其实也是个分类问题，类别数为词表的大小，最后softmax层输出每一个类别的概率。
3. 注意网络的输入是句子中单词的**索引**，因为在forward中第一层embedding中会按这些索引来取出词向量。
4. Embedding是nn中的函数，embedings是词嵌入层在类中的实例，embeds是将输出向量化后得到的向量。
5. `embeddings(inputs)`在取词向量时，索引可以是一个tensor，一行或是一列都可以。取出来以后的词向量都以一行一个词的形式表示。这里context为2所以取出两个词向量，后面的view方法把这两个词向量拼接了起来。
## 训练

```python
losses = []
loss_function = nn.NLLLoss()
model = NGramLanguageModeler(len(vocab), EMBEDDING_DIM, CONTEXT_SIZE)
optimizer = optim.SGD(model.parameters(), lr=0.001)

for epoch in range(10):
	total_loss = 0
	for context, target in trigrams:
		context_indx = torch.tensor([word_to_ix[w] for w in context], dtype = torch.long)
		model.zero_grad()
		log_probs = model(context_indx)
		loss = loss_function(log_probs, torch.tensor([word_to_ix[target]], dtype = torch.long))
		loss.backward()
		optimizer.step()
		total_loss += loss.item()
	losses.append(total_loss)
print(losses)
```

1. 开一个list来记录loss，之前一般在epoch中实时输出，这里是记录loss，在训练结束后一次性输出。
2. 该网络输入是索引，必须为LongTensor类型。有两种方法，`LongTensor()`直接初始化或者`tensor(data, dtype = torch.long)`。
3. loss_function里面的两个参数，第一个为softmax的输出，是一个list，每个元素为对应的word出现的概率的log，**第二个参数可以是索引，也可以是ont-hot表示**，即label=[3]和[0, 0, 0, 1, 0 ...]是相同的。在这里使用的是索引号。
4. loss是一个单元素tensor，取出数字的方法为`tensor.item()`。
