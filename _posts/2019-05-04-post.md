---
layout: post
title: Some Basic Concepts in Deep Learning
date: 2019-05-04 09:28:09
tags: [PyTorch]
categories: [PyTorch]
draft: true
---

# SGD

模型学习过程就是***确定参数***的过程，***对于某个参数的导数（多个导数组成梯度），指向误差增加最快方向***。所以向梯度***相反的方向***更新。

1. 梯度下降法：利用***全部样本***，前向传播，计算梯度后更新参数。
2. 随机梯度下降：利用***一个样本***，更快。
3. mini-batch梯度下降：中和前两种，每次用一个batch的数据更新。

由于每次使用不同批量的样本，也就避免了陷入局部最小值。

# BN & LN

# 反向传播

定义了网络之后，网络中的参数都是求导的对象，是leaf node，requires_grad=True。而我们是不必对输入求导的，只不过输入输出和网络参数进行了运算，所以在求梯度的时候会用到他们的值。

只对需要更新的参数求导，这些参数构成我们的网络。

要不要求导，和是不是Leaf node没有关系，网络参数和输入输出都是leaf node，但是我们不需要对输入求导。执行backward的过程，就是从结果变量开始，往后推，给每个requires_grad的leaf_node的grad位置都填上对应的梯度数据，虽然输入也是leaf node但我们不需要他的导数（虽然也可以求），所有中间经过变量的导数在运算中会得到，但是不会保存，计算完后即消失，想留住需要hook。

# LM & Conditional LM

## Concept

由前n-1个词预测第n个词的网络，就是LM。一个简单的RNN单元(without encoder)即为一个LM。诸如翻译模型，对话系统等都是Conditional LM，即计算第n个词时除了要关注前n-1个词，确保这句话是流畅的语言，还需要关注一个encoder得来的条件X，在翻译中是源句，在问答中是问题，以确保生成的语言与前文是相关的。


## Perplexity

The final common measure of language model accuracy is ***perplexity***, which is defined as the exponent of the average negative log likelihood per word:

​							$$		\operatorname { ppl } \left( \mathcal { E } _ { \text { test } } ; \theta \right) = e ^ { - \left( \log P \left( \mathcal { E } _ { \text { test } } ; \theta \right) \right) / \text { length } \left( \mathcal { E } _ { \text { test } } \right) }$$

An intuitive explanation of the ppl is "how confused is the model about its decision?" More accurately, it expresses the value "***if we randomly picked words from the probability distribution calculated by the language model at each time step, on average how many words would it have to pick to get the correct one?***" PPL value is always bigger, making the differences in models more easily perceptible by the human eye.

# 参数初始化

## 对称初始化(包括全0)

会导致每个神经元输出结果相同，BP之后仍然相同，所以无论训练多少轮，每个神经元weight都相同，无法学习到不同的特征。

## 太大/太小

初始化太大，会导致数据进入激活函数的**饱和区**，反向传播时梯度消失，尤其是sigmoid/tanh。

太小的话，同样会进入饱和区，但是除了sigmoid/tanh外，太小也会影响ReLU。

[知乎详解](https://zhuanlan.zhihu.com/p/62850258)

## Kaiming



## Xavier

# 激活函数

![](/Users/danyliu/myblog/static/img/activation.png)

## 为什么使用激活函数

普通的WX这种矩阵运算只能拟合出**线性函数**，为了**增加非线性因素**，所以使用激活函数。

## Sigmoid/TanH

早期的激活函数选择往往是Sigmoid，Tanh，它们的特点是：

1. **输出范围是有限的**，这样对于较大的输入值也会有稳定的输出，但是带来了**饱和区区分度不强的问题**。
2. 在正负无穷处**梯度消失**，反向传播时梯度为0。
3. 有一些expensive的操作，例如指数。

⚠️有些函数**导数与自身函数值有关系**，所以输出要局限在一定范围，太大则会梯度爆炸。

### 现在的用处

Sigmoid的输出在(0, 1)之间的特性使得它可以用来做**门控单元**（例如LSTM中）。

## ReLU

在x>0时导数恒为1，再大的输入也不会饱和，所以**解决了梯度消失问题**，**计算简单且快（导致收敛快），且没有饱和区，对于很大的输入也会有很大的反应，区分度强**。

但是在x<0的时候也会出现梯度消失的问题。所以有了Leaky ReLU等其他的变种，但是增加了网络参数。

但是x<0不激活，相当于一种dropout，学习到的特征更加**稀疏**，使模型具有更好的鲁棒性，反而是好事。

### 配合初始化

如果参数初始化不恰当，容易使得ReLU的输入值大部分为负，梯度全部为0，发生DEAD ReLU现象。

所以配合恰当的参数初始化，例如配合Xavier。

# 学习率

## Warm up

在Large Batch Size训练的初始阶段，下降方向是不稳定的，所以要使用warm up来慢慢增大。

## Decay

后面慢慢稳定下来，为了更精确的接近极值点，缩小步长。

# 语言模型

## 训练阶段

Batch train时输入(batch, time-step, embed-size)实际每一步是做切片[:, 0, :]取该时间步的一个词（带上batch）输入的，时间复杂度还是O(N)，只不过同时进行多句（batch）。

实际上也是one-by-one生成。

## 预测阶段

严格的one-by-one，但是没有ground-truth了，必须要用生成的上一个词。

## Teacher Forcing

在PyTorch实现的RNN中，我们一次性喂入(batch, time-step, embed-size)数据时就默认了teacher-forcing，每一时间步的输入都是ground-truth的。