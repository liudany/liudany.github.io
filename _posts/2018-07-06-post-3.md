---

layout: post
title: 《深度学习框架：PyTorch》学习笔记（3）Autograd
date: 2018-07-06 09:28:09
tags: [PyTorch]
categories: [PyTorch]

---
# Variable

Autograd中的核心数据结构是Variable，其封装了Tensor并记录对其的操作以构建计算图，一个Variable主要包括下面三个属性：

+ data: 包含了Variable所包含的Tensor数据。
+ grad: 与data相同形状，记录了data的梯度，也是一个Variable。也就是说调用梯度数据的方法为v.grad.data。
+ grad_fn: 指向一个Function，基于当前的Variable**是什么操作得来的**，grad_fn就是这种操作的反向传播函数，用于构建计算图。

构建一个Variable出了需要传入一个Tensor，还有`requires_grad`和`volatile`两个参数可选。

注意Variable支持绝大部分的Tensor操作，但是没有inplace操作，因为要自动求导的话需要用到原始数据，不可以随意修改。

+ Variable每次反向传播都累加梯度，要清空的话`v.grad.data.zeros_()`
+ 叶子结点是没有子节点的节点，一棵树的最下方的。根结点是最上方的一个节点。当用户创建一个Variable的时候，它就是一个叶子结点，对应的`grad_fn = None`，进行运算就是往上进行运算堆叠。

**Variable.backward(grad_variables = W)：将一个向量Variable先按照同形的W来加权，得到一个标量，再根据这个标量对叶子节点来分别求导。若Variable本身就是一个标量，那可以省略这个参数默认为1。**

# 计算图

计算过程实际构建了一个从叶子结点到根结点的计算图，反向传播即为从根节点往下的溯源过程。

|变量|含义|
|---|---|
|`v.grad_fn`|指向**创建**该Variable的Function|
|`v.grad_fn.next_functions`|本节点接收的上级节点的Function，即创建上级节点的Function|

注意v.grad_fn是一个Function变量，表示该Variable是怎么来的。而Function变量含有`.next_functions`方法查看其反向传播中下面节点的Function，**注意其返回值是一个tuple**，内部元素也是`(Function, num)`类型的tuple，所以常用`v.grad_fn.next_functions[0][0]`这种用法。

若是用户创建的Variable，即叶子结点，其`v.grad_fn = None`。但若在其上级节点中的`.next_functions`中查看，没有将`requires_grad`置位的依然为None，将`requires_grad`置位的类型为`AccumulatedGrad`，即前面讲过的梯度累加的叶子结点。

`v.backward(retain_graph = True)`意为在进行反向传播后，保存计算图及中间结果，为False时自动释放计算图。一般不用设置为True。

PytTorch的运算图是在前向传播时动态构建的，不需要像某T框架一样设计好计算图框架。这一点在自然语言处理中很有用？

`volatile = True`这个在PyTorch0.4.0版本中已经被弃用，改为`torch.no_grad():`环境下运行无反向传播的代码。

在一次`v.backward()`执行完毕后，中间变量的.grad数据都被清空为None，只保留叶子结点的grad。查看根节点z对中间结点y的梯度可以用`t.autograd.grad(z, y)`或者hook函数，方法如下：
```python
def variable_hook(hook):
	print('y的梯度：\r\n', grad)
hook_handle = y.register_hook(variable_hook)
z.backward()
hook_handle.remove()
```
注意用完之后注销hook。

# 扩展autograd

没有集成的一些特殊函数可以自行实现前向传播和反向传播，且手动求导并实现会比系统集成的更快。

# 线性回归

在画图时不要用Variable变量进行运算，不要在计算图中引入无关的变量。几个注意的点：

1. `t.rand()`为(0, 1)内的均匀分布，`t.randn()`为正态分布。
2. 前向传播时标准写法：`y_pred = x.mm(w) + b.expand_as(x)`，而利用Broadcast可以写为`y_pred = x * w + b`，因为此处为直线所以可以这么写，重载的加减乘除为`element-wise`元素操作。
3. `loss = 0.5 * (y_pred - y) ** 2`注意这里的y_pred - y也是element-wise操作，反向传播前将其变为标量：`loss = loss.sum()`。
4. 更新参数时：`w.data.sub_(w.grad.data * lr)`，这里第一注意只有tensor才有inplace操作，所以更新`w.data`；第二查看variable的梯度用`w.grad.data`，.grad也是一个varibale变量；第三个记得要乘学习率！要乘学习率！要乘学习率！
5. 想做图时，要用到w和b做运算记得用他们的data，即`y = w.data * x + b.data`，避免在计算图中引入多余操作。
6. 每一个循环的最后注意`w.grad.data.zero_()`，因为创建的variable梯度是累加的。