---

layout: post
title: PyTorch的一些坑
date: 2018-11-07 09:28:09
tags: [PyTorch]
categories: [PyTorch]
typora-root-url: ../../static
---

# 关于Tensor

## autograd

0.4以后已经和Variable合并，有了自动微分功能。当`requires_grad`属性为真时，记录这个tensor的grad值，模型前向网络中的参数的这个属性都为真。grad与tensor同形，记录着每一个位置对应的导数。每次执行f(x).backward()都会***累加grad(accumulated)***，这是每个optimizer.step()后zero_grad()的原因。

反向传播实际上就是：

```python
for p in net.parameters():
  p.data.sub_(p.grad.data * learning_rate) # inplace运算
```

这里取p的方式是按矩阵，例如Linear中会返回[out, in]和[out]形状的两个tensor。

- grad_fn: 叶子结点的grad_fn是None，因为自己创建了并没有经过运算，非叶子结点为True，记录着这个结点是由什么运算得来的。
- is_leaf: ***用户自己创建的节点属于叶子结点***。
- requires_grad: 叶子和根结点都要是True，是True才会记录他们的计算图。两个True的tensor计算得到的tensor的requires_grad属性也是true。

***所有对于tensor的操作都会被记录，包括torch.cat等操作，导致内存爆炸。***

## tensor.grad_fn

输出该tensor是由什么操作得来的。用户所创建的leaf node的grad_fn是None。

## grad_fn.next_functions

会输出grad_fn的输入grad_fn，即看上一步计算是怎么来的。常用方法是next_functions[0] [0]。两个运算得到的结果要用next_functions[1] [0]。

而那些需要求导的variable，例如参数等等，其grad_fn值是AccumulateGrad，也意味着它们的梯度是**累加的**。

## tensor.backward(gradient, retain_graph)

loss.backward()计算**每个参数的梯度**，并保存在(model.)parameter.grad中。***但是没有进行parameter的更新，只是计算出grad而已。***因此严格上来讲也不能叫做反向传播，只是计算导数并没有更新网络参数，***optimizer.step()***是真正的更新parameter。

梯度累积意味着，在调用optimizer.step()实现梯度下降之前，我们会求取parameter.grad张量中的几个反向操作的梯度和。

若一直进行backward()而没有zero_grad()，梯度会一直累积，所谓梯度累积训练就是经过N步backward()之后再进行一次optimizer.step()。

第一个参数在tensor为矢量时使用，一般情况tensor是loss是个0-dimensional tensor可以直接反向。

### grad_variable

如果此处tensor是一个scalar，那么不需要grad_variables这个参数。如果是一个矩阵，则该参数应该是一个形状相同的tensor，本质原因是***tensor无法对tensor求导***，所以我们会***先进行tensor与grad_variables的矩阵相乘得到一个scalar***，之后再利用这个scalar对影响到tensor的每个参数求导。所以这个grad_variables变量可以认为是各个元素的weight。

### retain_graph

为了内存的利用，每次backward()后，计算图会被释放。而如果我还需要这个计算图，就需要置这个参数为1。

这种情况在一个网络有多个输出的情况下非常重要。

## tensor.data

如果我们想要对tensor的值进行运算，但是不希望其被autograd引入计算图，就可以用.data方法。

## !=

两个`同形（不同会broadcast为同形，因此其中一个可以为一个数的tensor）`tensor可以用`!=`进行运算，返回一个同长度的tensor，相等的位置为0，不等的位置为1。若是两个list，返回值为bool类型，1或0。

## 填充值

tesnor.fill_(value)，改变tensor中的值，其中value必须为数字。

## 乘法

|函数|含义|
|---|---|
|tensor.bmm|batch乘法，必须是`b*n*m`与`b*m*p`的两个阵做运算，得到`b*n*p`的结果|
|tensor.mul|element-wise matrix multiply|
|tensor.mm|二维矩阵相乘|

## 转换

|函数|含义|
|---|---|
|tensor.tolist()|高维tensor转为list类型|
|tensor.item()|只包含一个元素的tensor转换为number，`loss.item()`|

## 变形

|函数|含义|
|---|---|
|tensor.view(1, -1)|如一个一维向量进行这个操作，不只是形状，且升级到二维|
|tensor.transpose(1, 2)|转置，交换两个维度|
|tensor.permute(0, 2, 1)|交换维度，可以对所有维度同时进行|

- .view()方法只能作用于**连续内存**，如果在取batch时做了切片等操作，那么数据就不是连续内存上的，需要在view前进行`tensor.contiguous().view()`这样的操作使其连续。

## 生成

传入list时，作转换作用。传入的不是list时，认为传入的是形状，随机生成。

|函数|含义|
|---|---|
|LongTensor(5)|这种情况认为5是一个维度，会生成一个形状为5的一维向量|
|LongTensor(5,1)|生成一个形状为5*1的tensor|
|LongTensor([5,1])|传入一个list，这是作转换作用，返回一个tensor([5,1])|

## max比较

1. max(tensor)返回tensor中最大的一个数，以tensor的形式。
2. max(tensor, dim)**返回(Tensor, LongTensor)**，分别是指定维度上最大的数，和其inx。
3. max(tensor1, tensor2)两个同形tensor，返回一个tensor，每个位置元素为1和2中最大的那个。

## size()

tensor.size()返回一个torch.Size类型，而tensor.size(0)返回一个int。

## 随机

|函数|含义|
|---|---|
|torch.randn_like(tensor)|生成和tensor同形状的Normal Distribution，均值0，方差1|

## torch.sort(tensor, descending=True)

对tensor的**每行**进行排序，默认是升序ascending。返回**(sorted_tensor, sorted_indices)**。

## tensor[tensor]

利用一个indices的tensor对另一个tensor进行行上的重新排序。

## tensor.index_select(dim, index)

在指定dim上，选择index(LongTensor)指定的顺序的行/列。可以认为是reorder的一种方式。

## tensor.cat(list, dim)

第一个参数***必须是tensor组成的list/tuple***，***dim中0是行1是列，一行一行拼起来或是一列一列拼起来。***

例如dim=1时，必须在0维（行）上要match，此时一列一列拼接，行数不变，列数增加，即一行中的元素数目增加。

## 切片

tensor[:, -1:]取最后一列，因为第二个冒号存在所以保持二维。
tensor[:, -1]取出最后一列，作为一个一维tensor。切片中一旦出现数字，就意味着维度的消失。

## tensor.repeat(\*size)

`tensor.repeat(4,2)`把这个tensor在行上复制四次，在列上复制两次。

## torch.triu(tensor, dim=0)

将tensor返回一个上三角矩阵，默认保留对角线元素。如果dim=1那么就少保留一斜线，dim=-1往下多留一斜线。

## tensor.any()

tensor必须是bool类型，判断是不是有任何非零值。

## tensor.mased_fill_()

`masked_fill_`函数的意思是，***mask是一个二值的ByteTensor，其值为1的位置需要用value(float)来代替***。注意value可以直接用数字或者用0维tensor(torch.tensor(5))。

## torch.randint(low = 0, high, size, device)

size是一个tuple，比如torch.randnint(5, (1,1))。默认low是0，第一个参数就是high。

⚠️**low(inclusive) high(exclusive)**.



# torch.multinomial

`torch.multinomial(input, num_samples, replacement=Flase)`，第一个参数input可以认为是weight/probability，每行是一个概率分布（和不一定是1），按照这个分布为权重取值，返回的是num_samples个被取到的下标，即参数input的形状为`(num_rows, weight)`，返回值形状为`(num_rows, num_samples)`。replacement表示有无放回。**不是绝对的按照weight取的，概率意义上的权重。**

torch.multinomial(torch.ones(n), 1)可以做到在1到n中随机取一个数。

**⚠️注意input中的权重值不能为负数！**

# dataset & dataloader

Tensors that have the same size of the first dimension. 第一个维度相等，意味着样本数量相等，number * 样本（可以是一维样本例如全都是index，可以是二维图片等等）。取batch就是沿着第一个维度取的。

```python
import torch.utils.data as Data
torch_dataset = Data.TensorDataset(x, y)	#这里的x和y必须是tensor类型，第一个维度为batch
loader = Data.DataLoader(
	dataset = torch_dataset,
	batch_size = BATCH_SIZE,
	shuffle = True,
	num_workers = 2,
	drop_last = True,		#抛弃最后一个，因为长度不一定
	pin_memory = True,		#把数据放到CUDA
	)
for step, (batch_x, batch_y) in enumerate(loader):
	pass
```

- pin_memory = torch.cuda_is_available
- shuffle=split=='train'
- num_workers=cpu_count()
- pin_memory=torch.cuda.is_available()

# nn.NLLLoss()

Negative Log Likelihood Loss，输入为(batch_size, vocab)，标签为(batch_size)，注意标签不需要显示的使用one-hot tensors，只需要使用整数表示位置即可。

原理为只关注**正确标签对应的logits**，**若使用one-hot标签，**对输入中的值全部加负号，有batch维度的话求均值，然后让它变小即让正确标签位置数据变大。

**⚠️注意NLLoss本身并没有Logsoftmax之类的操作！直接把输入的input按照label来计算！**

⚠️**我们期待使用NLL的前向传播时，给的第一个参数是logsoftmax之后的概率！所以使用NLLLoss之前一般在网络的最后一层加一个LogSofmax层。**

Softmax让logits位于[0, 1]范围内，而LogSofmax可以让数据位于[-无穷, 0]内。


# nn.CrossEntropyLoss

即LogSoftmax+NLLLoss()的组合。

接收两个参数，第一个是(N, class)形状的矩阵，其中N表示样本数，class表示类别数量，其中的数字未经过softmax的，第二个参数是一维的，(N)形状，必须为**LongTensor类型**，其中的数据为indices，不可one-hot。

所以对于常见的数据格式而言，经过前向传播得到的out格式为(batch, time-step, vocab_size)，需要`out.view(-1, vocab_size)`，相当于把所有句子连起来，一行一个logits。而target原本为(batch, time-step)，需要`target.view(-1)`。

输出为一个0维tensor，要记录到list的话使用**loss.item()**。

# nn.KLDivLoss

计算两个分布之间的距离，参数和交叉熵不同。但是也返回0维tensor。

# nn.Embedding

1. 索引是用**LongTensor**的indices(index的复数)，不用one-hot向量。这一点在定义数据集的时候就要注意。
2. 如果不指定初始化方式，默认使用(0, 1)的normal distribution正态分布。Embedding层有一个weight属性，是Parameter初始化的，可以通过`embedding.weight.data.uniform_(-1, 1)`改为均匀分布。
3. 不管输入的是什么格式的数据，都会在其`n * m`的维度后加上嵌入维度d，变为`n * m * d`类型的输出。比如我直接输入一个一维的一百个词的indices，`tensor([100])`，经过Embedding层后得到的是`tensor([100, dim])`每一行为一个词向量。没有固定要求，视后面的运算而定，加上batch的三维信息也可以。

# nn.Dropout

目的是正则化，防止神经元之间互相依赖。将一个tensor中的部分元素按照概率p置换为0，其他部分乘以缩放因子，使得总的大小保持相对稳定。

# nn.LSTM

## 定义

首先LSTM是一个重复使用的单元，只有一套参数，所以定义时只要规定好输入数据维度、隐层数据维度、层数和方向就可以了，time-step没限制，重复使用这个单元就可以，体现在输入中。

网络定义时，参数为`nn.LSTM(input_size, hidden_size, num_layers, batch_first, bidirectional)`，注意定义网络时没有time_step这个属性。其中**batch_first置位会影响input和output的形状，而对h_n和c_n无影响，它们的batch信息还是在中间维度。**

## 网络输入

lstm模块接收的参数为`(inputs, (h_n, c_n))`，前者为喂入网络的数据，格式**必须**为`(batch_size, time_step, feature_length)`。后者不传也可以，缺省状态为全0。

注意训练时输入的time-step和后面evaluate时候的time-step不必一致，只要保存下(h_n, c_n)的state，再作为下一次循环的输入就可以了。`output, state = model(input, state)`。

## 网络输出

网络返回两（三）个参数，`output, (h_n, c_n)`。第二个参数这种tuple的形式和输入第二参数是一致的，以供循环传递隐状态。

1. output的形状为`(batch(first), time_step, hidden_size * num_direction)`，保存了**最后一层**LSTM的**每一个time-step**隐状态即输出。如果是双向的，h_n的形状为(num_direction * hidden_size)，是单纯的拼接起来，取其`[:hidden_size]`个向量得到正向的这一步的输出。
2. h_n形状为`(num_layers * num_directions, batch, hidden_size)`，保存了**每一层**LSTM的**最后一个time-step（包括正反向）**的隐状态输出。如果是双向的，**第一个维度按（第一层正向，第一层反向，第二层正向，第二层反向…）**顺序来保存。
3. c_n同h_n，保存的是cell_state。

SGD在RNN中的应用？一个batch而言怎么sgd

# nn.Linear

Linear接受的输入是`(N, *, in_features)`并输出`(N, *, out_features)`，也就是说对最后一个维度进行线性处理，输出维度与之前格式相同。


# Evaluation

测试时，使用`with torch.no_grad()`环境来关闭计算图功能。一些常用的evaluate方法：

1. `_, prediction = torch.max(output.data, 1)`返回output中最大的数的下标，认为就是其类别。
2. `correct += (prediction == labels).sum().item()`，其中`tensor == tensor`按照对应位置是否相同返回是1或0，sum求和，item转换为python的number。

# batch_training

yunjey的这种构造batch_training的方法和train_loader的不同？好像他这样才是真正的batch

测试时候所谓的窗口大小信息又用在哪了？还是说训练就这么训练，我后面就不那么用。认为可以捕捉所有的上下文。

# 变长rnn序列

## padding & sort

首先padding到max_length，且记录每个样本的length，按长度降序排列。首先input是个list。

```python
length = len(input)

input.extend(['<pad>'] * (max_length - length))

sorted_lengths, sorted_idx = torch.sort(length_list, descending=True)

input_sequence = input_sequence[sorted_idx]
```

## nn.utils.rnn.pack_padded_sequence(input, length, batch_first=True)

1. 目的：为了RNN训练时的mini-batch training，所以有补齐一个minibatch中数据长度的需要，如果是每次一个样本的话，就没有这个必要了。
2. 输入：input是一个**经过padding，embedding，descending_order之后的**三维tensor。length是一个每一行样本有效长度序列。
3. 输出：返回一个PackedSequence对象，可以直接输入到RNN网络中。

## nn.utils.rnn.pad_packed_sequence(output, batch_first=True)



# 卷积

## nn.Conv2d(in_channels, out_channels, kernel_size)

**out_channel是卷积核的数量**，in_channel在输入图片的通道数，而卷积核（三维的，深度自动等于in_channels）的深度默认和输入通道一致，虽然有深度但是所有位置数字相乘加起来，算法一样的。所以一个kernel把多通道的图片压缩为一通道的平面矩阵，所以有多个kernel就会产生多个channel堆叠。这个out_channel也就是再接入下一层的in_channel大小。

- 虽然是2d，卷积核依然是3维有深度的。
- 一个卷积核对应着一个out_channel，生成一张feature map。
- Kernel_size也可以叫patch size，每次关注的区域。

输入数据`(batch, in_channels, height, width)`，得到`(batch, out_channels, new_height, new_width)`，从图片角度讲channel自然而然的放到第二个维度。

## nn.Conv1d(in_channels, out_channels, kernel_size)

类比，**in_channel是一个会自动被kernel覆盖的量**，我想覆盖整个词向量所以in_channel是embedding_size，kernel此时只是二维的，第一个维度是指定的，第二个维度自动覆盖in_channel。

注意输入数据维度与2d保持一致，`(batch, in_channels, length)`，即**词向量的维度embedding_size要放在中间**，而一般情况下**从Embedding取出来的词向量embedding_size处于最后一个维度**，所以要交换维度。
最后得到的输出也与2d保持一致，`(batch, out_channels, new_length)`。

卷积过程理解为，先把形状转置为(N, C, L)，然后卷积核横着走一遍，得到一个**行向量，作为一个channel的值**，然后下一个kernel再来卷一遍，得到下一行，所以最终行数等于out_channels，列数新定。

### 对比2d和1d

在2d中，每个像素点的属性是三维的(RGB)，所以一个kernel对应于每个点的乘加系数也是三维的(三个数)。

而在1d中，每个词的属性是embedding_size那么多维度的，所以一个kernel对应于每个点的乘加系数是embedding_size维度的。

在实现中，每个点有多少个属性就叫in_channels，kernel_size中不用指定这个维度，所以在图像中只需要指定剩余的2d平面上的(axb)为size，而在文本1d卷积中只需要(k)这个patch size就可以了。

![](/img/conv1d.png)



## nn.MaxPool1d

一维卷积是覆盖所有in_channel（卷积核实质上也理解为二维的），但是一维池化不是，一维池化的kernel就是一维的窗(1xkernel_size)在每一个channel上走，**默认stride=kernel_size（conv默认stride=1）**，不交叠，可以设置stride来改变。

输入`(N, Channel, Length)`，输出`(N, Channel, New_Length)`，前两个维度不变。

## nn.ConvTranspose1d(in_channels, out_channels, kernel_size, padding)

1. **根据padding值添加(kernel_size - 1 - padding)个zero-padding**，即padding就算等于0，也会添加kernel_size-1个零。
2. 这么做是为了让相同参数的Conv层和ConvTranspose层可以是互为对应的，在维度上可以互相恢复。
3. 计算TransConv的输出维度时，用计算Conv1d的反向算法，即认为我们要求的输出length是Conv1d的输入，反过来算，**无视上面的1条。**

# 批标准化

## nn.BatchNorm1d()

模型收敛的更加快速，对每个dimension进行normalization。

在test的时候只有1个data进来，不能在batch上计算均值和方差了，所以在test的时候，计算整个训练集的均值和方差，应用在test的时候。

# Numpy

## x.tolist()

np的元素打印出来虽然也有个中括号，但是毕竟是numpy类，且打印出来以空格隔开的。转为list以逗号隔开。

## np.randint(low, high=None, size)

若是np.randint(5, size=10)默认5是上限，生成一个一维的不超过5的10个数。

## np.copy(X)

若是直接`y=x`则两个变量指向同一内存空间，修改一个另一个也跟着同时改变。若是`y = np.copy(x)`则开辟了一块新的内存空间，不共享。

## 加法操作

在list操作中，直接两个`[]`list作加法，是连接起来。但是如果是两个numpy，作加法，是broadcast。

# 实现word_dropout

```python
if self.word_dropout_rate > 0:
	# randomly replace decoder input with <unk>
	prob = torch.rand(input_sequence.size())
	prob[(input_sequence.data - self.sos_idx) * (input_sequence.data - self.pad_idx) == 0] = 1
	decoder_input_sequence = input_sequence.clone()
	decoder_input_sequence[prob < self.word_dropout_rate] = self.unk_idx
	 = self.embedding(decoder_input_sequence)
```

# 内存分配

可使用`torch.cuda.memory_allocated(device)`查看内存使用情况，单位是bytes，除10^9转换为GB。

```python
def print_allocated_memory(device):
    print("{:.2f} GB".format(torch.cuda.memory_allocated(device) / 1024 ** 3)) # 先算power
```

注意这里的device是一个torch.device属性变量。

# 梯度裁剪

`torch.nn.utils.clip_grad_norm_(model.parameters(), clip)`，clip is a float number.

