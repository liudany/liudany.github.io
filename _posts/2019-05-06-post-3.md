---
layout: post
title: PyTorch Multi-GPU Parallel and 0.4 Migration
date: 2019-05-06 09:28:09
tags: [PyTorch]
categories: [PyTorch]
typora-root-url: ../../static
---

# Multi-GPU

## æ–°çš„deviceå±æ€§

åœ¨0.4åŠä¹‹åçš„ç‰ˆæœ¬ä¸­ï¼Œå¯¹äºtensoråŠ å…¥äº†deviceå±æ€§ï¼Œå…¶å€¼ä¸º`torch.device('{device_type}:{device_ordinal}')`ç±»å‹ï¼Œæ‹¬å·é‡Œæ˜¯ç”¨å¼•å·å¼•èµ·çš„å­—ç¬¦ä¸²åˆ¶å®šcpu/cudaï¼Œä½¿ç”¨cudaæ—¶ä¹Ÿå¯ä»¥ä¸åˆ¶å®šdevice_ordinalï¼Œæ­¤æ—¶è¯¥å‚æ•°é»˜è®¤ä¸º`torch.cuda.current_device()`ï¼Œä¸”åªç”¨ä¸€å—GPUã€‚

ä¹‹å‰çš„`get_device()`æ–¹æ³•åªå¯¹cudaå˜é‡ç”Ÿæ•ˆï¼Œè¿”å›å…¶æ‰€åœ¨çš„gpuç¼–å·ï¼Œå¯¹cpuå˜é‡æŠ¥é”™ã€‚ä½†æ˜¯`tensor.device`æ–¹æ³•æ˜¯é€šç”¨çš„ï¼Œè¿”å›å…¶deviceå±æ€§ã€‚

åœ¨åˆ›å»ºtensoræ—¶å¯ä»¥ç›´æ¥åˆ¶å®šå…¶deviceå±æ€§ï¼š

```python
x = torch.randn(3, 3, device = torch.device('cuda:1'))
```

***æ­¤æ—¶é»˜è®¤çš„requires_gradå±æ€§æ˜¯falseï¼Œä¸ºä»€ä¹ˆï¼Ÿ***

## CPUå’ŒGPUè½¬ç§»

åœ¨0.3åŠä¹‹å‰çš„ç‰ˆæœ¬ä¸­ï¼Œç”¨`tensor/model.cuda()/.cpu()`æ–¹æ³•æ¥å°†æ•°æ®/æ¨¡å‹åœ¨GPU/CPUä¸­è½¬ç§»ã€‚

æ–°ç‰ˆæœ¬ä¸­ï¼Œä½¿ç”¨`tensor/model.to(device)`æ–¹æ³•æ¥å°†æ•°æ®/æ¨¡å‹åšè½¬ç§»ï¼Œè¿™é‡Œçš„deviceé¡»ä¸Šé¢æåˆ°çš„`torch.device`ç±»å‹çš„å±æ€§å˜é‡ã€‚

***æ³¨æ„toæ–¹æ³•è¿”å›ä¸€ä¸ªæ–°çš„tensorè€Œä¸æ˜¯rewriteåŸæ¥çš„tensorã€‚***

ä¸”è¿™ç§æ–¹æ³•åªé€‚ç”¨äº**å•GPU**ï¼Œå³å°†tensor/model's parametersåˆ†é…åˆ°å“ªä¸€ä¸ªgpuä¸Šã€‚

***âš ï¸å¦‚æœåªæœ‰ä¸€å—GPUï¼Œä½¿ç”¨toæ–¹æ³•å°±å¯ä»¥äº†ï¼Œæ²¡å¿…è¦dataparallelã€‚***

## DEVICE-AGNOSTIC CODE

åœ¨ä¸ç¡®å®šæœ‰æ— GPUæ—¶ï¼Œé€šç”¨çš„ä»£ç å†™æ³•ã€‚åˆ©ç”¨äº†`torch.cuda.is_available()`å‡½æ•°ç¡®è®¤æœ‰æ— GPUå¯ç”¨ã€‚

```python
# at the beginning of code
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
...

# whenever get a new tensor or module
input = data.to(device)
model = MyModule.to(device)
```

æ€»ç»“ä¸‹æ¥å°±æ˜¯åœ¨ä»£ç å¼€å¤´å…ˆå£°æ˜ä¸€ä¸ªdeviceå˜é‡ï¼Œæ ¹æ®å½“å‰æœ‰æ— GPUå¯ç”¨æ¥ç¡®å®šæ˜¯cudaè¿˜æ˜¯cpuå˜é‡ã€‚ä¹‹ååœ¨å¼•å…¥tensorå’Œmodelçš„æ—¶å€™ç»Ÿä¸€ä½¿ç”¨`to(device)`çš„æ–¹æ³•æ¥è¿›è¡Œè¿ç§»ã€‚

## Multi-GPU Parallelism

æ ¹æ®[å®˜æ–¹çš„ä¸€ä¸ªtutorial](https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html#sphx-glr-beginner-blitz-data-parallel-tutorial-py)å’Œ[ç›¸å…³æºç ](https://pytorch.org/docs/stable/nn.html#dataparallel-layers-multi-gpu-distributed)ï¼Œæ¨¡å‹å¤šGPUå¹¶è¡ŒåŒ–çš„ä¸€èˆ¬æµç¨‹æ˜¯ï¼š

```python
device = torch.device("cuda:0")
model = Model()
model = nn.DataParallel(model, device_ids)
model.to(device)
input/output.to(device)			# è¿™é‡Œè¦æ³¨æ„ï¼Œæ•°æ®è·Ÿæ¨¡å‹åˆå§‹è¦åœ¨åŒä¸€å—å¡ä¸Šï¼Œä¸€èˆ¬æ˜¯device_ids[0]
```

å³å…ˆå¹¶è¡ŒåŒ–æ¨¡å‹å¹¶rewriteè‡ªèº«ï¼Œ**ä¹‹åå†ç§»åŠ¨åˆ°*æŸä¸€å—GPUä¸Šï¼ˆbatchæ•°æ®åŒæ ·ä¹Ÿè¦åœ¨è¿™ä¸€å—å¡ä¸Šï¼ï¼‰*ï¼Œç„¶åå¤åˆ¶ä¸ºnä»½æ”¾åˆ°ä¸åŒçš„GPUï¼ŒåŒæ—¶å°†æ•°æ®çš„batchç»´åº¦ç­‰åˆ†ä¸ºGPUæ•°é‡ä»½ï¼Œä¹Ÿåˆ†åˆ°ä¸åŒGPUä¸Šè¿›è¡Œå‰å‘ä¼ æ’­ã€‚åå‘ä¼ æ’­æ—¶ï¼Œå„ä¸ªå¤åˆ¶æ¨¡å‹çš„æ¢¯åº¦ç´¯åŠ åˆ°æ¨¡å‹æœ¬æ¥æ‰€åœ¨çš„é‚£å—GPUä¸Šè¿›è¡Œè¿ç®—**ã€‚

æœ€é‡è¦çš„æ˜¯`torch.nn.DataParallel(model, device_ids, output_device, dim=0)`å‡½æ•°ï¼Œå…¶å‚æ•°æ„ä¹‰ï¼š

1. device_idsç±»å‹ä¸ºpython list(ä¾‹å¦‚[0, 1, 2])ï¼Œé»˜è®¤ä¸º`torch.cuda.device_count()`è¿”å›GPUæ€»æ•°ï¼Œå…·ä½“æ¥è®²æ˜¯`list(range(torch.duda.device_count))`ï¼Œä¹Ÿæ˜¯ä¸€ä¸ªlistã€‚
2. output_deviceé»˜è®¤ä¸ºdevice_ids[0]ï¼Œå³ä¸Šä¸€ä¸ªå‚æ•°é»˜è®¤çš„ç¬¬ä¸€å—ã€‚
3. ***dimå‚æ•°é»˜è®¤ä¸º0ï¼Œå³å°†åˆ‡åˆ†ç¬¬ä¸€ä¸ªç»´åº¦åˆ°ä¸åŒçš„GPUï¼Œé»˜è®¤ç¬¬ä¸€ä¸ªç»´åº¦ä¸ºbatchã€‚ç”±è¿™ç‚¹ä¹Ÿè¦æ³¨æ„ï¼Œåœ¨å¤šGPUåˆ†å¸ƒè®­ç»ƒæ—¶å¯ä»¥é€‚å½“çš„åŠ å¤§batch_sizeï¼Œå› ä¸ºä¼šç­‰åˆ†åˆ°å„å—GPUçš„ã€‚***

è¦äº†è§£multi-gpuè¿ç®—è¿‡ç¨‹å…ˆçœ‹çœ‹DataParallelè¿™ä¸ªå‡½æ•°çš„å®ç°ï¼Œç”¨äº†è¿™äº›primitives(åŸºå…ƒ)ï¼Œä»–ä»¬ä¹Ÿå¯ä»¥è¢«å•ç‹¬ä½¿ç”¨ï¼Œéƒ½åœ¨nn.parallelä¸‹ï¼š

- replicate(module, device_ids): æŠŠæ¨¡å‹å¤åˆ¶åˆ°å¤šå—GPUä¸Šã€‚
- scatter(input, device_ids): æŒ‰ç…§dimæŒ‡å®šå‚æ•°ï¼ŒæŠŠinputåˆ†ä¸ºæŒ‡å®šçš„GPUæ•°é‡ç­‰ä»½ã€‚
- gather(outputs, output_device): scatterçš„åæ“ä½œï¼ŒæŠŠtensorä»¬æŒ‰ç…§dimç»´åº¦åˆå¹¶èµ·æ¥ã€‚
- parallel_apply(replicas, inputs): å°†åˆ†è§£å¥½çš„è¾“å…¥(inputs)åˆ†åˆ«é€åˆ°å¤åˆ¶å¥½çš„æ¨¡å‹(replicas)ä¸­ã€‚

å…·ä½“è¿‡ç¨‹å¦‚ä¸‹ï¼š

![](/img/multi-gpu1.png)

å¦‚å›¾ï¼Œå¯¹äºdevices_ids[0]è¿™å—GPUæ€»æ˜¯ä¼šå¤šåˆ©ç”¨ä¸€äº›ç©ºé—´çš„ï¼Œæ˜¯**ä¸å‡è¡¡**çš„è®¡ç®—ï¼Œè§£å†³æ–¹æ³•å¯ä»¥çœ‹[è¿™ç¯‡æ–‡ç« ](https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255)ã€‚

# Else

## Creating New Tensor

`torch.*_like(tensor)`å°†åˆ›å»ºå’Œtensorä¸€æ ·çš„å½¢çŠ¶/å±æ€§çš„æ–°tensorï¼Œä¾‹zeros_likeæˆ–è€…ones_like(éƒ½æœ‰s)ï¼Œä¹Ÿå¯åœ¨åˆ›å»ºæ—¶å£°æ˜éœ€è¦æ›´æ”¹çš„å±æ€§ï¼Œä¾‹å¦‚`torch.zeros_like(x, dtype=torch.int)`å°±ä¼šä¿®æ”¹dtypeå±æ€§ã€‚

`tensor.new_*(shape)`å°†è¿”å›å’ŒtensoråŒå±æ€§ï¼Œä½†æ˜¯å½¢çŠ¶ä¸ºshapeçš„æ–°tensorã€‚

`tensor.type()`å¯ä»¥è¿”å›tensorçš„ç±»å‹ï¼Œä½†æ˜¯`type(tensor)`åªèƒ½å¾—åˆ°æ˜¯ä¸ªtensorï¼Œå…·ä½“ç±»å‹æœªçŸ¥ã€‚

## tensor.data vs tensor.detach()

0.4å¼€å§‹ï¼ŒVariableåŠautogradæ“ä½œæ­£å¼åˆå…¥Tensorï¼Œè™½ç„¶Variable(tensor)ä»ç„¶å¯ä»¥ä½¿ç”¨ï¼Œä½†æ˜¯æœ¬è´¨ä¸Šå®ƒä»€ä¹ˆéƒ½æ²¡åšã€‚æƒ³ä½¿tensoræœ‰autogradåŠŸèƒ½ï¼Œéœ€è¦è®¾ç½®`tensor.requries_grad=True`ï¼Œæˆ–è€…åœ¨ç”Ÿæˆtensoræ—¶æŒ‡å®šå…¶requires_gradå±æ€§ã€‚

ç°åœ¨`y=x.data`è¿™ç§æ“ä½œï¼Œä¼šå¾—åˆ°ä¸€ä¸ªä¸xæ•°æ®ç›¸åŒçš„ï¼Œrequires_grad=Falseçš„æ–°tensorï¼Œä½†æ˜¯***äºŒè€…å…±äº«å†…å­˜***ï¼Œå¦‚æœæ­¤æ—¶å¯¹yè¿›è¡Œ***inplace operation***çš„è¯ï¼Œxçš„å€¼ä¼šéšä¹‹æ”¹å˜ï¼Œ***ä¼šä½¿backwardå¾—åˆ°é”™è¯¯çš„å¯¼æ•°***ï¼Œä¸”ç¨‹åºå¹¶ä¸ä¼šæŠ¥é”™ã€‚

è€Œå¦‚æœä½¿ç”¨`y=x.detach()`çš„è¯ï¼Œä¸ä¸Šé¢çš„yå±æ€§ç›¸åŒï¼Œä»ç„¶å…±äº«å†…å­˜ï¼Œä½†æ˜¯è¿™æ—¶å€™å¦‚æœä¿®æ”¹äº†yçš„å€¼å¯¼è‡´xçš„å€¼ä¹Ÿè·Ÿç€ä¿®æ”¹ï¼Œå°±ä¼šå¯¼è‡´åœ¨backward()å‡½æ•°æ‰§è¡Œæ—¶æŠ¥é”™ï¼š

```python
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation.
```

æ‰€ä»¥ä½¿ç”¨detach()æ–¹æ³•æ˜¯æ›´åŠ å®‰å…¨çš„ï¼



***ä¸€å®šè¦æ³¨æ„ï¼æˆ‘ä»¬éœ€è¦å‚æ•°çš„å¯¼æ•°ï¼Œæ”¹å˜æ¨¡å‹çš„å‚æ•°è¾¾åˆ°æ›´å¥½çš„æ•ˆæœï¼ä¿å­˜çš„ä¹Ÿæ˜¯å‚æ•°å¯¹åº”çš„å¯¼æ•°ï¼è¾“å…¥è¾“å‡ºè¦ä¸ªğŸ”¨çš„å¯¼æ•°ï¼ä¸éœ€è¦ï¼***

## 0-Dimensional Tensor

å¼•å…¥0ç»´çš„scalarï¼Œç»Ÿä¸€äº†ä¹‹å‰ç‰ˆæœ¬çš„å¦‚ä¸‹é—®é¢˜ï¼š

- tensor[0]è¿”å›çš„æ˜¯ä¸€ä¸ªpythonæ•°å­—ï¼Œè€Œvariable[0]è¿”å›çš„æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º(1,)çš„å‘é‡ã€‚
- åŒæ ·çš„ï¼Œtensor.sum()è¿”å›æ•°å­—ï¼Œvariable.sum()è¿”å›ä¸€ä¸ªsize(1,)çš„å˜é‡ã€‚

0.4ä¹‹åçš„ç‰ˆæœ¬***indexå’Œsumæ“ä½œçš„è¿”å›å€¼éƒ½æ˜¯0-dimensional tensoräº†***ï¼Œå¯ä»¥åˆ©ç”¨`torch.tensor(3.14)`è¿™ç§æ–¹å¼åˆå§‹åŒ–ä¸€ä¸ª0ç»´tensorï¼Œå…¶ç»´åº¦.size()è¿”å›çš„æ˜¯`torch.Size([])`ï¼Œè€Œä¸€ä¸ª1ç»´çš„æ•°ç»„ä¾‹å¦‚torch.tensor([3.14])å…¶ç»´åº¦æ˜¯torch.Size([1])ã€‚å¯ä»¥çœ‹å‡º***pytorchä¸­ç»´åº¦å±æ€§éƒ½æ˜¯ç”¨ä¸­æ‹¬å·æ‹¬èµ·æ¥çš„ï¼Œ0ç»´å°±å•¥éƒ½æ²¡æœ‰ï¼Œ1ç»´åªæœ‰ä¸€ä¸ªæ•°å­—ï¼ŒäºŒä½ä¸¤ä¸ªæ•°å­—ç­‰ç­‰***ã€‚

`tensor_0.item()`æ–¹æ³•å¯ä»¥è®©0ç»´tensorå˜æˆpython numberã€‚

### å…³äºLoss

0.3ä¹‹å‰çš„ç‰ˆæœ¬ï¼Œ**ç»è¿‡criterionå¾—åˆ°çš„**Lossæ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º(1,)çš„variableï¼Œä½†æ˜¯0.4å¾€å***Lossæ˜¯ä¸€ä¸ª0ç»´scalar***ï¼Œç”¨ä¸Šè¿°æ–¹æ³•æ“ä½œå®ƒã€‚

```python
total_loss += loss.data[0] # before 0.4 version
total_loss += loss.item()  # now
```

âš ï¸æ³¨æ„å¦‚æœä¸å°†lossè½¬æ¢ä¸ºpython numberè€Œç›´æ¥çš„å»ç´¯ç§¯å®ƒï¼ˆä¸€ä¸ªtensorï¼‰ï¼Œä¼šä¸æ–­çš„å¢å¤§è®¡ç®—å›¾ï¼Œè€—å°½æ˜¾å­˜ã€‚

## Volatile

0.4ä¹‹åçš„ç‰ˆæœ¬å¯ç”¨äº†volatileï¼Œåœ¨ä¸éœ€è¦è®°å½•è®¡ç®—å›¾çš„æ—¶å€™ä½¿ç”¨ï¼š

```python
with torch.no_grad():

or

torch.set_grad_enabled(True/False)

or

is_train = False
with torch.set_grad_enabled(is_train):
```

## a diagram from forum

![](/img/dataparallel.png)

[è¿™é‡Œ](https://discuss.pytorch.org/t/uneven-gpu-utilization-during-training-backpropagation/36117/5)ä¹Ÿè§£é‡Šäº†ä¸ºä»€ä¹ˆè¦åœ¨primary gpuä¸Šè®¡ç®—lossï¼Œå› ä¸º***è®¡ç®— lossæ‰€éœ€è¦çš„target tensorå¤„åœ¨primary gpuä¸Š***ï¼Œæ‰€ä»¥éœ€è¦å°†outputæ±‡åˆä¸€ä¸‹è®¡ç®—lossã€‚

ä¹‹åå†å°†lossæŒ‰åŸæ¥batchçš„åˆ†æ•£æ–¹æ³•è¿”å›æ¯ä¸ªgpuä¸Šï¼Œåˆ†åˆ«è®¡ç®—gradå€¼ï¼Œä¹‹åå°†æ‰€æœ‰çš„gradèšé›†åˆ°default gpuä¸Šï¼Œå¯¹æ¨¡å‹è¿›è¡Œå‚æ•°çš„æ›´æ–°ï¼Œç„¶åé‡å¤ä¸‹ä¸€è½®ã€‚

[å¦ä¸€ä¸ªé—®é¢˜é‡Œ](https://discuss.pytorch.org/t/how-pytorchs-parallel-method-and-distributed-method-works/30349/7)è°ˆäº†ä¸ºä»€ä¹ˆè¦å°†outputæ±‡åˆåˆ°primary gpuå†è®¡ç®—lossï¼Œè€Œä¸æ˜¯scatter targetåˆ°ä¸åŒGPUç„¶ååˆ†å¸ƒå¼è®¡ç®—lossï¼Œ**ä»–è®¤ä¸ºè®¡ç®—lossæ˜¯ä¸ªå¾ˆcheapçš„ä»»åŠ¡ï¼Œåˆ†å¸ƒè®¡ç®—ä¸ä¼šè·å¾—å¾ˆå¤šå¢ç›Šã€‚**

[ä¸€ä¸ªå…³äºDataParallelå®ç°ç»†èŠ‚çš„è®¨è®º](https://erickguan.me/2019/pytorch-parallel-model)



- å¯ä¸å¯ä»¥è®¤ä¸ºè®¡ç®—ç©outä¹‹åï¼Œhas nothing to do with data parallel. all depends on your code.