---
layout: post
title: PyTorch Multi-GPU Parallel and 0.4 Migration
date: 2019-05-06 09:28:09
tags: [PyTorch]
categories: [PyTorch]
typora-root-url: ../../static
---

# Multi-GPU

## 新的device属性

在0.4及之后的版本中，对于tensor加入了device属性，其值为`torch.device('{device_type}:{device_ordinal}')`类型，括号里是用引号引起的字符串制定cpu/cuda，使用cuda时也可以不制定device_ordinal，此时该参数默认为`torch.cuda.current_device()`，且只用一块GPU。

之前的`get_device()`方法只对cuda变量生效，返回其所在的gpu编号，对cpu变量报错。但是`tensor.device`方法是通用的，返回其device属性。

在创建tensor时可以直接制定其device属性：

```python
x = torch.randn(3, 3, device = torch.device('cuda:1'))
```

***此时默认的requires_grad属性是false，为什么？***

## CPU和GPU转移

在0.3及之前的版本中，用`tensor/model.cuda()/.cpu()`方法来将数据/模型在GPU/CPU中转移。

新版本中，使用`tensor/model.to(device)`方法来将数据/模型做转移，这里的device须上面提到的`torch.device`类型的属性变量。

***注意to方法返回一个新的tensor而不是rewrite原来的tensor。***

且这种方法只适用于**单GPU**，即将tensor/model's parameters分配到哪一个gpu上。

***⚠️如果只有一块GPU，使用to方法就可以了，没必要dataparallel。***

## DEVICE-AGNOSTIC CODE

在不确定有无GPU时，通用的代码写法。利用了`torch.cuda.is_available()`函数确认有无GPU可用。

```python
# at the beginning of code
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
...

# whenever get a new tensor or module
input = data.to(device)
model = MyModule.to(device)
```

总结下来就是在代码开头先声明一个device变量，根据当前有无GPU可用来确定是cuda还是cpu变量。之后在引入tensor和model的时候统一使用`to(device)`的方法来进行迁移。

## Multi-GPU Parallelism

根据[官方的一个tutorial](https://pytorch.org/tutorials/beginner/blitz/data_parallel_tutorial.html#sphx-glr-beginner-blitz-data-parallel-tutorial-py)和[相关源码](https://pytorch.org/docs/stable/nn.html#dataparallel-layers-multi-gpu-distributed)，模型多GPU并行化的一般流程是：

```python
device = torch.device("cuda:0")
model = Model()
model = nn.DataParallel(model, device_ids)
model.to(device)
input/output.to(device)			# 这里要注意，数据跟模型初始要在同一块卡上，一般是device_ids[0]
```

即先并行化模型并rewrite自身，**之后再移动到*某一块GPU上（batch数据同样也要在这一块卡上！）*，然后复制为n份放到不同的GPU，同时将数据的batch维度等分为GPU数量份，也分到不同GPU上进行前向传播。反向传播时，各个复制模型的梯度累加到模型本来所在的那块GPU上进行运算**。

最重要的是`torch.nn.DataParallel(model, device_ids, output_device, dim=0)`函数，其参数意义：

1. device_ids类型为python list(例如[0, 1, 2])，默认为`torch.cuda.device_count()`返回GPU总数，具体来讲是`list(range(torch.duda.device_count))`，也是一个list。
2. output_device默认为device_ids[0]，即上一个参数默认的第一块。
3. ***dim参数默认为0，即将切分第一个维度到不同的GPU，默认第一个维度为batch。由这点也要注意，在多GPU分布训练时可以适当的加大batch_size，因为会等分到各块GPU的。***

要了解multi-gpu运算过程先看看DataParallel这个函数的实现，用了这些primitives(基元)，他们也可以被单独使用，都在nn.parallel下：

- replicate(module, device_ids): 把模型复制到多块GPU上。
- scatter(input, device_ids): 按照dim指定参数，把input分为指定的GPU数量等份。
- gather(outputs, output_device): scatter的反操作，把tensor们按照dim维度合并起来。
- parallel_apply(replicas, inputs): 将分解好的输入(inputs)分别送到复制好的模型(replicas)中。

具体过程如下：

![](/img/multi-gpu1.png)

如图，对于devices_ids[0]这块GPU总是会多利用一些空间的，是**不均衡**的计算，解决方法可以看[这篇文章](https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255)。

# Else

## Creating New Tensor

`torch.*_like(tensor)`将创建和tensor一样的形状/属性的新tensor，例zeros_like或者ones_like(都有s)，也可在创建时声明需要更改的属性，例如`torch.zeros_like(x, dtype=torch.int)`就会修改dtype属性。

`tensor.new_*(shape)`将返回和tensor同属性，但是形状为shape的新tensor。

`tensor.type()`可以返回tensor的类型，但是`type(tensor)`只能得到是个tensor，具体类型未知。

## tensor.data vs tensor.detach()

0.4开始，Variable及autograd操作正式合入Tensor，虽然Variable(tensor)仍然可以使用，但是本质上它什么都没做。想使tensor有autograd功能，需要设置`tensor.requries_grad=True`，或者在生成tensor时指定其requires_grad属性。

现在`y=x.data`这种操作，会得到一个与x数据相同的，requires_grad=False的新tensor，但是***二者共享内存***，如果此时对y进行***inplace operation***的话，x的值会随之改变，***会使backward得到错误的导数***，且程序并不会报错。

而如果使用`y=x.detach()`的话，与上面的y属性相同，仍然共享内存，但是这时候如果修改了y的值导致x的值也跟着修改，就会导致在backward()函数执行时报错：

```python
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation.
```

所以使用detach()方法是更加安全的！



***一定要注意！我们需要参数的导数，改变模型的参数达到更好的效果！保存的也是参数对应的导数！输入输出要个🔨的导数！不需要！***

## 0-Dimensional Tensor

引入0维的scalar，统一了之前版本的如下问题：

- tensor[0]返回的是一个python数字，而variable[0]返回的是一个形状为(1,)的向量。
- 同样的，tensor.sum()返回数字，variable.sum()返回一个size(1,)的变量。

0.4之后的版本***index和sum操作的返回值都是0-dimensional tensor了***，可以利用`torch.tensor(3.14)`这种方式初始化一个0维tensor，其维度.size()返回的是`torch.Size([])`，而一个1维的数组例如torch.tensor([3.14])其维度是torch.Size([1])。可以看出***pytorch中维度属性都是用中括号括起来的，0维就啥都没有，1维只有一个数字，二位两个数字等等***。

`tensor_0.item()`方法可以让0维tensor变成python number。

### 关于Loss

0.3之前的版本，**经过criterion得到的**Loss是一个形状为(1,)的variable，但是0.4往后***Loss是一个0维scalar***，用上述方法操作它。

```python
total_loss += loss.data[0] # before 0.4 version
total_loss += loss.item()  # now
```

⚠️注意如果不将loss转换为python number而直接的去累积它（一个tensor），会不断的增大计算图，耗尽显存。

## Volatile

0.4之后的版本启用了volatile，在不需要记录计算图的时候使用：

```python
with torch.no_grad():

or

torch.set_grad_enabled(True/False)

or

is_train = False
with torch.set_grad_enabled(is_train):
```

## a diagram from forum

![](/img/dataparallel.png)

[这里](https://discuss.pytorch.org/t/uneven-gpu-utilization-during-training-backpropagation/36117/5)也解释了为什么要在primary gpu上计算loss，因为***计算 loss所需要的target tensor处在primary gpu上***，所以需要将output汇合一下计算loss。

之后再将loss按原来batch的分散方法返回每个gpu上，分别计算grad值，之后将所有的grad聚集到default gpu上，对模型进行参数的更新，然后重复下一轮。

[另一个问题里](https://discuss.pytorch.org/t/how-pytorchs-parallel-method-and-distributed-method-works/30349/7)谈了为什么要将output汇合到primary gpu再计算loss，而不是scatter target到不同GPU然后分布式计算loss，**他认为计算loss是个很cheap的任务，分布计算不会获得很多增益。**

[一个关于DataParallel实现细节的讨论](https://erickguan.me/2019/pytorch-parallel-model)



- 可不可以认为计算玩out之后，has nothing to do with data parallel. all depends on your code.