---
layout: post
title: 【翻译】《Understanding LSTM Networks》
date: 2018-07-13 09:28:09
tags: [RNN]
categories: [RNN]
typora-root-url: ../../static
---

Cloah的这篇是学习RNN绕不过的一篇文章，PyTorch官方文档里也推荐要看这一篇，翻译一下。

## RNN 循环神经网络

人类的思维不是每一秒独立进行的。就像读这篇文章的你，基于对前文的理解，才可以读懂后面的每个字。你不会抛掉前面所理解的东西，而从零开始重新思考。你的思维是有连续性的。

传统的神经网络不会这么做，并且这看起来是它最大的短板。举个例子，假设你想要对电影中每一秒正在发生的事件进行分类，很难讲清楚普通的神经网络能如何利用之前发生的事件信息来推测这一秒内所发生的行为。

RNN解决了这一问题，它是带有循环的网络，允许信息在网络中持续流传。

![](/img/RNN-rolled.png)

在上图中，神经网络A有一个输入xt和输出值ht。中间的循环箭头允许信息从一个时间步传递到下一个时间步。

这些循环箭头使得RNN网络看起来有点迷惑。然后，如果你思考一下，会发现它们与普通的神经网络并没有太大的差别。RNN可以被认为是一个相同网络的多份复制，每一个子网络将信息传递给下一个。如果我们将整个循环网络展开：

![](/img/RNN-unrolled.png)


这个链式的特征说明了RNN生来与序列信息相关，它们是用于这种数据类型的自然而然的结构。

并且它们已经被使用了！在过去的几年里，RNN在一系列问题中都取得了不可思议的成功：语音识别，语言模型，机器翻译，图像标题等，这个名单还在继续变长。RNN取得的成就和其非凡的效果可以在Andrej Karpathy的博客中去了解，[The Unreasonable Effectiveness of Recurrent Neural Networksj](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)。

RNN取得成功的重要因素之一就是“LSTM”的使用，这种特殊的RNN单元结构在许多任务上都比标准版本的RNN表现的更加优秀，几乎所有RNN所成就的令人激动的成绩都是基于LSTM实现的。这篇文章将会探索这种LSTM结构。

## 长期依赖问题

RNN最吸引人的一点是它有潜力在之前的信息和当前的任务之间建立联系，就像使用之前的视频帧来帮助理解当前的一帧。如果RNN可以做到这一点，那它将是非常有效的。不过它们可以做到吗？这要视情况而定。

有些时候，我们只需要使用最近不远处的一些信息来帮助执行当前的任务。比如，考虑一个基于前文预测下一个单词的语言模型。如果我们试着预测"the cloud are in the *sky*"的最后一个单词*sky*，我们不需要更远处的上文了，根据引号内的前文我们可以确定最后一个单词是sky。在这种情况下，预测这个单词所需要的前文只需要很少的近距离词汇就可以，RNN可以学习使用过去的信息。

![](/img/RNN-shorttermdepdencies.png)

不过有一些情况下，我们需要更多的，更远距离的上下文信息。考虑一下这种场景，如果我们要预测"I grew up in France... I speak fluent *French.*"中的最后一个单词*French*。近处的上下文信息"I speak fluent"只可以确定最后一个可能是某种语言的名称，如果我们想缩小搜索的范围，确定到底是哪一种语言，就需要来自很远处的上下文信息*France*。有时解决当前问题所需要的上下文信息的距离也可能是非常远的。

不幸的是，随着距离的增加，RNN关联上下文信息的能力变得越来越弱。

![](/img/RNN-longtermdependencies.png)

理论上来讲，RNN是有能力解决这种“长期依赖问题”的。人们可以精心的为RNN挑选一组参数来解决这种问题。然而，从实践上来讲，RNN并不能学习到这种能力。这个问题在[Hochreiter(1991)](http://people.idsia.ch/~juergen/SeppHochreiter1991ThesisAdvisorSchmidhuber.pdf)和[Bengio, et al. (1994)](http://www-dsi.ing.unifi.it/~paolo/ps/tnn-94-gradient.pdf)中被深度的讨论，并提出了一些为什么这个问题难以解决的基础性的原因。

而十分幸运的是，这些问题并不存在于LSTM网络！

## LSTM网络

长短期记忆网络，即LSTM网络，是一种特殊结构的RNN，可以学习到长期依赖的信息。它是由[Hochreiter & Schmidhuber (1997)](http://www.bioinf.jku.at/publications/older/2604.pdf)提出，并由许许多多的后人们改进的。LSTM在很多的问题上都表现优异，且目前的应用十分广泛。

LSMT的设计旨在避免长期依赖问题。记住长期的远处的信息是它默认的行为，而不是很勉强的去学习到的！

所有的RNN都有着链式的形状，重复着同一个神经网络模块。在标准的RNN网络中，这种重复的单元有着非常简单的结构，比如一个tanh层。

![](/img/LSTM3-SimpleRNN.png)

LSTM也有同样的链式结构，但是所重复的模块内部有着不同的结构。与单一的tanh或是其他神经网络层不同，LSTM内部有着4个相互作用的模块。

![](/img/LSTM3-chain.png)

不要担心模块内部所进行的细节，一会儿我们会一步一步的走一遍LSTM的流程。现在，我们先熟悉一下将会用到的符号。

![](/img/LSTM2-notation.png)

在上图中，每条实线都携带着一个向量，从一个节点的输出到其他节点的输入。粉红色的圈圈代表元素级别运算（element-wise），例如向量加法。黄色的方块是可学习的神经网络层。两根线的合并表示着合并，而一根线的分叉表示原来线上的内容被复制为两份，分别去往不同的地方。

## LSTM背后的核心原理

LSTM的关键是单元的状态（cell state），一条水平的直线贯穿流程图的顶部。

单元状态像是一种传送带，它直接穿过整个RNN链式结构，只经历一些细小的线性运算，信息非常容易的就可以不经改变的流过网络。

![](/img/LSTM3-C-line.png)

LSTM可以在单元状态中添加或删除信息，这一功能是通过一种叫“门”的结构实现并管理的。

门有选择性的让信息穿过。它是由一个sigmoid层和一个元素乘积运算组成的。

![](/img/LSTM3-gate.png)

Sigmoid层的输出是介于0和1之间的数字，描述了每个元素可以穿过这道门的比例。0表示不可以穿过，1表示让该元素所有信息都穿过！

一个LSTM单元有着三个门，来维护和控制单元状态。

## 步步详解LSTM

### 遗忘门

LSTM的第一步是决定单元状态中有多少信息应该被抛弃。这个决定是由称为”遗忘门“的sigmoid层实现的。它的输入是ht-1和xt，输出的是与Ct-1同形状的矩阵，数值在0到1之间。1表示完全保留这个分量，而0表示完全抛弃这个分量。

让我们回到语言模型的例子，在这个问题中，单元状态可以包含着当前主语的性别信息，以便于使用正确的代词形式。当我们看到一个新的主语的时候，我们就会想要忘记旧主语的性别信息。

![](/img/LSTM3-focus-f.png)

### 输入门

下一步是决定单元状态中要增加哪些新的信息。这由两部分构成，第一部分是称为“输入门”的sigmoid层，用来决定哪些值需要更新。接着是一个tanh层，用来生成一个备选向量，提供备选的单元状态各分量更新值。下一步，我们结合这两个向量来生成一个单元状态的更新向量。

在语言模型的例子中，我们想在单元状态中添加新主语的性别信息，来替换旧的我们正在遗忘的信息。

![](/img/LSTM3-focus-i.png)

现在是时候更新旧的单元状态了。之前的两步已经决定了如何去做，现在只要真的实现它们就可以了。

我们将旧的单元状态乘以遗忘门的输出ft，忘记应该忘记的那些信息。然后将输入门的输出与备选向量相乘得到新的备选向量，它由每个单元状态的每个分量的多少比例应该被更新来决定。将这个备选向量与遗忘之后的单元状态相加，得到新的单元状态Ct。

在语言模型的例子中，正如之前讨论的，这就是我们丢弃关于旧的主语的性别信息和添加新的信息的方法。

### 输出门

最后，我们需要决定输出哪些信息。输出向量基于我们的单元状态，但是将会被过滤之后再输出。首先，我们运行一个sigmoid层来决定单元状态的哪些部分将会被输出。然后，我们将单元状态通过一个tanh层，将每个分量的值置于-1和1之间，再将其乘以sigmoid层的输出，这样我们就可以只输出我们决定输出的部分。

在语言模型的例子中，由于我们看到一个主语，网络可能想要输出关于动词的信息，因为这可能是下一个词将会出现的。举个例子，它将会输出主语是单数还是复数，以便于我们了解下一个动词应该是什么形式。

![](/img/LSTM3-focus-o.png)

## 长短期记忆网络的变体

刚才所描述的都是非常普通的LSTM网络。但是不是所有的LSTM都和上述一样的。事实上，在很多使用LSTM的论文中都使用了略有不同的版本，差距很微小，但是这其中有一些变体是值得一提的。

### 窥孔 Peephole

一种流行的LSTM变体，由[Gers & Schmidhuber (2000)](ftp://ftp.idsia.ch/pub/juergen/TimeCount-IJCNN2000.pdf)提出，增加了“窥孔连接”。这意味着我们让门层可以看到单元的状态，而普通的门层输入仅为当前的输入xt和上一时刻的输出ht-1。

![](/img/LSTM3-var-peepholes.png)

上图中对所有的门都增加了窥孔连接，不过许多论文中只给部分门以窥孔，而非全部的门。

### 组合的遗忘门与输入门

还有一种变体是结合的遗忘门与输入门。与以往分别决定遗忘哪些分量和向哪些分量添加新的信息不同，这种模型下我们同时做这两个决策。我们只有在添加新的信息的时候才去遗忘一些东西，也只在遗忘一些旧的东西的时候去为状态添加新值。

![](/img/LSTM3-var-tied.png)

### 门控循环单元 GRU

一个令人惊喜的LSTM变体是门控循环单元，也叫GRU，是[Cho, et al. (2014)](http://arxiv.org/pdf/1406.1078v3.pdf)提出的。它将遗忘门与输入门结合为一个“更新门”，同时融合了单元状态和隐藏状态，并作了一些其他的改变。最后形成的GRU模型比标准的LSTM模型要简单一些，并变得越来越流行。

![](/img/LSTM3-var-GRU.png)

（我说两句：首先GRU取消了单元状态；这里右边的sigmoid就是更新门，往上走是遗忘，往右走是更新；不同之处在于备选向量的生成处rt的存在）

只有一小部分LSTM变体是值得注意的。有很多其他种变体，例如[Yao, et al. (2015)](http://arxiv.org/pdf/1508.03790v2.pdf)提出的深度门控RNN等。也有许多完全不同的捕捉长距离依赖关系的方法，例如[Koutnik, et al. (2014)](http://arxiv.org/pdf/1402.3511v1.pdf)提出的时钟驱动RNN等。

哪种变体是最好用的呢？这些变化真的有作用吗？[Greff, et al. (2015)](http://arxiv.org/pdf/1503.04069.pdf)做了一个很好的流行变体之间的比较，结果是它们几乎是类似的。[Jozefowicz, et al. (2015)](http://jmlr.org/proceedings/papers/v37/jozefowicz15.pdf)一文测试了上万种RNN结构，发现某些变体会在特定任务上比LSTM网络表现更好一些。

## 结论

早些时候我提到过一些RNN达到的令人印象深刻的成果，几乎所有的这些都是通过LSTM实现的。对大部分任务而言，LSTM都是更好的选择！

只是写下冷冰冰的公式会显得LSTM非常可怕，然而，看完这篇文章，一步步的了解其中的结构之后，LSTM就显得平易近人许多了。

LSTM是RNN发展历程上的一大步。人们很自然地会问：那接下来的一大步会在哪呢？研究者中一个普遍的观点是：Attention就是接下来的一大步！这个想法是让RNN在每一步选择信息时从更大的一个范围内去进行。例如，如果想用RNN为图片生成说明，它每输出一个词的时候会选择图片的某一个部分来关注。论文[Xu, et al. (2015)](http://arxiv.org/pdf/1502.03044v2.pdf)做的正是这个工作，如果你想要探索attention，这或许是个有趣的开始！现在有许多有趣的attention的应用，而且似乎还有更多的事情即将来临。

除了Attention，RNN中也有许多其他令人惊喜的研究路线。例如[Kalchbrenner, et al. (2015)](http://arxiv.org/pdf/1507.01526v1.pdf)所提出的网格LSTM看起来就十分可靠。在生成对抗网络中使用RNN，例如[Gregor, et al. (2015)](http://arxiv.org/pdf/1502.04623.pdf)，[Chung, et al. (2015)](http://arxiv.org/pdf/1506.02216v3.pdf)和[Bayer & Osendorfer (2015)](http://arxiv.org/pdf/1411.7610v3.pdf)，看起来也十分有趣。过去的几年是RNN网络发展巨大的几年，接下来的日子一定会是更加令人兴奋的。