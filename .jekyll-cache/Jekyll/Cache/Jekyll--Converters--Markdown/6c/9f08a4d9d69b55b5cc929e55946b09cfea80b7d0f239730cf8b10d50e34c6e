I"<p>From Hung-yi Lee 2018.</p>

<h1 id="1-introduction">1 Introduction</h1>

<h2 id="basic-idea">Basic Idea</h2>

<p>GAN的目标也是训练出一个Generator，输入一个随机的向量（特征向量），生成一个样本。</p>

<p>但是也有一个Discriminator，输入生成的样本，输出一个scalar，代表quality，越大越真实。</p>

<p>二者是一起进步的：生成器生成了假样本，判别器根据生成的样本和dataset之间的样本比较相似度。</p>

<h2 id="algorithm">Algorithm</h2>

<p><img src="/img/ganalgo.png" alt="" /></p>

<p>不是一步接一步的，可以固定G训练N次D，然后固定D训练N次G。</p>

<h2 id="为什么不让generator直接训练">为什么不让Generator直接训练</h2>

<p>如果要这么做，首先要给dataset中每个样本assign一个code，然后G根据code生成对应的样本。</p>

<p>这种做法是类似普通AE的，问题在于对于生成新样本而言，采样的空间不是连续的，不可以随意从一个先验里面采样。这也是VAE出现的原因，在code上加上一些噪音的影响，使得code布满一个prior空间。</p>

<p>如果不用Discriminator来判断相似度，而用简单的欧拉距离来判断，是不准确的（例如平移）。</p>

<p>且Generator自己很难在有限的层数判断不同维度之间的相关度。</p>

<h2 id="为什么不用discriminator生成">为什么不用Discriminator生成</h2>

<p>这种打分的机制是可以用来生成新图片的，随机生成一个样本，用D打分，找高分就可以了。</p>

<p>但是怎么训练它呢，我们手里只有正面样本，这样的问题是D在训练过程中一直打高分，所以后面无论输入什么都倾向于打高分。</p>

<p>所以问题在于怎么找假样本，如果用随机噪声的话，这个样本也太差了，会使得D的要求特别低。只有用质量非常高的negative example才能让D变得更加强大。</p>

<p>那可以从随机噪声开始，用穷举的方法迭代D本身，用上一轮D认为是好的样本，做下一轮的假样本。</p>

<p><img src="/img/gandis.png" alt="" /></p>

<p>在Graphical Model中，采用的训练思想跟上面是一样的，整个graph就是一个discriminator。</p>

<h2 id="比较g和d">比较G和D</h2>

<p><img src="/img/gvsd.png" alt="" /></p>

<p>Generator可以生成，但考虑不到大局。D可以从大局考虑图片质量，但是生成很累。</p>

<p>将D单独训练中的找negative example这个过程，变成了用G来生成当前D认为的positive example。</p>

<p><img src="/img/benifit.png" alt="" /></p>

<p>G使得采假样本这个过程变得简单，D使得在计算loss时可以更加从大局的角度入手。</p>

<h1 id="conditional-generation">Conditional Generation</h1>

<p><img src="/img/cgan.png" alt="" /></p>

<p>注意D的输入也要有类别c的信息，不然G会偷懒只生成一个高质量的图片，而不管是什么分类，因为这样就足够从D那里取得高分了。</p>

<p>负样本的输入也与普通GAN不同，不能只有假图片，同时还要有不同的标签来表征类别之间的不同。因为D是要全局的学习condition和example之间的关系的，判断两件事：图片真不真，图片符合不符合分类。</p>

<p>算法流程：</p>

<p><img src="/img/cganalgo.png" alt="" /></p>

<p>注意D中损失函数的变化，第二项是图片不清楚的，第三项是图文不符的。</p>

<p>CGAN的网络结构：</p>

<p><img src="/img/cgandis.png" alt="" /></p>

<p>第一种方法是结合两种evaluation的综合分数，第二个是两种分数分开叙述。看起来第二种更具体一些。</p>

<p>第一种会在低分时confuse到底是图文不符，还是图片不清晰。</p>

<p>但是还是第一种用的多，算是一种端到端的简化。</p>

<h2 id="stack-gan">Stack GAN</h2>

<p>先生成小的图，再产生大的图，还用了点variational的东西。</p>

<p><img src="/img/stackgan.png" alt="" /></p>

<h2 id="patch-gan">Patch GAN</h2>

<p>一种image-to-image的变化，以图为条件，去改变风格质，这里的Discriminator每次只检查一个区域。</p>

<p><img src="/img/imagetoimage.png" alt="" /></p>

<p>这里最后一个加close的意思是，我们之前使用discriminator的时候会忽略了生成的假样本和目标图片之间的直接关系，而是以(c, x)(c, x假)(c假, x)这样的形式去让D判断，丧失了x和(x假)之间的关系。</p>

<h2 id="总结">总结</h2>

<p>所以看起来，所有端到端的supervise训练过程都可以用CGAN来实现，让生成的东西更加精确清晰。</p>

<h2 id="gan-for-sequence">GAN for Sequence</h2>

<p>为什么在Seq2seq里面要用GAN，因为<strong>评价标准</strong>，例如chat-bot，how are you，标准答案I’m fine。</p>

<p>如果按照Maximize likelihood的角度看，I’m John是个比Not bad更好的答案，与gold相似的地方更多。</p>

<p>这就是问题。</p>

<p><img src="/../../Library/Application Support/typora-user-images/image-20190706104203448.png" alt="image-20190706104203448" /></p>

<p><img src="/../../Library/Application Support/typora-user-images/image-20190706105401126.png" alt="image-20190706105401126" /></p>

<p>Update参数这一步很重要。</p>

<p><img src="/../../Library/Application Support/typora-user-images/image-20190706105650943.png" alt="image-20190706105650943" /></p>

<p>与普通的最大化ground truth样本出现的可能的区别是，每个生成样本都给一个reward，weighted sum。</p>

<h3 id="引入gan">引入GAN</h3>

<p>在RL里面，feedback 也就是这个reward是人给的（游戏场景下比较明显）。</p>

<p>但是在GAN里面，feedback是discriminator给的。</p>

<p>问题是，取词的过程有sample的过程，这个阶段是不可微分的。</p>

<p><img src="/../../Library/Application Support/typora-user-images/image-20190706115350619.png" alt="image-20190706115350619" /></p>

<p>三种办法解决，用一个特殊的softmax，或者使用sample前的连续vector，或者RL。</p>

<p>第二种方法问题是gold样本标签都是one-hot，我们的distribution都不是，这样D很容易作弊，G也会学坏。</p>

<p>所以RL。</p>

<h3 id="rl">RL</h3>

<p>把R(c, x)换成用discriminator来算，D(c, x)。</p>

<p><img src="/../../Library/Application Support/typora-user-images/image-20190706115728841.png" alt="image-20190706115728841" /></p>

<p>也要G，D分步交替反复进行。</p>

<p>给Reward的时候总的给是不合适的，因为可能第一个词是对的。</p>

<p>how are you? i am John 第一个词是对的。</p>

<p>解决方法是给每一步都打分，可用蒙特卡洛搜索，但是计算量很大。</p>

<p>所以应该是把Policy Gradient用在GAN方式的训练之中。</p>
:ET