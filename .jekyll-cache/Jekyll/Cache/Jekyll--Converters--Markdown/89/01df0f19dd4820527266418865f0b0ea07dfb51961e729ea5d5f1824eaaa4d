I" ¡<p>è¯¦è§£<a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html#greedy-decoding">Harvardå®ç°çš„è¿™ä¸ªtransformerä»£ç </a>ã€‚</p>

<h2 id="embedding">Embedding</h2>

<ul>
  <li>Positional Encoding: åœ¨<a href="https://arxiv.org/pdf/1705.03122.pdf">CNN seq2seq</a>è¿™ç¯‡æ–‡ç« é‡Œé¢æåˆ°äº†è®¸å¤špositional encodingçš„æ–¹æ³•ï¼Œæœ‰learnedå’Œfixedã€‚</li>
  <li>Embedding and Softmax: Share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to <a href="https://arxiv.org/abs/1608.05859">cite</a>.</li>
</ul>

<h2 id="optimizer">Optimizer</h2>

<p>åˆ©ç”¨äº†warm upçš„å…ˆå¢åå‡çš„å­¦ä¹ ç‡ã€‚</p>

<h2 id="regularization">Regularization</h2>

<p><a href="https://arxiv.org/abs/1512.00567">Label smoothing</a>ï¼Œå› ä¸ºone-hotå‘é‡ä¼šä½¿æ¨¡å‹over-confidentï¼Œåœ¨è®­ç»ƒé›†å°çš„æ—¶å€™æ›´ä¼šå‡ºç°è¿™ç§æƒ…å†µï¼Œå‡å°one-hotä¸­1çš„å¤§å°ï¼Œå¹¶å‡åŒ€çš„åˆ†é…åˆ°å…¶ä»–ä½ç½®ï¼Œä½†æ˜¯å’Œä¾ç„¶ä¸º1ã€‚</p>

<h2 id="decoderencoder">Decoder/Encoder</h2>

<p><code class="highlighter-rouge">Decoder(layer, n)</code>æ¨¡å—æ˜¯å°†<code class="highlighter-rouge">DecoderLayer</code>é‡å¤<code class="highlighter-rouge">n</code>æ¬¡ï¼Œç„¶å<strong>è·Ÿä¸€ä¸ªLayerNromå±‚</strong>ã€‚</p>

<p><code class="highlighter-rouge">DecoderLayer</code>ç±»ä¸­è¿˜æœ‰ä¸€ä¸ªå‚æ•°<code class="highlighter-rouge">self.sublayer = clones(SublayerConnection(size, dropout), 3)</code>ã€‚</p>

<p><code class="highlighter-rouge">SublayerConnection</code>è¿™ä¸ªç±»æ„é€ æ—¶æ²¡æŒ‡å®šåŒ…å«ä»€ä¹ˆæ¨¡å—ï¼Œä½†åœ¨ä¼ å‘ä¼ æ’­æ—¶<code class="highlighter-rouge">forward(x, sublayer)</code>æŒ‡å®šè¾“å…¥å’Œå±‚ï¼Œè¿›è¡Œäº†å¦‚ä¸‹å›¾çš„æ“ä½œè¿‡ç¨‹ã€‚</p>

<p><img src="/img/flow1.png" alt="" /></p>

<p>ç»§ç»­çœ‹<code class="highlighter-rouge">DecoderLayer</code>å…¶ä¼ æ’­è¿‡ç¨‹ä¸ºå°†ç¬¬ä¸€å±‚sublayeræŒ‡å®šä¸º<strong><em>å¯¹xè¿›è¡Œçš„self-attention</em></strong>ï¼Œç¬¬äºŒå±‚<strong><em>è®¡ç®—encoderè¾“å‡ºåºåˆ—(memory)å’Œx(output of previous sublayer)ä¹‹é—´çš„å‰åattention</em></strong>ï¼Œç”¨çš„å‡½æ•°æ˜¯ä¸€æ ·çš„ï¼Œç¬¬ä¸‰å±‚ä½¿ç”¨<strong><em>ä¸€ä¸ªå…¨è¿æ¥å±‚</em></strong>ã€‚</p>

<p>æ‰€ä»¥decoderçš„æµç¨‹å°±æ˜¯ä¸æ–­é‡å¤ä¸Šæ®µçš„è¿‡ç¨‹ï¼Œå†çœ‹<strong>encoderå’Œdecoderçš„æ¥å£éƒ¨åˆ†</strong>ã€‚</p>

<p>Encoderçš„è¾“å‡ºä¸ºmemoryï¼Œç”¨æ¥è®¡ç®—å‰åattentionï¼Œdecoderä¸­çš„<strong>xæ˜¯targetçš„embedding</strong>ï¼Œè¿™ä¸€ç‚¹ä¸RNNç›¸åŒã€‚</p>

<p>è€Œtarget_maskå’Œsrc_maskçš„åº”ç”¨ä¸å®ç°åˆ†åˆ«åœ¨self_attnå’Œsrc_attnä¸­ï¼Œéƒ½æ˜¯<code class="highlighter-rouge">MultiHeadAttention</code>ç±»çš„å®ä¾‹ã€‚</p>

<p><strong><em>Encoderç»“æ„å¤§è‡´ç›¸åŒï¼Œåªæ˜¯æ¯ä¸ªencoder layerä¸­æ²¡æœ‰src_attentionå±‚ã€‚</em></strong></p>

<h2 id="attention">Attention</h2>

<p>æœ¬æ–‡æ‰€ç”¨çš„Scaled Dot Product Attentionå®ç°ä¸ºä¸€ä¸ª<strong><em>å‡½æ•°(è€Œéç±»)</em></strong>ï¼š</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="n">d_k</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">d_k</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">masked_fill_</span><span class="p">(</span><span class="n">mask</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> <span class="n">value</span><span class="o">=-</span><span class="mf">1e9</span><span class="p">)</span>
    <span class="n">p_attn</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">dropout</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">p_attn</span> <span class="o">=</span> <span class="n">dropout</span><span class="p">(</span><span class="n">p_attn</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">p_attn</span><span class="p">,</span> <span class="n">value</span><span class="p">),</span> <span class="n">p_attn</span>
</code></pre></div></div>

<p>Dot-productå°±æ˜¯åˆ©ç”¨queryå’Œkeyä¹‹é—´çš„<strong><em>ç‚¹ä¹˜</em></strong>è¿ç®—æ¥è®¡ç®—scoresï¼ŒScaledçš„æ„æ€æ˜¯é™¤ä»¥æ ¹å·ç»´åº¦(scaling factor ç¼©æ”¾å› å­)ã€‚ç”¨queryå’Œkeyè®¡ç®—å‡ºå¯¹äºæ¯ä¸ªkeyçš„åˆ†æ•°ï¼Œç„¶ååˆ©ç”¨åˆ†æ•°æ±‚valueçš„åŠ æƒå’Œã€‚</p>

<p>BTWï¼Œå¸¸ç”¨çš„å¦ä¸€ç§attentionè®¡ç®—æ–¹æ³•æ˜¯addictive attentionï¼Œåˆ©ç”¨ä¸€ä¸ªå‰å‘ç½‘ç»œæ¥æ±‚åˆ†æ•°ã€‚</p>

<p>è¿™é‡Œçš„<code class="highlighter-rouge">masked_fill_</code>å‡½æ•°çš„ç”¨æ³•æ˜¯ï¼Œåˆ©ç”¨<code class="highlighter-rouge">mask == 0</code>äº§ç”Ÿä¸€ä¸ªäºŒå€¼çš„ByteTensorï¼Œå…¶å€¼ä¸º1çš„ä½ç½®éœ€è¦ç”¨value(float)æ¥ä»£æ›¿ã€‚æ³¨æ„valueå¯ä»¥ç›´æ¥ç”¨æ•°å­—æˆ–è€…ç”¨0ç»´tensor(torch.tensor(5))ã€‚</p>

<p>è¿™é‡Œçš„maskåˆ†src_mask(æŒ‡å®špad)ï¼Œå’Œtgt_mask(é˜¶æ¢¯)ã€‚</p>

<p><code class="highlighter-rouge">src_mask</code>åœ¨encoderå’Œdecoderçš„src_attnå­å±‚ä¸­ç”¨åˆ°ï¼Œä½œç”¨ä¸ºå°†queryå’Œkeyä½œmultiplyå¾—åˆ°çš„ä¹˜ç§¯çŸ©é˜µï¼Œå…¶ä¸­æºå¥ä¸ºpadçš„ä½ç½®ç½®æ¢ä¸ºæ— ç©·å°çš„æ•°å­—ï¼Œä¹‹åå†ç»è¿‡softmaxæ±‚åˆ†æ•°åè¯¥ä½ç½®å˜ä¸º0ï¼Œ<strong><em>æœ€ç»ˆçš„å½±å“æ˜¯åˆ©ç”¨å¾—åˆ†æ±‚weighted averageçš„æ—¶å€™ä¼šå¿½ç•¥åˆ°valueä¸­å¯¹åº”æºå¥padä½ç½®çš„å‘é‡ã€‚</em></strong>ä½†æ˜¯è¿™ä¸ç­‰äºpadä½ç½®åé¢çš„å‘é‡éƒ½æ˜¯0äº†ï¼Œå› ä¸ºè¾“å…¥è¾“å‡ºæ˜¯ä¸ä¸€å®šç­‰é•¿çš„ï¼Œè¿™äº›ä½ç½®ä¹Ÿä¼šæœ‰è¾“å‡ºã€‚</p>

<p><code class="highlighter-rouge">tgt_mask</code>åœ¨decoderçš„self-attnå±‚ä¸­ç”¨åˆ°ï¼Œå…¶ä½œç”¨æ˜¯è®©åˆ†æ•°çŸ©é˜µå˜æˆä¸€ä¸ªä¸‰è§’é˜µï¼Œè¿™æ ·åœ¨æ±‚weighted averageçš„æ—¶å€™ç¬¬tä¸ªä½ç½®åªèƒ½æ ¹æ®(1, t)ä½ç½®ä¸Šçš„å‘é‡è¿›è¡Œè¿ç®—ï¼Œä¹‹åçš„éƒ½ä¹˜ä»¥0ä¸å‚ä¸è®¡ç®—ã€‚<strong><em>ç›¸å½“äºç¬¬tä¸ªä½ç½®æœ€ç»ˆçš„é¢„æµ‹ä¸ä¾èµ–å…¶ä¹‹åçš„è¯è¯­ï¼Œè¿™æ ·è®©decoderæ›´ç¬¦åˆçœŸå®åœºæ™¯ä¸‹çš„é¢„æµ‹ã€‚</em></strong></p>

<p><strong><em>âš ï¸è¿™é‡Œencoderé‡Œæ˜¯attention(x, x, x)ï¼Œdecoderæ˜¯(x, m, m)ï¼Œvalueæ˜¯mï¼Œè¿‡å¤šçš„å…³æ³¨memoryäº†ã€‚</em></strong></p>

<p>æ­¤å¤–è¿™é‡Œçš„dropoutå°†åˆ†æ•°å‘é‡ä¸­çš„ä¸€éƒ¨åˆ†éšæœºæ¢æˆ0ï¼Œå…¶ä»–çš„ä¹˜ä»¥ç¼©æ”¾å› å­ï¼›å¾€ä¸‹æƒ³å°±æ˜¯éšæœºæŠ›å¼ƒäº†ä¸€äº›encoderè¾“å‡ºä¸­çš„ä½ç½®ã€‚</p>

<p>æ¥ä¸‹æ¥æ˜¯MultiHeadAttentionçš„å®ç°ï¼Œåªçœ‹å‰å‘ä¼ æ’­éƒ¨åˆ†ï¼š</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
    <span class="c1"># Same mask applied to all h heads.
</span>    <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">nbatches</span> <span class="o">=</span> <span class="n">query</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="c1"># 1) Do all the linear projections in batch from d_model =&gt; h x d_k
</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="o">=</span> <span class="p">[</span><span class="n">l</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">nbatches</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> 
                     <span class="k">for</span> <span class="n">l</span><span class="p">,</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linears</span><span class="p">,</span> <span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">))]</span>
<span class="c1"># 2) Apply attention on all the projected vectors in batch.
</span><span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">attn</span> <span class="o">=</span> <span class="n">attention</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">mask</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">)</span>
<span class="c1"># 3) "Concat" using a view and apply a final linear.
</span><span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">nbatches</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">h</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">d_k</span><span class="p">)</span>
<span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">linears</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">](</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<p><strong>1ï¼‰zipçš„ç»“æœæ˜¯[(linear1, query), (linear2, key), (linear3, value)]ï¼Œä¹‹åæ¯ä¸€ç»„å‰å‘ä¼ æ’­ä¹‹åtransposeå˜å½¢ä¸º<code class="highlighter-rouge">[batch, h, time-step, d_k]</code>ï¼Œå…¶ä¸­hå°±æ˜¯headï¼Œç„¶åæ³¨æ„æœ€å¤–å±‚çš„ä¸­æ‹¬å·å°†å…¶ä¿å­˜ä¸º<code class="highlighter-rouge">[new_q, new_k, new_v]</code>ã€‚</strong></p>

<p>2ï¼‰ç”¨åˆ°äº†ä¸Šé¢çš„attentionå‡½æ•°ï¼Œåªå¯¹æœ€åä¸¤ä¸ªç»´åº¦è¿›è¡Œäº†attentionåˆ†æ•°è®¡ç®—ï¼Œå‰é¢ä¸¤ä¸ªç»´åº¦ä¸å˜äº†ã€‚</p>

<p>3ï¼‰å°†ä¸Šä¸€æ­¥å¾—åˆ°çš„ç»“æœï¼ˆåŠ æƒå’Œï¼‰ï¼Œå˜æ¢å›<code class="highlighter-rouge">[batch, time-step, h, d_k]</code>çš„å½¢å¼ï¼Œç„¶ååˆå¹¶åä¸¤ä¸ªç»´åº¦ã€‚</p>

<p>4ï¼‰æœ€åå†ç»è¿‡ä¸€ä¸ªå…¨è”æ¥å±‚ï¼ˆæ‰€ä»¥æ€»å…±ç”¨åˆ°4ä¸ªFFNï¼‰ã€‚</p>

<p><strong>è¿™é‡Œæ³¨æ„å‰ä¸¤è¡Œï¼Œå¦‚æœæœ‰maskçš„è¯ï¼Œå°†å…¶æ‰©å±•ç»´åº¦åˆ°4ç»´ã€‚</strong></p>

<h2 id="è®­ç»ƒæ¡†æ¶">è®­ç»ƒæ¡†æ¶</h2>

<p>è®­ç»ƒéƒ¨åˆ†è¢«<code class="highlighter-rouge">run_epoch(data_iter, model, loss)</code>å‡½æ•°å°è£…ï¼Œç¬¬ä¸€ä¸ªå‚æ•°äº§ç”Ÿbatchæ•°æ®ï¼Œç¬¬äºŒä¸ªæ˜¯æ¨¡å‹ï¼ˆå¤šGPUå¹¶è¡Œï¼‰ï¼Œç¬¬ä¸‰ä¸ªæ˜¯è®¡ç®—å¤šGPUä¸‹çš„Losså€¼ã€‚</p>

<h3 id="the-first-parameter-data_iter">The first parameter: data_iter</h3>

<p><code class="highlighter-rouge">run_epoch()</code>å‡½æ•°æ˜¯ä¸€ä¸ªforå¾ªç¯ï¼Œä»<code class="highlighter-rouge">data_iter</code>é‡Œé¢å–batchï¼Œå‰å‘ä¼ æ’­ï¼Œè®¡ç®—losså¹¶ç»Ÿè®¡ã€‚</p>

<p>åœ¨<strong>å¤åˆ¶ä»»åŠ¡</strong>å®ä¾‹ä¸­ï¼Œdata_iterç”¨çš„æ˜¯<code class="highlighter-rouge">data_gen(vocab_size, batch_size, nbatches)</code>å‡½æ•°ï¼Œè¿™ä¸ªå‡½æ•°é‡Œç”¨äº†ä¸ª<code class="highlighter-rouge">numpy.random.randint(low, high, size)</code>å‡½æ•°ç”Ÿæˆå‡çš„æ•°æ®ï¼Œè¯è¡¨å¤§å°å³highï¼ŒsizeäºŒç»´<code class="highlighter-rouge">[batch, 10]</code>ï¼Œå› ä¸ºæ˜¯å¤åˆ¶ä»»åŠ¡æ‰€ä»¥æºå’Œç›®æ ‡åºåˆ—ç›¸åŒï¼Œæœ€å<code class="highlighter-rouge">yeild Batch(src, tgt, 0)</code>ï¼Œæ‰€ä»¥ç»§ç»­çœ‹<code class="highlighter-rouge">Batch()</code>ã€‚</p>

<p><code class="highlighter-rouge">Batch(src, trg, pad)</code>ä¸­å¯¹targetå¤„ç†ä¸ºtrgå’Œtrg_yï¼Œå‰è€…å»æ‰æœ€åä¸€ä¸ªï¼Œåè€…ä»ç¬¬äºŒä¸ªå¼€å§‹ã€‚è¿™ä¸ªç±»é‡Œæœ‰ä¸€ä¸ªæ–¹æ³•<code class="highlighter-rouge">make_std_mask</code>ï¼š</p>

<p>é¦–å…ˆçœ‹å…¶ä¸­çš„<code class="highlighter-rouge">subsequent_mask</code>ï¼š</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">subsequent_mask</span><span class="p">(</span><span class="n">size</span><span class="p">):</span>
    <span class="s">"""
    Mask out subsequent positions.
    """</span>
    <span class="n">attn_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="p">,</span> <span class="n">size</span><span class="p">)</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">numpy</span><span class="o">.</span><span class="n">triu</span><span class="p">(</span><span class="n">numpy</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">attn_shape</span><span class="p">),</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s">'uint8'</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span>
</code></pre></div></div>

<p>ä¸Šé¢ä»£ç ğŸ‘†ä¸­triuä¸º<strong>upper triangle</strong>ä¸Šä¸‰è§’é˜µï¼Œç¬¬kå¯¹è§’çº¿å¾€ä¸‹éƒ½æ˜¯0ï¼Œk=0æ—¶æ˜¯æ ‡å‡†çš„ä¸Šä¸‰è§’ï¼Œä¸»å¯¹è§’çº¿ä»¥ä¸‹çš„å…¨æ˜¯0ï¼Œ<strong><em>è¿™é‡Œk=1çš„è¯ä¸»å¯¹è§’çº¿ä¸Šä¹Ÿéƒ½æ˜¯0</em></strong>ã€‚è¿”å›å€¼æ˜¯maskçŸ©é˜µä¸­ä¸º0çš„ä½ç½®è¿”å›1ï¼Œå…¶ä»–ä½ç½®0ï¼Œè¿™ä¸¤è¡Œç»“åˆèµ·æ¥å°±<strong><em>è¿”å›äº†ä¸€ä¸ªï¼ˆ1, size, sizeï¼‰ä¸‹ä¸‰è§’å…¨1çš„çŸ©é˜µï¼ŒåŒ…æ‹¬å¯¹è§’çº¿</em></strong>ã€‚</p>

<p>å†çœ‹æ€»çš„ï¼š</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">make_std_mask</span><span class="p">(</span><span class="n">tgt</span><span class="p">,</span> <span class="n">pad</span><span class="p">):</span>
    <span class="s">"""
    Create a mask to hide padding and future words.
    """</span>
    <span class="n">tgt_mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">tgt</span> <span class="o">!=</span> <span class="n">pad</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">tgt_mask</span> <span class="o">=</span> <span class="n">tgt_mask</span> <span class="o">&amp;</span> <span class="n">Variable</span><span class="p">(</span><span class="n">subsequent_mask</span><span class="p">(</span><span class="n">tgt</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">tgt_mask</span><span class="o">.</span><span class="n">data</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">tgt_mask</span>
</code></pre></div></div>

<p>ç¬¬ä¸€æ­¥targetä¸­padçš„ä½ç½®ä¸º0ï¼Œå…¶ä»–åœ°æ–¹éƒ½æ˜¯1ï¼Œå¾—åˆ°tgt_maskå½¢çŠ¶[30, 1, 9]ï¼Œvariableè¿”å›çš„å³ä¸Šé¢è¯´çš„[1, 9, 9]ï¼Œä½œ&amp;è¿ç®—æ—¶broadcastï¼Œå˜æˆäº†[30, 9, 9]ï¼Œ<strong><em>æœ¬è´¨å°±æ˜¯æœ‰30ä¸ª1x9çš„å¥å­ï¼Œæ¯ä¸ªå¥å­è‡ªèº«å¤åˆ¶ä¸º9x9ä¹‹åå†ä¸ä¸‹ä¸‰è§’çŸ©é˜µä½œä¸è¿ç®—ï¼Œç¬¬ä¸€è¡Œåªä¿ç•™ç¬¬ä¸€ä¸ªè¯ï¼ŒåŒç†ç›´åˆ°å¥å­ä¸­padä¹‹å‰çš„æœ€å¤§é•¿åº¦ã€‚</em></strong></p>

<p>ä¸Šé¢è¯´çš„æ˜¯subsequent_maskå‡½æ•°çš„è¾“å‡ºï¼Œå³Batchç±»ä¸­çš„trg_maskå˜é‡ã€‚å›åˆ°æœ€åˆè¯´çš„ï¼Œ<code class="highlighter-rouge">run_epoch()</code>å‡½æ•°å¾ªç¯è°ƒç”¨<code class="highlighter-rouge">data_iter</code>ï¼ˆå¤åˆ¶ä»»åŠ¡ä¸­å³data_genï¼‰ï¼Œyeildçš„ç”¨æ³•å°±æ˜¯æ¯æ¬¡ç”Ÿæˆä¸€æ¬¡æ–°çš„ï¼Œ<strong><em>è¿™æ ·ä¸å ç”¨å†…å­˜</em></strong>ã€‚</p>

<p>æ€»ç»“å‰å‘ä¼ æ’­çš„å››ä¸ªå‚æ•°ï¼š</p>

<ol>
  <li><code class="highlighter-rouge">batch.src</code>æ˜¯æ²¡æœ‰ç»è¿‡embeddingçš„è¾“å…¥ï¼Œå½¢çŠ¶<strong>(batch, time_steps)</strong>ã€‚</li>
  <li><code class="highlighter-rouge">batch.trg</code>æ˜¯è¾“å‡ºåºåˆ—ï¼Œä»ç¬¬ä¸€ä¸ªåˆ°å€’æ•°ç¬¬äºŒä¸ªï¼Œå½¢çŠ¶(<strong>batch, time_steps-1)</strong>ã€‚</li>
  <li><code class="highlighter-rouge">batch.trg_y</code>ç”¨äºè®¡ç®—lossï¼Œæ˜¯ç¬¬äºŒä¸ªåˆ°æœ€åä¸€ä¸ªï¼Œå½¢çŠ¶(<strong>batch, time_steps-1)</strong>ã€‚</li>
  <li><code class="highlighter-rouge">batch.src_mask</code>æ˜¯æºåºåˆ—ä¸­ä¸æ˜¯padçš„ä½ç½®æ˜¯1ï¼Œæ˜¯padçš„ä½ç½®æ˜¯ï¼Œå½¢çŠ¶<strong>(batch, 1, time_steps-1)</strong>ã€‚</li>
  <li><code class="highlighter-rouge">batch.trg_mask</code>æ˜¯ä¸Šé¢æ‰€è¯´çš„ï¼Œé˜¶æ¢¯å½¢çŠ¶çš„äºŒå€¼çŸ©é˜µï¼Œå½¢çŠ¶<strong>(batch, time_steps-1, time_steps-1)</strong>ã€‚</li>
</ol>

<h3 id="the-third-parameter-loss_compute">The third parameter: loss_compute</h3>

<p>å¤åˆ¶ä»»åŠ¡ä¸­ç”¨çš„<code class="highlighter-rouge">SimpleLossCompute(model.generator, criterion, model_opt)</code>ï¼Œè´Ÿè´£å°†æ¨¡å‹è¾“å‡ºæ˜ å°„åˆ°ç›®æ ‡ç©ºé—´ï¼Œå¹¶è®¡ç®—losså€¼ï¼Œå¹¶åšå‡ºä¸€ä¸ªä¼˜åŒ–æ­¥ã€‚</p>

<p>å…¶ä¸­ç¬¬ä¸€ä¸ªå‚æ•°generatorä½œæ¨¡å‹ç»´åº¦åˆ°è¯è¡¨çš„å˜æ¢ç„¶åsoftmaxï¼Œç¬¬äºŒä¸ªå‚æ•°<code class="highlighter-rouge">LabelSmoothing(size=V, padding_idx=0, smoothing=0.0)</code>ç”¨åˆ°äº†labelsmoothingï¼Œå…¶æœ¬è´¨å°±æ˜¯ä¸€ä¸ªloss_functionï¼ˆè¿™é‡Œç”¨çš„KLLossï¼‰çš„æ‰©å±•ã€‚ç¬¬ä¸‰ä¸ªå‚æ•°æ˜¯ä¼˜åŒ–å™¨optimizerï¼Œä¹Ÿæ˜¯ä¸€ä¸ªAdamä¼˜åŒ–å™¨æ‰©å±•ï¼Œä½¿ç”¨äº†Noamçš„æ–¹æ³•ï¼Œå³ç»“åˆäº†warmupå’Œdecayä¸¤ç§å­¦ä¹ ç‡è°ƒæ•´çš„æ–¹æ³•ã€‚</p>

<blockquote>
  <p>The â€œnoamâ€ scheme is just a particular way how to put the warmup and decay together (linear warmup for a given number of steps followed by exponential decay).</p>
</blockquote>

<p>æ‰€ä»¥run_epochè¿™ä¸ªå‡½æ•°ä¾æ¬¡åšäº†ï¼š</p>

<ol>
  <li>ä»data_iterä¸­å¾—åˆ°æºå’Œç›®æ ‡å¥å­ï¼Œå¹¶å®Œæˆç›®æ ‡ä½ç§»å’Œmaskç­‰å·¥ä½œã€‚</li>
  <li>åˆ©ç”¨model.forward()è¿›è¡Œå‰å‘ä¼ æ’­å¾—åˆ°outã€‚</li>
  <li>åˆ©ç”¨loss_compute()å‡½æ•°ä¾æ¬¡å°†outæ˜ å°„åˆ°vocabç©ºé—´ï¼Œå†å’Œä½ç§»è¿‡çš„targetä¹‹é—´çš„å·®è·ï¼Œå¹¶åå‘ä¼ æ’­ä¸€æ¬¡ã€‚</li>
</ol>

<p>Realworldä»»åŠ¡ä¸­ç”¨çš„<code class="highlighter-rouge">MultiGPULossCompute(generator, criterion, devices, opt=None, chunk_size=5)</code>åšçš„äº‹æƒ…æ˜¯å·®ä¸å¤šçš„ï¼Œ</p>

<h2 id="greedy-decode">Greedy Decode</h2>

<p>é¢„æµ‹é˜¶æ®µswtich the model to evalue modeï¼Œç”¨greedy_decodeå‡½æ•°æ¥é¢„æµ‹è¾“å‡ºã€‚<strong><em>åœ¨åªæœ‰æºå¥æ²¡æœ‰ç›®æ ‡å¥çš„çœŸå®é¢„æµ‹åœºæ™¯ä¸‹ï¼Œä¸å†èƒ½ä½¿ç”¨maskå‡ºçš„é˜¶æ¢¯çŸ©é˜µï¼Œå¿…é¡»one-by-oneé¢„æµ‹ã€‚</em></strong></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">greedy_decode</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">src</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">max_len</span><span class="p">,</span> <span class="n">start_symbol</span><span class="p">):</span>
    <span class="n">memory</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">src</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">)</span>
    <span class="n">ys</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="n">start_symbol</span><span class="p">)</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">src</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_len</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">memory</span><span class="p">,</span> <span class="n">src_mask</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> 
                           <span class="n">subsequent_mask</span><span class="p">(</span><span class="n">ys</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">src</span><span class="o">.</span><span class="n">data</span><span class="p">))</span>
        <span class="n">prob</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generator</span><span class="p">(</span><span class="n">out</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">_</span><span class="p">,</span> <span class="n">next_word</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="nb">max</span><span class="p">(</span><span class="n">prob</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">next_word</span> <span class="o">=</span> <span class="n">next_word</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">ys</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">ys</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">src</span><span class="o">.</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">fill_</span><span class="p">(</span><span class="n">next_word</span><span class="p">)],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ys</span>
</code></pre></div></div>

<p>ä¸€æ¬¡æ€§å°†srcå¥å­encodeå‡ºmemoryï¼ŒysæŒç»­è®°å½•æ–°çš„é¢„æµ‹è¯æ±‡ï¼Œé¦–å…ˆå®šä¹‰ä¸º1x1çš„sosï¼Œåˆ©ç”¨ä»–å»é¢„æµ‹ä¸‹ä¸€ä¸ªï¼Œç„¶åä¸æ–­çš„catåˆ°åé¢ï¼Œæ³¨æ„decoderçš„è¾“å‡ºå¹¶æ²¡æœ‰ä¸€ä¸ªæ˜ç¡®çš„é™åˆ¶ï¼Œ<strong><em>ä½ 1x512çš„ä¹Ÿå¯ä»¥ç›´æ¥è¾“å…¥è¿›å»ï¼Œä½œä¸ºqueryå’Œ10x512çš„ä½œattentionæœ€åè¾“å‡º1x512çš„æ²¡é—®é¢˜ã€‚</em></strong>è€Œä¸”batchç»´åº¦ä¹Ÿä¸æ˜¯å¿…é¡»çš„ï¼Œattentionè®¡ç®—åªåœ¨ä¹æœ€åä¸¤ç»´ã€‚</p>

<h2 id="ç»´åº¦å˜åŒ–">ç»´åº¦å˜åŒ–</h2>

<p><img src="/img/flow2.png" alt="" /></p>

<h2 id="multi-gpu">Multi-GPU</h2>

<p>æŒ‰ç…§ä½œè€…çš„è¯´æ³•ï¼Œä»–ä»¬å®ç°äº†å¤šGPUçš„word generationï¼Œå°±æ˜¯åœ¨è®­ç»ƒé˜¶æ®µå°†è¯çš„ç”Ÿæˆåˆ†è§£ä¸ºä¸åŒçš„å—åœ¨GPUä¸Šå¹¶è¡Œå®ç°ï¼Œç”¨çš„pytorchå¹¶è¡ŒåŸè¯­å®ç°çš„ã€‚</p>

<p>å®ç°ä¸º<code class="highlighter-rouge">MultiGPULossCompute(generator, criterion, devices, opt, chunk_size)</code>ï¼Œæœ¬è´¨è·Ÿä¸Šé¢æåˆ°çš„å¤åˆ¶ä»»åŠ¡çš„SimpleLossæ˜¯ä¸€æ ·çš„ï¼Œ<strong>æ˜ å°„åˆ°vocabï¼Œæ±‚æŸå¤±å¹¶ä¼˜åŒ–ã€‚</strong></p>

<ul>
  <li>nn.parallel.scatter(tensor, devices)è¿”å›ä¸€ä¸ªtupleï¼Œå…¶ä¸­å…ƒç´ æ˜¯æŒ‰ç¬¬ä¸€ä¸ªç»´åº¦åˆ†å‰²çš„tensorã€‚</li>
  <li>nn.parallel.replicate(model, devices)å°†ä¸€ä¸ªåœ¨primary gpuçš„modelå¤åˆ¶ä¸ºnä»½æ”¾åˆ°ä¸åŒdevicesä¸Šï¼Œè¿”å›ä¸€ä¸ªlistï¼Œå…ƒç´ æ˜¯ä¸åŒçš„modelã€‚</li>
  <li>nn.parallel.parallel_apply(models, tensors)å°±æ˜¯ä¸Šé¢ä¸¤ä¸ªè¯­å¥ç”Ÿæˆçš„ä¸œè¥¿ï¼Œéƒ½å¿…é¡»æ˜¯tuple/listå½¢å¼ï¼Œå°†ä¼šåœ¨ä¸åŒGPUä¸Šä¸€ä¸€å¯¹åº”çš„è¿›è¡Œå‰å‘ä¼ æ’­ï¼Œç„¶å<strong><em>å°†outç»“æœä¿å­˜ä¸ºä¸€ä¸ªlistï¼Œå…¶ä¸­å…ƒç´ ä¸ºä½äºä¸åŒGPUä¸Šçš„å„ä¸ªè¾“å‡º</em></strong>ã€‚ç¬¬äºŒä¸ªå‚æ•°å…¶ä¸­tensorå†è¢«ä¸€ä¸ªä¸­æ‹¬å·åŒ…å›´ä¹Ÿæ— æ‰€è°“ã€‚</li>
  <li>nn.parallel.gather(distributed_tensors, target_devices)ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯ä¸Šé¢ä¸€æ­¥ğŸ‘†ç”Ÿæˆçš„list of tensorï¼Œç¬¬äºŒä¸ªå‚æ•°æ˜¯ä¸€ä¸ª<strong>æ•°å­—æˆ–è€…torch.deviceå˜é‡</strong>ï¼Œ<strong><em>è¿™ä¸€æ­¥è¿”å›ä¸€ä¸ªtensorï¼Œå°†listä¸­æ‰€æœ‰tensorçš„ç¬¬ä¸€ä¸ªç»´åº¦å‹ç¼©åœ¨ä¸€èµ·ã€‚</em></strong></li>
</ul>

<p>targetçš„å½¢çŠ¶ä¼šå°‘ä¸€ç»´ï¼Œæ²¡ç»è¿‡embeddingã€‚</p>

<p>criterionæœ¬è´¨ä¸Šä¹Ÿæ˜¯ä¸€ä¸ªå‰å‘ç½‘ç»œï¼Œè¿™ä¸ªä»Label_smoothingçš„å†™æ³•ä¸Šä¹Ÿèƒ½çœ‹å‡ºæ¥ã€‚</p>

<ol>
  <li>ä¸€äº›å‡†å¤‡å·¥ä½œã€‚</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">total</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="n">generator</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">replicate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">generator</span><span class="p">,</span> <span class="n">devices</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">devices</span><span class="p">)</span>
<span class="n">out_scatter</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">target_gpus</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">devices</span><span class="p">)</span>
<span class="n">out_grad</span> <span class="o">=</span> <span class="p">[[]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">out_scatter</span><span class="p">]</span>	<span class="c1"># ä¸€ä¸ªlisté‡Œæœ‰å‡ ä¸ªç©ºçš„list
</span><span class="n">targets</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">target_gpus</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">devices</span><span class="p">)</span>
</code></pre></div></div>
<p>â€‹	æ³¨æ„è¿™ä¸ªout_gradçš„æ ¼å¼ä¸º[[], [], â€¦, []]ï¼Œå…ƒç´ æ•°ç›®å’ŒGPUæ•°ç›®ç›¸åŒã€‚</p>
<ol>
  <li>å°†è¯çš„ç”Ÿæˆå˜ä¸ºchunk-levelï¼Œåˆ†æˆäº†å‡ ä»½ã€‚</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">out_scatter</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">chunk_size</span><span class="p">):</span>
	<span class="c1"># æ¥ä¸‹æ¥è¿›è¡Œçš„éƒ½åœ¨è¿™ä¸ªå¾ªç¯ä¹‹å†…
</span></code></pre></div></div>

<ol>
  <li>åˆ†å¸ƒå¼é¢„æµ‹æ¯ä¸ªä½ç½®çš„è¯æ±‡ã€‚</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">out_column</span> <span class="o">=</span> <span class="p">[[</span><span class="n">Variable</span><span class="p">(</span><span class="n">o</span><span class="p">[:,</span> <span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="n">chunk_size</span><span class="p">]</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">opt</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">)]</span> 							<span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">out_scatter</span><span class="p">]</span>	<span class="c1"># [[tensor], [tensor], ..., [tensor]]
</span><span class="n">gen</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">parallel_apply</span><span class="p">(</span><span class="n">generator</span><span class="p">,</span> <span class="n">out_column</span><span class="p">)</span>
</code></pre></div></div>

<p>â€‹	ç¬¬ä¸€å¥è¯å¯¹åˆ†å¸ƒåœ¨ä¸åŒGPUä¸Šçš„out_scatterè¿›è¡Œåˆ‡ç‰‡ï¼Œå–å‰chunk_sizeè¡Œï¼ˆä¸ªä½ç½®ï¼‰ï¼Œå»é¢„æµ‹è¯¥ä½ç½®çš„è¯æ±‡ã€‚</p>

<p>â€‹	è¿™ä¸€æ­¥æ˜¯åˆ†å¸ƒåœ¨å„ä¸ªGPUä¸Šè¿›è¡Œçš„ï¼Œå› ä¸ºout_scatterä¸­çš„å…ƒç´ æœ¬æ¥å°±æ˜¯åœ¨ä¸åŒGPUä¸Šçš„ã€‚</p>

<p><strong><em>âš ï¸ç‰¹åˆ«æ³¨æ„è¿™é‡Œçš„.dataï¼Œä½¿å¾—è„±ç¦»äº†ä¹‹å‰çš„è®¡ç®—å›¾ç‹¬ç«‹ï¼å˜æˆäº†ä¸€ä¸ªæ–°çš„leaf_nodeï¼æ˜¯åé¢é€»è¾‘çš„èµ·ç‚¹ã€‚</em></strong></p>

<ol>
  <li>è®¡ç®—æŸå¤±ã€‚</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">y</span> <span class="o">=</span> <span class="p">[(</span><span class="n">g</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">g</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)),</span> <span class="n">t</span><span class="p">[:,</span> <span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="n">chunk_size</span><span class="p">]</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span> 			 <span class="k">for</span> <span class="n">g</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">gen</span><span class="p">,</span> <span class="n">targets</span><span class="p">)]</span> <span class="c1"># g: (batch/n, chunk_size, vocab), t: (batch/n, vocab)
</span><span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">parallel_apply</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="c1"># criterion is also a pytroch module
</span></code></pre></div></div>

<p>â€‹	ç¬¬ä¸€å¥åšäº†ä¸€ä¸ªç»´åº¦reductionï¼Œä¿æŒæœ€åä¸€ä¸ªvocabç»´åº¦ä¸å˜ï¼Œå‹ç¼©äº†batch_newå’Œchunkä¸¤ä¸ªç»´åº¦ï¼›åŒæ—¶è®©targetå˜æˆä¸€æ’ï¼Œè¿™æ ·æ–¹ä¾¿ç¬¬äºŒæ­¥ä½œlossè®¡ç®—ã€‚</p>

<p>â€‹	ç¬¬äºŒæ­¥å°†y[(g1, t1), (g2, t2), .., (gn, tn)]ï¼Œè¾“å…¥åˆ°å¯¹åº”çš„criterionç½‘ç»œï¼Œ[c1, c2, â€¦, cn]ã€‚è¿”å›ä¸€ä¸ªlossçš„listã€‚æ³¨æ„è¿™é‡Œçš„criterionåœ¨1æ­¥ä¸­ä¹Ÿå·²ç»å¤åˆ¶åˆ°äº†å„å—GPUä¸Šã€‚<strong>æ³¨æ„âš ï¸è¿™é‡Œçš„loss listä¸­çš„æ¯ä¸€ä¸ªå€¼éƒ½æ˜¯torch.size([])çš„0ç»´tensorã€‚</strong></p>

<ol>
  <li>ç´¯ç§¯lossï¼Œ<strong>å¹¶normalize</strong></li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">l</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">target_device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">devices</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">l</span> <span class="o">=</span> <span class="n">l</span><span class="o">.</span><span class="nb">sum</span><span class="p">()</span>
<span class="k">if</span> <span class="n">l</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([]):</span>  <span class="c1"># handle 1 GPU case
</span>		<span class="n">total</span> <span class="o">+=</span> <span class="n">l</span><span class="o">.</span><span class="n">item</span><span class="p">()</span> <span class="o">/</span> <span class="n">normalize</span>
<span class="k">else</span><span class="p">:</span>
		<span class="n">l</span> <span class="o">=</span> <span class="n">l</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="n">normalize</span>  
		<span class="n">total</span> <span class="o">+=</span> <span class="n">l</span><span class="o">.</span><span class="n">data</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
</code></pre></div></div>

<p>â€‹	é¦–å…ˆç¬¬ä¸€è¡Œç¬¬ä¸€ä¸ªå‚æ•°lossæ˜¯ä¸€ä¸ªlist of 0-dimensional tensorsï¼Œå…¶å…ƒç´ åˆ†å¸ƒåœ¨ä¸åŒçš„GPUä¸Šï¼Œç»è¿‡gatherä¹‹åç»´åº¦ä¼šå‘ç”Ÿå˜åŒ–ï¼Œæˆä¸º1ç»´tensorï¼Œå½¢çŠ¶ç”±<strong><em>torch.size([])-&gt;torch.size([n])</em></strong>ã€‚</p>

<p>â€‹	ç„¶ååšsum()è¿ç®—åˆå˜æˆ0ç»´tensorï¼Œ<strong>è¿™é‡Œçš„æ¡ä»¶è¯­å¥åº”è¯¥æœ‰é—®é¢˜ï¼Œæˆ‘è§‰å¾—æ°¸è¿œéƒ½æ˜¯ç¬¬ä¸€ç§æƒ…å†µã€‚</strong>totalå˜é‡<strong><em>ç´¯ç§¯æœ¬æ¬¡å¾ªç¯çš„chunkçš„losså€¼</em></strong>ï¼Œè¿™é‡Œä¼ çš„normalizeå‚æ•°æ˜¯batch.ntokensï¼Œæ˜¯ä¸€ä¸ªbatchä¸­æ‰€æœ‰çš„tokenæ•°ç›®ã€‚</p>

<ol>
  <li>åå‘ä¼ æ’­è®¡ç®—æ¢¯åº¦ï¼Œåœ¨trainæ—¶optæœ‰ï¼Œæ˜¯Noamï¼›evalæ—¶æ²¡æœ‰optã€‚</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">opt</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
		<span class="n">l</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
		<span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">loss</span><span class="p">):</span>
				<span class="n">out_grad</span><span class="p">[</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">out_column</span><span class="p">[</span><span class="n">j</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">clone</span><span class="p">())</span>
</code></pre></div></div>

<p>â€‹	æ³¨æ„è¿™é‡Œçš„çš„<code class="highlighter-rouge">l</code>çš„è®¡ç®—å›¾åˆ°out_columnä¸ºæ­¢ï¼Œå› ä¸ºåˆ›å»ºout_columnæ—¶è°ƒç”¨äº†<code class="highlighter-rouge">.data</code>ã€‚</p>

<p>â€‹	è¿™é‡Œçš„losså˜é‡æ˜¯4.æ­¥ä¸­æœ€åå¾—åˆ°çš„ï¼Œåˆ†å¸ƒåœ¨ä¸åŒGPUä¸Šçš„0ç»´tensorã€‚è¿™ä¸ªforå¾ªç¯ä¸»è¦æ˜¯ç”¨è¿™ä¸ªlossè°ƒå–GPUä¸ªæ•°å¯¹åº”çš„jï¼Œ<strong><em>out_columnæ˜¯ä¸€ä¸ªä¸¤å±‚liståŒ…è£¹çš„tensorï¼Œæ‰€ä»¥è¦å–ä¸¤æ¬¡[]</em></strong>ï¼Œå°†å¯¹åº”GPUä¸Šçš„out_columnçš„è¿™ä¸ªchunkçš„grad append åˆ° out_gradè¿™ä¸ªlistå¯¹åº”çš„2çº§listä½ç½®é‡Œï¼Œout_gradä¹Ÿæ˜¯ä¸€ä¸ªä¸¤å±‚çš„listã€‚</p>

<p>â€‹	æ‰€ä»¥out_gradè¿™ä¸ªlisté‡Œæœ‰nä¸ªlistï¼Œå…¶ä¸­æ¯ä¸ªlistæ˜¯ä¸€ä¸ª(batch/n, chunk_size, out_size)ä¸ºå…ƒç´ çš„listï¼Œå…¶å†…å®¹æ˜¯åˆšåˆšbackwardå¾—åˆ°çš„gradã€‚</p>

<p>â€‹	å¾ªç¯å°±åˆ°è¿™é‡Œç»“æŸã€‚</p>

<ol>
  <li>æ‰€æœ‰<strong>å¾ªç¯ç»“æŸä»¥å</strong>ï¼Œè¿›è¡Œä¸‹é¢çš„ï¼š</li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">opt</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
		<span class="n">out_grad</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">og</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">og</span> <span class="ow">in</span> <span class="n">out_grad</span><span class="p">]</span>
    <span class="n">o1</span> <span class="o">=</span> <span class="n">out</span>
    <span class="n">o2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="n">out_grad</span><span class="p">,</span> <span class="n">target_device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">devices</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">o1</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">gradient</span><span class="o">=</span><span class="n">o2</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">opt</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="k">return</span> <span class="n">total</span> <span class="o">*</span> <span class="n">normalize</span>
</code></pre></div></div>

<p>â€‹	ç¬¬ä¸€æ­¥æŠŠout_gradä¸­æ¯ä¸ªäºŒçº§list(å³æ¯ä¸ªGPUä¸Šçš„)ä¸åŒchunkçš„gradæŒ‰dim=1ï¼Œä¹Ÿå°±æ˜¯è¡Œ(å› ä¸ºè¿™é‡Œæ˜¯3ç»´ï¼Œç¬¬ä¸€ä¸ªç»´åº¦æ˜¯batchç¬¬äºŒä¸ªæ˜¯è¡Œç¬¬ä¸‰ä¸ªæ˜¯åˆ—)ï¼Œæ‹¼æ¥èµ·æ¥ï¼Œå›åˆ°æ²¡åˆ†chunkä¹‹å‰çš„å½¢çŠ¶ã€‚æ­¤æ—¶out_gradæ˜¯ä¸€ä¸ªå…ƒç´ ä¸ºtensorçš„listã€‚</p>

<p>â€‹	æ‰€ä»¥gatherä¸åŒGPUä¸Šçš„gradå°±ç†æ‰€å½“ç„¶äº†ï¼Œæ³¨æ„è¿™ä¸ªgradæ˜¯<strong><em>loss wrt. out</em></strong>ï¼Œæ ¹æ®é“¾å¼æ³•åˆ™è¦ä¹˜ä»¥<strong><em>out wrt. Parameter</em></strong>ï¼Œæ‰€ä»¥ä¼šæœ‰<code class="highlighter-rouge">o1.backward(gradient=o2)</code>è¿™å¥è¯ï¼Œå°†o2è®¤ä¸ºæ˜¯æƒé‡å³å¯ã€‚</p>

<p>â€‹	ä¹‹åè¿›è¡Œä¸€ä¸ªä¼˜åŒ–stepå¹¶æ¸…é›¶æ¢¯åº¦ã€‚</p>

<p>â€‹	æœ€åè¿”å›loss * normalizeï¼Œä¹‹å‰é™¤ä»¥normalizeè¿™é‡Œå†ä¹˜ã€‚</p>

:ET