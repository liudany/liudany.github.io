I"r=<h1 id="tutorial-on-emnlp">Tutorial on EMNLP</h1>

<h2 id="how-to-code">How to code</h2>
<ol>
  <li>Meaningful names</li>
  <li>Shape comments on tensors</li>
  <li>
    <p>Comments describing non-obvious logic</p>
  </li>
  <li>Don’t start from scratch! Use someone else’s components.</li>
</ol>

<p><strong>write code for people, not for machine.</strong></p>

<ol>
  <li>
    <p>minimal test. 一次性检查所有的实验是浪费时间，但是有些minimal单元不可测试，make sure data processing works consistently, taht tensor. operations run, gradients are non-zero. Run on small test fixtures.</p>
  </li>
  <li>
    <p>no hard-code. 不要把参数直接写在函数里，例如 self.input_embedding = embedding(100)</p>
  </li>
  <li>
    <p>Use EXCEL to keep track of what you ran. Note down the experimenter, background and any other detailed information.</p>
  </li>
  <li>
    <p>控制变量， it’s better to use a configuration files.</p>
  </li>
  <li>
    <p>Use Tensorboard NOW.</p>
  </li>
  <li>
    <p>Source control 版本控制，如果事情不对，赶紧revert回原来的版本</p>
  </li>
  <li>
    <p>unit test, check that a small part of your code works correctly.</p>
  </li>
</ol>

<h2 id="allennlp">AllenNLP</h2>

<p>DatasetReader file-&gt;instance</p>

<p>TokenIndexer(character-level? bpe? pos?….)</p>

<p>Field -&gt; Instance -&gt; Vocabulary</p>

<p>DataIterator 迭代batch</p>

<h3 id="tokenindexers--toeknembedder">TokenIndexers &amp; ToeknEmbedder</h3>

<p>words -&gt; TokenIndexers -&gt; ids -&gt; TokenEmbedder -&gt; tensors</p>

<h3 id="seq2vecencoder">Seq2VecEncoder</h3>

<p>(batch_size, sequence_length, embedding_dim) -&gt; (batch_size, embedding_dim), such as BOW, LSTM(last output), CNN + Pooling</p>

<h3 id="seq2seqencoder">Seq2SeqEncoder</h3>

<p>(batch_size, sequence_length, embedding_dim) -&gt; (batch_size, sequence_length, embedding_dim), RNNs, self-attention, do-nothing(lol)</p>

<h3 id="attention">Attention</h3>

<p>(batch_size, sequence_length, embedding_dim)（很长的输入）, (batch_size, embedding_dim) （一个词）-&gt; (batch_size, sequence_length)（算出这个词和每个词的attention）</p>

<h3 id="matrixattention">MatrixAttention</h3>

<p>扩展到一句话的每个词，对另一句话的attention。</p>

<h3 id="spanextractor">SpanExtractor</h3>

<p>提出一段文本的embedding.</p>

<h3 id="flowchart">Flowchart</h3>

<p>file -&gt; DatasetReader -&gt; instance -&gt; Model -&gt; predictions/loss 
use a JSONN config and AllenNLP training code</p>

<h3 id="declarative-syntax">Declarative syntax</h3>

<p>Using JSON to specify an entire experiment, including changing architectures without changing code</p>

<h1 id="github-tutorial">Github tutorial</h1>

<h2 id="preliminary">Preliminary</h2>

<p>讲基于Json config文件实现模型的机理。</p>

<h2 id="dataset-instance-and-field">Dataset, Instance and Field</h2>

<p>We train and evaluate our models on <code class="highlighter-rouge">Dataset</code>.</p>

<p>A <code class="highlighter-rouge">Dataset</code> is a collection of <code class="highlighter-rouge">Instance</code>s.</p>

<p>An <code class="highlighter-rouge">Instance</code> consists of <code class="highlighter-rouge">Field</code>s, such as <code class="highlighter-rouge">TextField</code> and <code class="highlighter-rouge">SequenceLabelField</code>.</p>

<p>注意<code class="highlighter-rouge">TextField</code>里面是words/tokens，这里还没有变成Embedding。</p>

<p>有什么用？？？？？？？</p>

<h2 id="datasetreader">DatasetReader</h2>

<p>The first section we define in configuration file.</p>
<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="w">  </span><span class="nl">"dataset_reader"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"sequence_tagging"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"word_tag_delimiter"</span><span class="p">:</span><span class="w"> </span><span class="s2">"/"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"token_indexers"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
      </span><span class="nl">"tokens"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"single_id"</span><span class="p">,</span><span class="w">
        </span><span class="nl">"lowercase_tokens"</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
      </span><span class="p">},</span><span class="w">
      </span><span class="nl">"token_characters"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"characters"</span><span class="w">
      </span><span class="p">}</span><span class="w">
    </span><span class="p">}</span><span class="w">
  </span><span class="p">}</span><span class="err">,</span><span class="w">
</span></code></pre></div></div>
<p><code class="highlighter-rouge">word_tag_delimiter</code> specify the symbol between a word the corresponding tag.
There is also <code class="highlighter-rouge">token_delimiter</code>. The deafult is split-on-whitespace.</p>

<p>Note that we should convert tokens/words into arrays(not embedding yet) by <code class="highlighter-rouge">token_indexers</code>. Here we specify two methods, <code class="highlighter-rouge">toekns</code> and <code class="highlighter-rouge">toekn_characters</code> which indicates character-level encoding.
这里tokens和token_characters名字可以自己乱起吗？？？重在type的值来重构子类，好像和每个type上面的名字无关，</p>

<h2 id="training-and-validation-data">Training and Validation Data</h2>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="w">  </span><span class="nl">"train_data_path"</span><span class="p">:</span><span class="w"> </span><span class="s2">"https://allennlp.s3.amazonaws.com/datasets/getting-started/sentences.small.train"</span><span class="err">,</span><span class="w">
  </span><span class="nl">"validation_data_path"</span><span class="p">:</span><span class="w"> </span><span class="s2">"https://allennlp.s3.amazonaws.com/datasets/getting-started/sentences.small.dev"</span><span class="err">,</span><span class="w">
</span></code></pre></div></div>

<p>They can be specified either as local paths or URLs.</p>

<h2 id="model">Model</h2>

<h3 id="type">type</h3>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="w">  </span><span class="nl">"model"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"simple_tagger"</span><span class="p">,</span><span class="w">
</span></code></pre></div></div>

<p>Firstly we specify the subclass of our model. A <code class="highlighter-rouge">,</code> indicates the end of a property.</p>

<h3 id="text_field_embedder">text_field_embedder</h3>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="w">  </span><span class="nl">"text_field_embedder"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"tokens"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
      </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"embedding"</span><span class="p">,</span><span class="w">
      </span><span class="nl">"embedding_dim"</span><span class="p">:</span><span class="w"> </span><span class="mi">50</span><span class="w">
    </span><span class="p">},</span><span class="w">
    </span><span class="nl">"token_characters"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
      </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"character_encoding"</span><span class="p">,</span><span class="w">
      </span><span class="nl">"embedding"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="nl">"embedding_dim"</span><span class="p">:</span><span class="w"> </span><span class="mi">8</span><span class="w">
      </span><span class="p">},</span><span class="w">
      </span><span class="nl">"encoder"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"cnn"</span><span class="p">,</span><span class="w">
        </span><span class="nl">"embedding_dim"</span><span class="p">:</span><span class="w"> </span><span class="mi">8</span><span class="p">,</span><span class="w">
        </span><span class="nl">"num_filters"</span><span class="p">:</span><span class="w"> </span><span class="mi">50</span><span class="p">,</span><span class="w">
        </span><span class="nl">"ngram_filter_sizes"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="w">
          </span><span class="mi">5</span><span class="w">
        </span><span class="p">]</span><span class="w">
      </span><span class="p">},</span><span class="w">
      </span><span class="nl">"dropout"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.2</span><span class="w">
    </span><span class="p">}</span><span class="w">
  </span><span class="p">}</span><span class="err">,</span><span class="w">
</span></code></pre></div></div>
<p>It has an entry for each of the <code class="highlighter-rouge">token_indexers</code> methods.</p>

<p>The “tokens” input gets fed into an Embedding module that embeds the vocabulary words in a 50-dimensional space, as specified by the <code class="highlighter-rouge">embedding_dim</code> parameter.</p>

<p>The output of <code class="highlighter-rouge">text_field_embedder</code> is a 50-dimensional vector for “tokens” <strong>concatenated</strong> with a 50-dimensional vector for “token_characters”; that is, a 100-dimensional vector.</p>

<p>一个type后面跟着的，都是这个子类的属性，直到逗号结束。
一个大括号开始的，必然是一个type和type的值。选择两种embedding方法那里除外，也可以说如果没有type，就是一种 concatenate的方法？？？</p>

<h3 id="seq2seqencoder-1">Seq2SeqEncoder</h3>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="w">    </span><span class="nl">"encoder"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
      </span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"lstm"</span><span class="p">,</span><span class="w">
      </span><span class="nl">"input_size"</span><span class="p">:</span><span class="w"> </span><span class="mi">100</span><span class="p">,</span><span class="w">
      </span><span class="nl">"hidden_size"</span><span class="p">:</span><span class="w"> </span><span class="mi">100</span><span class="p">,</span><span class="w">
      </span><span class="nl">"num_layers"</span><span class="p">:</span><span class="w"> </span><span class="mi">2</span><span class="p">,</span><span class="w">
      </span><span class="nl">"dropout"</span><span class="p">:</span><span class="w"> </span><span class="mf">0.5</span><span class="p">,</span><span class="w">
      </span><span class="nl">"bidirectional"</span><span class="p">:</span><span class="w"> </span><span class="kc">true</span><span class="w">
    </span><span class="p">}</span><span class="w">		</span><span class="err">//注意最后一个属性的结束可以不加逗号</span><span class="w">
</span></code></pre></div></div>
<p>The LSTM layer is just a thin wrapper around <code class="highlighter-rouge">torch.nn.LSTM</code>, and its parameters are passed through to the PyTorch constructor. Its input size should <strong>match</strong> the 100-dimensional output size of the previous embedding layer.</p>

<p>The output of this lstm layer gets passed to a linear layer that <strong>doesn’t need any configuration</strong>.
这里线性层的输出维度在datasetreader时候就决定了吗？？？？</p>

<h2 id="train-your-model">Train your model</h2>

<p>The rest of config file is dedicated to the training process.</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="w">  </span><span class="nl">"iterator"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="nl">"type"</span><span class="p">:</span><span class="w"> </span><span class="s2">"basic"</span><span class="p">,</span><span class="w"> </span><span class="nl">"batch_size"</span><span class="p">:</span><span class="w"> </span><span class="mi">32</span><span class="p">}</span><span class="err">,</span><span class="w">
</span></code></pre></div></div>
<p>The <code class="highlighter-rouge">BasicIterator</code> pads our data and processes it in batches of size 32.</p>

<div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="w">  </span><span class="nl">"trainer"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="nl">"optimizer"</span><span class="p">:</span><span class="w"> </span><span class="s2">"adam"</span><span class="p">,</span><span class="w">
    </span><span class="nl">"num_epochs"</span><span class="p">:</span><span class="w"> </span><span class="mi">40</span><span class="p">,</span><span class="w">
    </span><span class="nl">"patience"</span><span class="p">:</span><span class="w"> </span><span class="mi">10</span><span class="p">,</span><span class="w">
    </span><span class="nl">"cuda_device"</span><span class="p">:</span><span class="w"> </span><span class="mi">-1</span><span class="w">
  </span><span class="p">}</span><span class="w">
</span><span class="err">}</span><span class="w">
</span></code></pre></div></div>

<p>The first line indicates Adam optimizer is used with default parameters.</p>

<p><code class="highlighter-rouge">patience</code> stop prematurely if we get no improvement for 10 epochs.</p>

<p>Change <code class="highlighter-rouge">cuda_device</code> to your GPU device id. ‘-1’ indicates CPU.</p>

<p>The training configuration is always saved as part of the model archive, which means that you can always see how a saved model was trained.</p>

<h2 id="conclusion">Conclusion</h2>

<p>The configuration file consists of 5 parts: <code class="highlighter-rouge">data_path</code>, <code class="highlighter-rouge">dataset_reader</code>, <code class="highlighter-rouge">model</code>, <code class="highlighter-rouge">iterator</code> and <code class="highlighter-rouge">trainer</code>.</p>

<h1 id="creating-your-own-models">Creating Your Own Models</h1>

<p>logits(未经过归一化的概率，一般是softmax的输入)</p>
:ET