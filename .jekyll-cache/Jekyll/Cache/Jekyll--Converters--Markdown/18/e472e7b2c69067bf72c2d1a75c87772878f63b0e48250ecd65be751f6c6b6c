I"gJ<p>最近毕业论文要开题了，其实压力有点大的哎。</p>

<h1 id="learning-bilingual-word-embeddings-with-almost-no-bilingual-data">Learning bilingual word embeddings with (almost) no bilingual data</h1>

<h2 id="概述">概述</h2>

<p>文章提出了一种self-learning的<strong>框架</strong>来对齐两个词向量的空间，不可以说是完全的无监督，起始过程中需要一定数量的词汇作为种子来启动训练过程。</p>

<h2 id="framwork">framwork</h2>

<p><img src="/img/self-learning.png" alt="" /></p>

<ul>
  <li>D为初始化的种子词对，X和Z分别为source和target的语言空间，W为学习的线性变换。</li>
  <li>从D中现有的词对学习linear transformation W。</li>
  <li>由上一步得到的W将X空间中的向量全部映射到Z区间，即XW与Z同在Z空间，之后在Z空间寻找新的词对，扩充D。</li>
  <li>由扩充的D，重新学习W，重复上面两步。</li>
</ul>

<h2 id="embedding-mapping-w">Embedding mapping W</h2>

<script type="math/tex; mode=display">W ^ { * } = \operatorname { argmin } _ { W } \sum _ { i } \sum _ { j } D _ { i j } \left\| X _ { i } * W - Z _ { j \downarrow } \right\| ^ { 2 }</script>

<p>最小化上式，其中$D_{ij}$表示X中的第i个向量$X _ {i*}$与Z中的第j个向量$Z _ {j*}$是否为对齐的，即是否为对应词。同时有以下操作：</p>
<ul>
  <li>对X和Z进行length normalize and mean center的预处理。</li>
  <li>限制W为一个正交矩阵<strong>（这一步的目的？）</strong>。</li>
</ul>

<p>这样最小化上式等价于最大化：</p>

<p><script type="math/tex">W ^ { \backslash ^ { * } } = \operatorname { argmax } _ { W } \operatorname { Tr } \left( X W Z ^ { T } D ^ { T } \right)</script>
该问题的最优正交解由下式给出：</p>

<p><script type="math/tex">W ^ { \backslash ^ { * } } = U V ^ { T }</script>
这是一个Singular Value Decomposition（奇异值分解）：</p>

<p><script type="math/tex">X ^ { T } D Z = U \sum V ^ { T }</script>
由于D是一个稀疏的矩阵，所以计算会非常有效率。<strong>（？）</strong></p>

<h2 id="dictionary-induction-d">Dictionary induction D</h2>

<p>Previous work通常使用nearest neighbor retrieval方法，而本文作者使用dot product来衡量两个向量之间的相似性（XW和Z之间），这一步基于前面的length normalize和mean center来保证准确性。</p>

<p>set $D _ {ij}=1$ if $j = argmax _ {k}(X _ {i*}W)\cdot Z _ {k*}$ and $D _ {ij}=0$ otherwise.</p>

<p>这一步也可以通过vectorize的方法来减少运算资源。</p>

<h1 id="unsupervised-machine-translation">Unsupervised Machine Translation</h1>

<h2 id="概述-1">概述</h2>

<p>这个模型是基于上面一篇的Unsupervised embedding mappings的工作来的，完全使用两种语言各自的单语料库进行训练，具体模型也是modified attentional encoder-decoder model，细节上结合了denoise（降噪）和back-translation（回译）技术。</p>

<h2 id="模型结构">模型结构</h2>

<p><img src="/img/shared-encoder.png" alt="" /></p>

<ol>
  <li>
    <p>左边是一个shared-encoder，将两种语言映射到同一个latent space，并期待捕捉到真正关乎<strong>语义</strong>、与语言种类无关的信息。这一步的工作也是作者团队之前的工作<a href="http://aclweb.org/anthology/P17-1042">Learning bilingual word embeddings with (almost) no bilingual data</a>。</p>
  </li>
  <li>
    <p>右边是两个decoder，分别用于将encoder编码后的句子decompose回两种语言空间内。</p>
  </li>
  <li>
    <p>用于两种语言的是<strong>fixd embedding</strong>，具体来讲word2vec对两种单语料训练完毕后，在后续训练中不再更新。encoder所做的工作就是将这些fixed embeddings映射到同一个表征语义的laten space。</p>
  </li>
</ol>

<h2 id="训练过程">训练过程</h2>

<p>基于上面的模型训练过程中交替执行denoise和on-the-fly back translation两种步骤：</p>

<h3 id="denoise">denoise</h3>

<p>即L1通过shared-encoder之后再经过L1-decoder来reconstruct原句，期望decoder能够学到从语义空间重构句子的能力。但是这里训练起来很容易变成一个<strong>word-by-word copy</strong>的过程，为了避免这种问题，作者在原句子中添加一些噪声，这里具体的指交换部分词语的顺序。这里也是为了让decoder能够学到不同语言的语序信息。</p>

<h3 id="on-the-fly-back-translation">on-the-fly back-translation</h3>

<p>back-translation是Sennrich 15年提出的解决语料缺少问题的方法<a href="https://arxiv.org/abs/1511.06709">Improving Neural Machine Translation Models with Monolingual Data</a>，普通的back-translaiton是用训练好的翻译模型将L1单语料库一次性翻译为L2语言，之后再将其<strong>用于L2-&gt;L1</strong>的翻译训练中，<strong>这是因为对于源语言的准确度不做要求，翻译问题更关注的是翻译出的句子要是高质量的</strong>，这点与生活中的翻译是相似的，优秀的翻译对于稍显凌乱的输入也要有一定的容错率，但是翻译出来的句子一定要是通顺连贯的。</p>

<p>本文中on-the-fly的意思为对于每一个mini-batch进行back-translation（通过shared-encoder和另一种语言的decoder），这样随着训练过程回译的句子质量也会提升。</p>

<table>
  <tbody>
    <tr>
      <td>具体的流程为L1语言的句子s1经过encoder编码（没有denoise过程），再经过L2decoder解码，得到s2，得到了peseudo pair(s2, s1)，<strong>这个过程模型处于inference mode，不对任何参数做更新</strong>。得到了句子对之后，进行翻译过程的训练：s2经过encoder编码，L1decoder解码，得到translated s1，通过最大化P(s1</td>
      <td>s2)来更新参数。所以这一步更新了<strong>encoder和L1 decoer的参数，并没有更新L2 decoder的参数</strong>。</td>
    </tr>
  </tbody>
</table>

<h3 id="overall">overall</h3>

<p>整个模型的训练过程按mini-batch为单位重复以下4步：</p>

<ol>
  <li>L1 noise and reconstruct，更新encoder和L1 decoder的参数。</li>
  <li>L2 noise and reconstruct，更新encoder和L2 decoder的参数。</li>
  <li>on-the-fly back-translation from L1 to L2，更新encoder和L1 decoder的参数。</li>
  <li>on-the-fly back-translation from L2 to L1，更新encoder和L2 decoder的参数。</li>
</ol>

<h1 id="word-translation-without-parallel-data">Word Translation without Parallel Data</h1>

<h2 id="概述-2">概述</h2>

<p>这和第一篇文章都旨在对齐两个空间，第一篇文章提出了一种self-learning的框架，而这篇使用的是完全无监督的adversirial traing的方法来对齐，以及一些零碎的工作拼凑在一起，比如使用Procrustes algorithm方法来做refinement，并在生成词典时使用自己提出的CSLS方法。</p>

<h2 id="schema">Schema</h2>

<p><img src="/img/fb-word-translation.png" alt="" /></p>

<ol>
  <li>In panel(B)，用GAN方法来对齐两个空间，mapping matrix W相当于generator，descriminator来鉴别WX与Y，使之越来越相近。这样初步训练完毕后，得到了一个roughly对齐的线性变换X。</li>
  <li>In panel(C)，使用<strong>Procrustes</strong>来进一步refine。这一步使用上一步中的<strong>高频词</strong>作为anchor来进一步优化W，并将优化过后的W再应用于source space，这样可以迭代几步。</li>
  <li>In panel(D), 完成word translation，使用WX将X映射到Y空间，并利用作者提出的方法(dubbed CSLS)来寻找word pairs，这种方法可以使一些分布密集处的词汇(dubbed ‘hub’)变得稀疏一些，从而利于找到真正对应的词。</li>
</ol>

<p>所以整个系统的训练过程分为两步，一是Adversarial Training，二是利用普氏分析来refine（这里用到了CSLS）。</p>

<h2 id="adversarial-learning">Adversarial Learning</h2>

<p>两个空间的词向量都直接使用fastText训练好的（300维，基于Wikipedia），W视为generator，另外训练一个D用于鉴别WX和Y，训练具体方法都参照Goodfellow文章中的建议。</p>

<p>要注意的是矩阵W保持正交矩阵，研究表明这会提高性能，本文通过如下方法：</p>

<script type="math/tex; mode=display">W\leftarrow (1+\beta )W-\beta (WW^{T})W</script>

<h2 id="csls">CSLS</h2>

<p><strong>Cross-domain Similarity Local Scaling</strong>是作者提出的一种寻找word pair或者说寻找最近邻的新指标。首先是$r _ {T}(Wx _ {s})$这个指标来表征平均相似度：
<script type="math/tex">r _ { T } \left( W x _ { s } \right) = \frac { 1 } { K } \sum _ { y _ { t } \in N _ { T } \left( W x _ { s } \right) } \cos \left( W x _ { s } , y _ { t } \right)</script></p>

<p>用$N _ {T}(Wx _ {s})$表示在Target空间中Wxs最近的K个embedding，$cos$表示余弦相似度。</p>

<p>将$Wx _ {s}$与$y _ {t}$的相似度定义为：</p>

<script type="math/tex; mode=display">C S L S \left( W x _ { s } , y _ { t } \right) = 2 \cos \left( W x _ { s } , y _ { t } \right) - r _ { T } \left( W x _ { s } \right) - r _ { S } \left( y _ { T } \right)</script>

<h2 id="refinement">Refinement</h2>

<p>生成对抗网络训练结束后已经得到一个映射W了，但是作者认为其中的rare word没有得到充分的对齐，且也更难以对齐，所以想再加一步refine其对齐的精度，使用的是Procrustes Algorithm（普氏分析，图像中常用人脸对齐）。</p>

<p>具体操作为：</p>
<ol>
  <li>从上一步得到的WX和Y中，根据CSLS和mutual nearest neighbors的原则（word pair必须互为CSLS原则下的最近邻），选取一个高频率且高质量的词典。</li>
  <li>对词典应用SVD分解得到的solution，从而得到一个更佳的W。这一步基于W是正交阵的条件。</li>
  <li>重复1，由更佳的W再次推断一个新的dictionary（理论上会更加丰富），从而迭代。</li>
</ol>

<p>作者提到这种方法在多次迭代后，实际上提高的word translation准确率只有1%左右。<strong>（所以只进行一次Refine就好？）</strong></p>

<h2 id="validation-metric--model-selection">Validation Metric &amp; Model Selection</h2>

<p>由于是一个无监督的系统，工作在没有平行语料的前提下，所以作者提出了这种情况下的验证和模型选择方法：</p>

<ol>
  <li>从source words中选择频率最高的10k个词，用CSLS原则寻找他们的翻译词。</li>
  <li>计算这些翻译词的平均余弦距离，并将其作为validation metric。</li>
</ol>

<p>结果显示这个简单的指标与translation accuracy well correlated，所以将其作为train时的stopping criterion和最后模型选择时的标准。</p>

<h1 id="unsupervised-machine-translation-using-monolingual-corpora-only">Unsupervised Machine Translation Using Monolingual Corpora Only</h1>

<h2 id="system-overview">System Overview</h2>

<p>整体想法和前两篇文章的团队基本一致，竟然是同年发表，想法撞车了。</p>

<p><img src="/img/fb-unsupervised-system.png" alt="" /></p>

<p>所谓iterative training指的是每次encoder和decoder更新之后，就会再次进行一次训练过程。和上一篇文章的交替目标函数不同，这个系统每次将上图的4个loss加起来训练。</p>

<h3 id="denoising-auto-encoder">Denoising auto-encoder</h3>

<ul>
  <li>首先明确auto-encoder也是由encoder和decoder组成的，其作用就是先将输入压缩到一个latent space，再重构出输入。</li>
  <li>若使用最初的输入，auto-encoder只会从中学会word-by-word copy的行为，学习不到有用的信息。因此在输入中加入噪声，具体在本文中是<strong>swap&amp;drop</strong>，这样auto-encoder就必须学习到噪声化输入中的有用信息。</li>
  <li>学习到的这些有用信息，就成为<strong>latent space</strong>，作者希望这个空间是与语种无关的，单纯的保留语义信息的一个空间。因此才会有后面的adversarial过程，想要对齐这个潜在空间。</li>
  <li>这个auto-encoder中的decoder学到的实质上就是一个语言模型。</li>
</ul>

<h3 id="cross-domain-trainingback-translation">Cross-domain training(back-translation)</h3>

<ul>
  <li>理论上讲，如果encoder完美的将不同语言输入映射到同一个表征语义的laten space，decoder又完美的学会恢复到不同的语言的话，翻译系统就完成了。</li>
  <li>但是在denoising auto-encoder的训练过程中，我们只能保证decoder可以从latent space恢复出原有语种的句子，而没有进行跨语种重构的训练。因此要有cross-domain training的存在。即在训练过程中显示的构造翻译过程。</li>
  <li>首先利用上一个时刻的翻译模型，得到伪句对，之后将伪句对按照<strong>反方向</strong>进行翻译训练。</li>
</ul>

<h3 id="adversarial-training">Adversarial training</h3>

<p>鉴别编码之后的向量是来自source还是target，根本目的也是将encoder后的表征对齐到同一个latent space，能骗过descriminator。</p>

<h2 id="remark">Remark</h2>

<ul>
  <li>对两种语言使用同一个encoder和decoder，在不同的lookup table中切换即可。</li>
  <li>word-by-word translation会保留大部分的语义信息。虽然语序和结构还有点差别。</li>
</ul>

<h2 id="自然自语">自然自语</h2>

<h3 id="fb-论文总">FB 论文总</h3>

<ul>
  <li>首先，FB这篇文章真的是总结吗？？？果然不是，是综合了前两篇的好处，提出了第三种，因为BPE的应用，共享的embedding和decoder完全不同于之前的。</li>
  <li>有几个encoder和decoder？</li>
  <li>不确定正确：FB文章所讲的language model/autoencoder，意义在于，我第一轮word-by-word translation之后，语序上，句子结构上都有很大的问题，我希望通过autoencoder把它调整为正确的句子，也就是流畅一些的翻译。但是这样的话，latent space的意义？或者后面不用word-by-word了，这样做还有意义吗？</li>
  <li>PPT page2中，C和D的连续性？这两个过程有啥关系？D好像是割裂的一个back-translation过程。</li>
  <li>第一步word-by-word translation也可以用BPE，FB这篇EMNLP best paper对相似语料共同使用BPE算法来分sub-words。</li>
  <li>iterative back-translation?普通的back-translation利用平行语料库训练一个T-&gt;S的模型，再利用这个模型把大量的monolingual corpora of T换成带噪声的S’，这样大量的S’-&gt;T用来数据增强。在无监督中，这个过程被加强为，S-&gt;T的模型被加强以后，再利用这个模型去生成大量的T’，训练T-&gt;S这个方向的模型，提高效果。提高了之后又可以T-&gt;S增加语料。形成了dual-learning的结构。</li>
  <li>dual-learning是不是本身就是基于back-translation的？dual-learning的论文中说，back-translation虽然可以augment，但是不能保证质量。there is no guarantee/control on the quality of the pseudo bilingual sentence pairs.</li>
  <li>initialize的时候也不用双语词典了，直接BPE jonitly on monolingual data of S&amp;T。然后训练embedding，应用在encoder和decoder。</li>
</ul>

<h2 id="cho组论文">Cho组论文</h2>

<ul>
  <li>
    <p>所谓back-translation。如若我有一个模型Mst和Mts，S-Mst-&gt;T’， T’-Mts-&gt;S’ &lt;-&gt; S 更新 Mts得到新的Mts。
T-Mts-&gt;S’, S’-&gt;Mst-&gt;T’ &lt;-&gt; T 更新Mst得到新的Mst。
迭代。 
所以问题是，如何得到最初的Mst和Mts？In Cho组，是不是就是通过第一轮的reconstruct L1 and L2初始化整个encoder和decoder？</p>
  </li>
  <li>
    <p>这个shared encoder的意义在于？除了噱头上寻找语义空间，但是寻找这个空间是必须的吗？我用两个encoder不行吗？为了创新？因为好的效果？知乎说：使用 shared embedding 减少参数，降低了学习难度？
答：可能，使用shared encoder才会让reconstruction这个过程对翻译是有益的？如果不是shared encoder，一个reconstruct的训练过程对于翻译是无益的。没用。</p>
  </li>
  <li>
    <p>fixed-cross-lingual-embedding 让输入统一化，本身就可以捕捉语义了（初步，无语序之类的）。但是语序不行！这也是为什么要autoencoder？</p>
  </li>
</ul>

<h2 id="facebook第一篇论文">facebook第一篇论文</h2>

<ul>
  <li>用word-by-word生成第一轮翻译之后呢？用它作为平行语料，来训练encoder和decoder？对！那Cho的文章第一轮，是直接用encoder+decoder来生成平行语料了。facebook这篇的第二轮才开始用encoder-decoder生成平行语料。</li>
</ul>

<h2 id="facebook最终论文">facebook最终论文</h2>
<ul>
  <li>
    <p>所谓的language model，就是在word-by-word之后再对齐进行重构，以期望得到一个流畅正确的句子。这和我们训练auto-encoder是一样的。word-by-word + language model(auto-encoder)这就是个早期的，第一轮翻译模型。</p>
  </li>
  <li>如果iterative指的是每次翻译model更新（encoder+decoder）后再次训练，那这与普通的深度网络训练有何区别？iteratively在哪里？</li>
  <li>所谓对齐两个空间，只是学到一个变换W吗？要不要把词向量对齐到同一个latent space？还是说只在无监督的系统中，将句子对齐到同一个latent space？</li>
  <li>只有1个encoder和1个decoder怎么控制decoder得到的是哪个domain内的？同理，encoder也要注意所编码的语言的语种吗，是同一套参数吗。</li>
  <li>这个GAN的意义在哪里？是让latent space更能学习到语义上的信息吗。</li>
  <li>最后一点区别就是，Cho 那篇是从头开始训模型的；而 FAIR 这篇是用 word-by-word translation 生成第一轮时的初始翻译。<strong>从头开始训练的？没有用对齐的词典？</strong></li>
  <li>Cho 的文章直接让两种语言共用一个 encoder 和共享的词向量矩阵；而这篇文章是两种语言各有一个 encoder 和 decoder，两个 encoder 共享隐层参数，两个 decoder 共享隐层和 attention 参数，输入和输出层则不共享，但是用一个判别器分辨 encoder 编码的结果来自哪种语言，通过对抗的方法让两个 encoder 编码的语义空间重合到一块儿去，从而词向量不会在训练过程中跑偏。这种做法和前面对齐词向量的方法一脉相承，工作的延续性更好一些；而 Cho 那个拼凑感有些强。</li>
  <li>Final objective function中有两个auto和两个crossdomain，一轮迭代中为什么会有两个？不应该是只有一个输入句子吗？难道是从source中取两个，从target中取两个，都做这样的运算？i</li>
  <li>混杂语料训练？</li>
</ul>
:ET