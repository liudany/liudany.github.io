I"”9<p>ä»Šå¤©è¯»ä¸€ä¸‹PyTorch Tutorialsé‡Œ<a href="https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html#getting-dense-word-embeddings">å…³äºWord Embeddingçš„éƒ¨åˆ†</a>ã€‚</p>

<h1 id="æ¦‚å¿µ">æ¦‚å¿µ</h1>

<ol>
  <li>è¯åµŒå…¥æ˜¯ã€ŒEncoding Lexical Semanticsã€ï¼Œæ˜¯å¯¹å•è¯çš„è¯­ä¹‰è¿›è¡Œäº†ç¼–ç ã€‚æ™®é€šçš„ASCIIç æˆ–æ˜¯One-hotç¼–ç éƒ½æ˜¯å°†è¯æ±‡è§†ä¸ºindependent entitiesè€Œå¿½è§†äº†è¯æ±‡ä¹‹é—´çš„similarityæˆ–è€…è¯´semantic relationã€‚</li>
  <li>Sematic similarityå¸¸åŸºäºdistributional hypothesisï¼šè®¤ä¸ºå…·æœ‰ç›¸ä¼¼çš„ä¸Šä¸‹æ–‡çš„è¯è¯­ï¼Œåœ¨è¯­ä¹‰ä¸Šä¹Ÿæ˜¯ç›¸ä¼¼çš„ã€‚</li>
  <li>è¯åµŒå…¥å‘é‡çš„æ¯ä¸€ä¸ªç»´åº¦æ˜¯ä¸€ä¸ªç‰¹å¾ï¼Œä½†æ˜¯ç½‘ç»œå­¦ä¹ åˆ°çš„å‘é‡çš„ç‰¹å¾å¾€å¾€æ˜¯ä¸å¯è§£é‡Šçš„ã€‚</li>
</ol>

<h1 id="åŸºç¡€ç”¨æ³•">åŸºç¡€ç”¨æ³•</h1>

<p>è¯åµŒå…¥ä»¥ä¸€ä¸ªV*Dçš„çŸ©é˜µçš„çŸ©é˜µå½¢å¼ï¼Œæ¯è¡Œæ˜¯ä¸€ä¸ªè¯çš„åµŒå…¥ï¼ŒDæ˜¯å‘é‡ç»´åº¦ã€‚è¿™é‡Œä¹Ÿéœ€è¦ä¸€ä¸ªç´¢å¼•å·æ¥å¯¹åº”è¯æ±‡å’Œå‘é‡ï¼Œç”¨æ³•ä¸å‰é¢è¯è¢‹æ¨¡å‹ä¸­çš„<strong>word_to_ix</strong>ç›¸åŒï¼Œä»¥å•è¯ä¸ºç´¢å¼•ï¼Œå¯¹åº”çš„ç´¢å¼•å·ä¸ºvalueã€‚</p>

<p>åœ¨PyTorchä¸­ï¼ŒEmbeddingä¸å…¶ä»–å±‚çš„ç”¨æ³•ç›¸åŒï¼Œå…ˆå®ä¾‹åŒ–ä¸€ä¸ªå›ºå®šç»´åº¦çš„Embeddingï¼Œè¿™æ—¶å€™å…¶ä¸­çš„è¯å‘é‡éƒ½æ˜¯éšæœºåˆå§‹åŒ–çš„ï¼Œå¯ä»¥å¯¹å®ä¾‹ä¼ å…¥indexå‚æ•°<strong>ï¼ˆä¸éœ€è¦ont-hotç›´æ¥indexï¼‰</strong>æ¥å–å‡ºå¯¹åº”ä½ç½®ï¼ˆè¯æ±‡ï¼‰çš„è¯å‘é‡ã€‚ä¹‹åå†å»ºç«‹ç¥ç»ç½‘ç»œï¼Œ<strong>é€šè¿‡learningçš„æ–¹æ³•æ¥ä¸æ–­æ›´æ–°Embeddingä¸­çš„å‚æ•°</strong>ï¼Œå¸Œæœ›è®©è¿™äº›å‚æ•°èƒ½å¤Ÿè•´å«å•è¯çš„è¯­æ„ä¿¡æ¯ã€‚</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">word_to_ix</span> <span class="o">=</span> <span class="p">{</span><span class="s">"hello"</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span> <span class="s">"world"</span><span class="p">:</span><span class="mi">1</span><span class="p">}</span>
<span class="n">embeds</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">lookup_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">word_to_ix</span><span class="p">[</span><span class="s">"hello"</span><span class="p">]],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="nb">long</span><span class="p">)</span>
<span class="n">hello_embed</span> <span class="o">=</span> <span class="n">embeds</span><span class="p">(</span><span class="n">lookup_tensor</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">hello_embed</span><span class="p">)</span>
</code></pre></div></div>

<p>è¦æ³¨æ„ï¼š</p>
<ol>
  <li>Embeddingè¦å®ä¾‹åŒ–ï¼Œå®šä¹‰æ—¶å‚æ•°ä¸º(vocab_size, embedding_dim)ã€‚</li>
  <li>ä¹‹ååœ¨å®ä¾‹ä¸­ç´¢å¼•æ—¶æ³¨æ„<strong>ç´¢å¼•å¿…é¡»æ˜¯LongTensorç±»å‹</strong>ã€‚</li>
  <li>å¾—åˆ°çš„hello_embedæ˜¯æ¯è¡Œä¸€ä¸ªè¯å‘é‡ã€‚</li>
</ol>

<h1 id="n-gram-language-model">N-Gram Language Model</h1>

<p>ä¸Šé¢è¯´åˆ°è¯å‘é‡æ˜¯è®­ç»ƒå¾—æ¥çš„ï¼Œæ—¢ç„¶æ˜¯è®­ç»ƒï¼Œå°±è¦æœ‰ç›®æ ‡è¦æœ‰æŸå¤±ã€‚ç”¨Language Modelè®­ç»ƒè¯å‘é‡æ˜¯ä¸€ç§æ–¹æ³•ï¼Œå³ä»ä¸Šä¸‹æ–‡çš„è§’åº¦å»å®šä¹‰è¯­ä¹‰ã€‚è¿™ä¸€ç‚¹ä¸åˆ†å¸ƒå‡è¯´æ˜¯ç¬¦åˆçš„ã€‚</p>

<h2 id="é¢„å¤„ç†">é¢„å¤„ç†</h2>

<p>å°†è¯­æ–™åˆ‡åˆ†ä¸ºN-Gramçš„å½¢å¼ï¼Œè¿™é‡Œå…·ä½“ä½¿ç”¨tri-gramã€‚å¹¶æ„å»ºè¯å…¸ï¼Œå¹¶åˆ†é…ç´¢å¼•ã€‚</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">CONTEXT_SIZE</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">EMBEDDING_DIM</span> <span class="o">=</span> <span class="mi">10</span>
<span class="c1">#è¯­æ–™ä½¿ç”¨Shakespeareçš„Sonnet2ï¼Œè¿™é‡Œæ„æ€ä¸€ä¸‹å°±è¡Œ
</span><span class="n">test_sentence</span> <span class="o">=</span> <span class="s">"""When forty winters shall besiege thy brow,
And dig deep trenches in thy beauty's field..."""</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
<span class="n">trigrams</span> <span class="o">=</span> <span class="p">[([</span><span class="n">test_sentence</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">test_sentence</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]],</span> <span class="n">test_sentence</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">2</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">test_sentence</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)]</span>
<span class="n">vocab</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">test_sentence</span><span class="p">)</span>
<span class="n">word_to_ix</span> <span class="o">=</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vocab</span><span class="p">)}</span>
</code></pre></div></div>

<h2 id="å®šä¹‰ç½‘ç»œ">å®šä¹‰ç½‘ç»œ</h2>

<p>æ³¨æ„Embeddingå±‚çš„å®šä¹‰æ–¹å¼å’Œå…¶ä»–å±‚ä¸€æ ·ã€‚</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">NGramLanguageModeler</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
	<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">context_size</span><span class="p">):</span>
		<span class="nb">super</span><span class="p">(</span><span class="n">NGramLanguageModeler</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">)</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">linear1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">embedding_size</span> <span class="o">*</span> <span class="n">context_size</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">linear2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>

	<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
		<span class="n">embeds</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddings</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
		<span class="n">out</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear1</span><span class="p">(</span><span class="n">embeds</span><span class="p">))</span>
		<span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
		<span class="n">log_probs</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
		<span class="k">return</span> <span class="n">log_probs</span>
</code></pre></div></div>

<p>æ³¨æ„çš„åœ°æ–¹ï¼š</p>
<ol>
  <li>ç¬¬ä¸€å±‚FCå±‚ï¼Œè¾“å…¥ç»´åº¦ä¸º<strong>embedding_size * context_size</strong>ï¼Œå› ä¸ºæ¯æ¬¡æ ¹æ®context_sizeä¸ªè¯æ±‡ä½œä¸ºè¾“å…¥ï¼Œæ¥é¢„æµ‹ä¸‹ä¸€ä¸ªã€‚</li>
  <li>ç¬¬äºŒå±‚FCå±‚ï¼Œè¾“å‡ºç»´åº¦ä¸º<strong>vocab_size</strong>ï¼Œå› ä¸ºè¯­è¨€æ¨¡å‹å…¶å®ä¹Ÿæ˜¯ä¸ªåˆ†ç±»é—®é¢˜ï¼Œç±»åˆ«æ•°ä¸ºè¯è¡¨çš„å¤§å°ï¼Œæœ€åsoftmaxå±‚è¾“å‡ºæ¯ä¸€ä¸ªç±»åˆ«çš„æ¦‚ç‡ã€‚</li>
  <li>æ³¨æ„ç½‘ç»œçš„è¾“å…¥æ˜¯å¥å­ä¸­å•è¯çš„<strong>ç´¢å¼•</strong>ï¼Œå› ä¸ºåœ¨forwardä¸­ç¬¬ä¸€å±‚embeddingä¸­ä¼šæŒ‰è¿™äº›ç´¢å¼•æ¥å–å‡ºè¯å‘é‡ã€‚</li>
  <li>Embeddingæ˜¯nnä¸­çš„å‡½æ•°ï¼Œembedingsæ˜¯è¯åµŒå…¥å±‚åœ¨ç±»ä¸­çš„å®ä¾‹ï¼Œembedsæ˜¯å°†è¾“å‡ºå‘é‡åŒ–åå¾—åˆ°çš„å‘é‡ã€‚</li>
  <li><code class="highlighter-rouge">embeddings(inputs)</code>åœ¨å–è¯å‘é‡æ—¶ï¼Œç´¢å¼•å¯ä»¥æ˜¯ä¸€ä¸ªtensorï¼Œä¸€è¡Œæˆ–æ˜¯ä¸€åˆ—éƒ½å¯ä»¥ã€‚å–å‡ºæ¥ä»¥åçš„è¯å‘é‡éƒ½ä»¥ä¸€è¡Œä¸€ä¸ªè¯çš„å½¢å¼è¡¨ç¤ºã€‚è¿™é‡Œcontextä¸º2æ‰€ä»¥å–å‡ºä¸¤ä¸ªè¯å‘é‡ï¼Œåé¢çš„viewæ–¹æ³•æŠŠè¿™ä¸¤ä¸ªè¯å‘é‡æ‹¼æ¥äº†èµ·æ¥ã€‚
    <h2 id="è®­ç»ƒ">è®­ç»ƒ</h2>
  </li>
</ol>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">loss_function</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">NGramLanguageModeler</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">),</span> <span class="n">EMBEDDING_DIM</span><span class="p">,</span> <span class="n">CONTEXT_SIZE</span><span class="p">)</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
	<span class="n">total_loss</span> <span class="o">=</span> <span class="mi">0</span>
	<span class="k">for</span> <span class="n">context</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">trigrams</span><span class="p">:</span>
		<span class="n">context_indx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">word_to_ix</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">context</span><span class="p">],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="nb">long</span><span class="p">)</span>
		<span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
		<span class="n">log_probs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">context_indx</span><span class="p">)</span>
		<span class="n">loss</span> <span class="o">=</span> <span class="n">loss_function</span><span class="p">(</span><span class="n">log_probs</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">word_to_ix</span><span class="p">[</span><span class="n">target</span><span class="p">]],</span> <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="nb">long</span><span class="p">))</span>
		<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
		<span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
		<span class="n">total_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
	<span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">total_loss</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
</code></pre></div></div>

<ol>
  <li>å¼€ä¸€ä¸ªlistæ¥è®°å½•lossï¼Œä¹‹å‰ä¸€èˆ¬åœ¨epochä¸­å®æ—¶è¾“å‡ºï¼Œè¿™é‡Œæ˜¯è®°å½•lossï¼Œåœ¨è®­ç»ƒç»“æŸåä¸€æ¬¡æ€§è¾“å‡ºã€‚</li>
  <li>è¯¥ç½‘ç»œè¾“å…¥æ˜¯ç´¢å¼•ï¼Œå¿…é¡»ä¸ºLongTensorç±»å‹ã€‚æœ‰ä¸¤ç§æ–¹æ³•ï¼Œ<code class="highlighter-rouge">LongTensor()</code>ç›´æ¥åˆå§‹åŒ–æˆ–è€…<code class="highlighter-rouge">tensor(data, dtype = torch.long)</code>ã€‚</li>
  <li>loss_functioné‡Œé¢çš„ä¸¤ä¸ªå‚æ•°ï¼Œç¬¬ä¸€ä¸ªä¸ºsoftmaxçš„è¾“å‡ºï¼Œæ˜¯ä¸€ä¸ªlistï¼Œæ¯ä¸ªå…ƒç´ ä¸ºå¯¹åº”çš„wordå‡ºç°çš„æ¦‚ç‡çš„logï¼Œ<strong>ç¬¬äºŒä¸ªå‚æ•°å¯ä»¥æ˜¯ç´¢å¼•ï¼Œä¹Ÿå¯ä»¥æ˜¯ont-hotè¡¨ç¤º</strong>ï¼Œå³label=[3]å’Œ[0, 0, 0, 1, 0 â€¦]æ˜¯ç›¸åŒçš„ã€‚åœ¨è¿™é‡Œä½¿ç”¨çš„æ˜¯ç´¢å¼•å·ã€‚</li>
  <li>lossæ˜¯ä¸€ä¸ªå•å…ƒç´ tensorï¼Œå–å‡ºæ•°å­—çš„æ–¹æ³•ä¸º<code class="highlighter-rouge">tensor.item()</code>ã€‚</li>
</ol>
:ET