I"³y<h1 id="å…³äºtensor">å…³äºTensor</h1>

<h2 id="autograd">autograd</h2>

<p>0.4ä»¥åå·²ç»å’ŒVariableåˆå¹¶ï¼Œæœ‰äº†è‡ªåŠ¨å¾®åˆ†åŠŸèƒ½ã€‚å½“<code class="highlighter-rouge">requires_grad</code>å±æ€§ä¸ºçœŸæ—¶ï¼Œè®°å½•è¿™ä¸ªtensorçš„gradå€¼ï¼Œæ¨¡å‹å‰å‘ç½‘ç»œä¸­çš„å‚æ•°çš„è¿™ä¸ªå±æ€§éƒ½ä¸ºçœŸã€‚gradä¸tensoråŒå½¢ï¼Œè®°å½•ç€æ¯ä¸€ä¸ªä½ç½®å¯¹åº”çš„å¯¼æ•°ã€‚æ¯æ¬¡æ‰§è¡Œf(x).backward()éƒ½ä¼š<strong><em>ç´¯åŠ grad(accumulated)</em></strong>ï¼Œè¿™æ˜¯æ¯ä¸ªoptimizer.step()åzero_grad()çš„åŸå› ã€‚</p>

<p>åå‘ä¼ æ’­å®é™…ä¸Šå°±æ˜¯ï¼š</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
  <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">sub_</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span> <span class="o">*</span> <span class="n">learning_rate</span><span class="p">)</span> <span class="c1"># inplaceè¿ç®—
</span></code></pre></div></div>

<p>è¿™é‡Œå–pçš„æ–¹å¼æ˜¯æŒ‰çŸ©é˜µï¼Œä¾‹å¦‚Linearä¸­ä¼šè¿”å›[out, in]å’Œ[out]å½¢çŠ¶çš„ä¸¤ä¸ªtensorã€‚</p>

<ul>
  <li>grad_fn: å¶å­ç»“ç‚¹çš„grad_fnæ˜¯Noneï¼Œå› ä¸ºè‡ªå·±åˆ›å»ºäº†å¹¶æ²¡æœ‰ç»è¿‡è¿ç®—ï¼Œéå¶å­ç»“ç‚¹ä¸ºTrueï¼Œè®°å½•ç€è¿™ä¸ªç»“ç‚¹æ˜¯ç”±ä»€ä¹ˆè¿ç®—å¾—æ¥çš„ã€‚</li>
  <li>is_leaf: <strong><em>ç”¨æˆ·è‡ªå·±åˆ›å»ºçš„èŠ‚ç‚¹å±äºå¶å­ç»“ç‚¹</em></strong>ã€‚</li>
  <li>requires_grad: å¶å­å’Œæ ¹ç»“ç‚¹éƒ½è¦æ˜¯Trueï¼Œæ˜¯Trueæ‰ä¼šè®°å½•ä»–ä»¬çš„è®¡ç®—å›¾ã€‚ä¸¤ä¸ªTrueçš„tensorè®¡ç®—å¾—åˆ°çš„tensorçš„requires_gradå±æ€§ä¹Ÿæ˜¯trueã€‚</li>
</ul>

<p><strong><em>æ‰€æœ‰å¯¹äºtensorçš„æ“ä½œéƒ½ä¼šè¢«è®°å½•ï¼ŒåŒ…æ‹¬torch.catç­‰æ“ä½œï¼Œå¯¼è‡´å†…å­˜çˆ†ç‚¸ã€‚</em></strong></p>

<h2 id="tensorgrad_fn">tensor.grad_fn</h2>

<p>è¾“å‡ºè¯¥tensoræ˜¯ç”±ä»€ä¹ˆæ“ä½œå¾—æ¥çš„ã€‚ç”¨æˆ·æ‰€åˆ›å»ºçš„leaf nodeçš„grad_fnæ˜¯Noneã€‚</p>

<h2 id="grad_fnnext_functions">grad_fn.next_functions</h2>

<p>ä¼šè¾“å‡ºgrad_fnçš„è¾“å…¥grad_fnï¼Œå³çœ‹ä¸Šä¸€æ­¥è®¡ç®—æ˜¯æ€ä¹ˆæ¥çš„ã€‚å¸¸ç”¨æ–¹æ³•æ˜¯next_functions[0] [0]ã€‚ä¸¤ä¸ªè¿ç®—å¾—åˆ°çš„ç»“æœè¦ç”¨next_functions[1] [0]ã€‚</p>

<p>è€Œé‚£äº›éœ€è¦æ±‚å¯¼çš„variableï¼Œä¾‹å¦‚å‚æ•°ç­‰ç­‰ï¼Œå…¶grad_fnå€¼æ˜¯AccumulateGradï¼Œä¹Ÿæ„å‘³ç€å®ƒä»¬çš„æ¢¯åº¦æ˜¯<strong>ç´¯åŠ çš„</strong>ã€‚</p>

<h2 id="tensorbackwardgradient-retain_graph">tensor.backward(gradient, retain_graph)</h2>

<p>loss.backward()è®¡ç®—<strong>æ¯ä¸ªå‚æ•°çš„æ¢¯åº¦</strong>ï¼Œå¹¶ä¿å­˜åœ¨(model.)parameter.gradä¸­ã€‚<strong><em>ä½†æ˜¯æ²¡æœ‰è¿›è¡Œparameterçš„æ›´æ–°ï¼Œåªæ˜¯è®¡ç®—å‡ºgradè€Œå·²ã€‚</em></strong>å› æ­¤ä¸¥æ ¼ä¸Šæ¥è®²ä¹Ÿä¸èƒ½å«åšåå‘ä¼ æ’­ï¼Œåªæ˜¯è®¡ç®—å¯¼æ•°å¹¶æ²¡æœ‰æ›´æ–°ç½‘ç»œå‚æ•°ï¼Œ<strong><em>optimizer.step()</em></strong>æ˜¯çœŸæ­£çš„æ›´æ–°parameterã€‚</p>

<p>æ¢¯åº¦ç´¯ç§¯æ„å‘³ç€ï¼Œåœ¨è°ƒç”¨optimizer.step()å®ç°æ¢¯åº¦ä¸‹é™ä¹‹å‰ï¼Œæˆ‘ä»¬ä¼šæ±‚å–parameter.gradå¼ é‡ä¸­çš„å‡ ä¸ªåå‘æ“ä½œçš„æ¢¯åº¦å’Œã€‚</p>

<p>è‹¥ä¸€ç›´è¿›è¡Œbackward()è€Œæ²¡æœ‰zero_grad()ï¼Œæ¢¯åº¦ä¼šä¸€ç›´ç´¯ç§¯ï¼Œæ‰€è°“æ¢¯åº¦ç´¯ç§¯è®­ç»ƒå°±æ˜¯ç»è¿‡Næ­¥backward()ä¹‹åå†è¿›è¡Œä¸€æ¬¡optimizer.step()ã€‚</p>

<p>ç¬¬ä¸€ä¸ªå‚æ•°åœ¨tensorä¸ºçŸ¢é‡æ—¶ä½¿ç”¨ï¼Œä¸€èˆ¬æƒ…å†µtensoræ˜¯lossæ˜¯ä¸ª0-dimensional tensorå¯ä»¥ç›´æ¥åå‘ã€‚</p>

<h3 id="grad_variable">grad_variable</h3>

<p>å¦‚æœæ­¤å¤„tensoræ˜¯ä¸€ä¸ªscalarï¼Œé‚£ä¹ˆä¸éœ€è¦grad_variablesè¿™ä¸ªå‚æ•°ã€‚å¦‚æœæ˜¯ä¸€ä¸ªçŸ©é˜µï¼Œåˆ™è¯¥å‚æ•°åº”è¯¥æ˜¯ä¸€ä¸ªå½¢çŠ¶ç›¸åŒçš„tensorï¼Œæœ¬è´¨åŸå› æ˜¯<strong><em>tensoræ— æ³•å¯¹tensoræ±‚å¯¼</em></strong>ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¼š<strong><em>å…ˆè¿›è¡Œtensorä¸grad_variablesçš„çŸ©é˜µç›¸ä¹˜å¾—åˆ°ä¸€ä¸ªscalar</em></strong>ï¼Œä¹‹åå†åˆ©ç”¨è¿™ä¸ªscalarå¯¹å½±å“åˆ°tensorçš„æ¯ä¸ªå‚æ•°æ±‚å¯¼ã€‚æ‰€ä»¥è¿™ä¸ªgrad_variableså˜é‡å¯ä»¥è®¤ä¸ºæ˜¯å„ä¸ªå…ƒç´ çš„weightã€‚</p>

<h3 id="retain_graph">retain_graph</h3>

<p>ä¸ºäº†å†…å­˜çš„åˆ©ç”¨ï¼Œæ¯æ¬¡backward()åï¼Œè®¡ç®—å›¾ä¼šè¢«é‡Šæ”¾ã€‚è€Œå¦‚æœæˆ‘è¿˜éœ€è¦è¿™ä¸ªè®¡ç®—å›¾ï¼Œå°±éœ€è¦ç½®è¿™ä¸ªå‚æ•°ä¸º1ã€‚</p>

<p>è¿™ç§æƒ…å†µåœ¨ä¸€ä¸ªç½‘ç»œæœ‰å¤šä¸ªè¾“å‡ºçš„æƒ…å†µä¸‹éå¸¸é‡è¦ã€‚</p>

<h2 id="tensordata">tensor.data</h2>

<p>å¦‚æœæˆ‘ä»¬æƒ³è¦å¯¹tensorçš„å€¼è¿›è¡Œè¿ç®—ï¼Œä½†æ˜¯ä¸å¸Œæœ›å…¶è¢«autogradå¼•å…¥è®¡ç®—å›¾ï¼Œå°±å¯ä»¥ç”¨.dataæ–¹æ³•ã€‚</p>

<h2>!=</h2>

<p>ä¸¤ä¸ª<code class="highlighter-rouge">åŒå½¢ï¼ˆä¸åŒä¼šbroadcastä¸ºåŒå½¢ï¼Œå› æ­¤å…¶ä¸­ä¸€ä¸ªå¯ä»¥ä¸ºä¸€ä¸ªæ•°çš„tensorï¼‰</code>tensorå¯ä»¥ç”¨<code class="highlighter-rouge">!=</code>è¿›è¡Œè¿ç®—ï¼Œè¿”å›ä¸€ä¸ªåŒé•¿åº¦çš„tensorï¼Œç›¸ç­‰çš„ä½ç½®ä¸º0ï¼Œä¸ç­‰çš„ä½ç½®ä¸º1ã€‚è‹¥æ˜¯ä¸¤ä¸ªlistï¼Œè¿”å›å€¼ä¸ºboolç±»å‹ï¼Œ1æˆ–0ã€‚</p>

<h2 id="å¡«å……å€¼">å¡«å……å€¼</h2>

<p>tesnor.fill_(value)ï¼Œæ”¹å˜tensorä¸­çš„å€¼ï¼Œå…¶ä¸­valueå¿…é¡»ä¸ºæ•°å­—ã€‚</p>

<h2 id="ä¹˜æ³•">ä¹˜æ³•</h2>

<table>
  <thead>
    <tr>
      <th>å‡½æ•°</th>
      <th>å«ä¹‰</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>tensor.bmm</td>
      <td>batchä¹˜æ³•ï¼Œå¿…é¡»æ˜¯<code class="highlighter-rouge">b*n*m</code>ä¸<code class="highlighter-rouge">b*m*p</code>çš„ä¸¤ä¸ªé˜µåšè¿ç®—ï¼Œå¾—åˆ°<code class="highlighter-rouge">b*n*p</code>çš„ç»“æœ</td>
    </tr>
    <tr>
      <td>tensor.mul</td>
      <td>element-wise matrix multiply</td>
    </tr>
    <tr>
      <td>tensor.mm</td>
      <td>äºŒç»´çŸ©é˜µç›¸ä¹˜</td>
    </tr>
  </tbody>
</table>

<h2 id="è½¬æ¢">è½¬æ¢</h2>

<table>
  <thead>
    <tr>
      <th>å‡½æ•°</th>
      <th>å«ä¹‰</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>tensor.tolist()</td>
      <td>é«˜ç»´tensorè½¬ä¸ºlistç±»å‹</td>
    </tr>
    <tr>
      <td>tensor.item()</td>
      <td>åªåŒ…å«ä¸€ä¸ªå…ƒç´ çš„tensorè½¬æ¢ä¸ºnumberï¼Œ<code class="highlighter-rouge">loss.item()</code></td>
    </tr>
  </tbody>
</table>

<h2 id="å˜å½¢">å˜å½¢</h2>

<table>
  <thead>
    <tr>
      <th>å‡½æ•°</th>
      <th>å«ä¹‰</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>tensor.view(1, -1)</td>
      <td>å¦‚ä¸€ä¸ªä¸€ç»´å‘é‡è¿›è¡Œè¿™ä¸ªæ“ä½œï¼Œä¸åªæ˜¯å½¢çŠ¶ï¼Œä¸”å‡çº§åˆ°äºŒç»´</td>
    </tr>
    <tr>
      <td>tensor.transpose(1, 2)</td>
      <td>è½¬ç½®ï¼Œäº¤æ¢ä¸¤ä¸ªç»´åº¦</td>
    </tr>
    <tr>
      <td>tensor.permute(0, 2, 1)</td>
      <td>äº¤æ¢ç»´åº¦ï¼Œå¯ä»¥å¯¹æ‰€æœ‰ç»´åº¦åŒæ—¶è¿›è¡Œ</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>.view()æ–¹æ³•åªèƒ½ä½œç”¨äº<strong>è¿ç»­å†…å­˜</strong>ï¼Œå¦‚æœåœ¨å–batchæ—¶åšäº†åˆ‡ç‰‡ç­‰æ“ä½œï¼Œé‚£ä¹ˆæ•°æ®å°±ä¸æ˜¯è¿ç»­å†…å­˜ä¸Šçš„ï¼Œéœ€è¦åœ¨viewå‰è¿›è¡Œ<code class="highlighter-rouge">tensor.contiguous().view()</code>è¿™æ ·çš„æ“ä½œä½¿å…¶è¿ç»­ã€‚</li>
</ul>

<h2 id="ç”Ÿæˆ">ç”Ÿæˆ</h2>

<p>ä¼ å…¥listæ—¶ï¼Œä½œè½¬æ¢ä½œç”¨ã€‚ä¼ å…¥çš„ä¸æ˜¯listæ—¶ï¼Œè®¤ä¸ºä¼ å…¥çš„æ˜¯å½¢çŠ¶ï¼Œéšæœºç”Ÿæˆã€‚</p>

<table>
  <thead>
    <tr>
      <th>å‡½æ•°</th>
      <th>å«ä¹‰</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>LongTensor(5)</td>
      <td>è¿™ç§æƒ…å†µè®¤ä¸º5æ˜¯ä¸€ä¸ªç»´åº¦ï¼Œä¼šç”Ÿæˆä¸€ä¸ªå½¢çŠ¶ä¸º5çš„ä¸€ç»´å‘é‡</td>
    </tr>
    <tr>
      <td>LongTensor(5,1)</td>
      <td>ç”Ÿæˆä¸€ä¸ªå½¢çŠ¶ä¸º5*1çš„tensor</td>
    </tr>
    <tr>
      <td>LongTensor([5,1])</td>
      <td>ä¼ å…¥ä¸€ä¸ªlistï¼Œè¿™æ˜¯ä½œè½¬æ¢ä½œç”¨ï¼Œè¿”å›ä¸€ä¸ªtensor([5,1])</td>
    </tr>
  </tbody>
</table>

<h2 id="maxæ¯”è¾ƒ">maxæ¯”è¾ƒ</h2>

<ol>
  <li>max(tensor)è¿”å›tensorä¸­æœ€å¤§çš„ä¸€ä¸ªæ•°ï¼Œä»¥tensorçš„å½¢å¼ã€‚</li>
  <li>max(tensor, dim)<strong>è¿”å›(Tensor, LongTensor)</strong>ï¼Œåˆ†åˆ«æ˜¯æŒ‡å®šç»´åº¦ä¸Šæœ€å¤§çš„æ•°ï¼Œå’Œå…¶inxã€‚</li>
  <li>max(tensor1, tensor2)ä¸¤ä¸ªåŒå½¢tensorï¼Œè¿”å›ä¸€ä¸ªtensorï¼Œæ¯ä¸ªä½ç½®å…ƒç´ ä¸º1å’Œ2ä¸­æœ€å¤§çš„é‚£ä¸ªã€‚</li>
</ol>

<h2 id="size">size()</h2>

<p>tensor.size()è¿”å›ä¸€ä¸ªtorch.Sizeç±»å‹ï¼Œè€Œtensor.size(0)è¿”å›ä¸€ä¸ªintã€‚</p>

<h2 id="éšæœº">éšæœº</h2>

<table>
  <thead>
    <tr>
      <th>å‡½æ•°</th>
      <th>å«ä¹‰</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>torch.randn_like(tensor)</td>
      <td>ç”Ÿæˆå’ŒtensoråŒå½¢çŠ¶çš„Normal Distributionï¼Œå‡å€¼0ï¼Œæ–¹å·®1</td>
    </tr>
  </tbody>
</table>

<h2 id="torchsorttensor-descendingtrue">torch.sort(tensor, descending=True)</h2>

<p>å¯¹tensorçš„<strong>æ¯è¡Œ</strong>è¿›è¡Œæ’åºï¼Œé»˜è®¤æ˜¯å‡åºascendingã€‚è¿”å›<strong>(sorted_tensor, sorted_indices)</strong>ã€‚</p>

<h2 id="tensortensor">tensor[tensor]</h2>

<p>åˆ©ç”¨ä¸€ä¸ªindicesçš„tensorå¯¹å¦ä¸€ä¸ªtensorè¿›è¡Œè¡Œä¸Šçš„é‡æ–°æ’åºã€‚</p>

<h2 id="tensorindex_selectdim-index">tensor.index_select(dim, index)</h2>

<p>åœ¨æŒ‡å®šdimä¸Šï¼Œé€‰æ‹©index(LongTensor)æŒ‡å®šçš„é¡ºåºçš„è¡Œ/åˆ—ã€‚å¯ä»¥è®¤ä¸ºæ˜¯reorderçš„ä¸€ç§æ–¹å¼ã€‚</p>

<h2 id="tensorcatlist-dim">tensor.cat(list, dim)</h2>

<p>ç¬¬ä¸€ä¸ªå‚æ•°<strong><em>å¿…é¡»æ˜¯tensorç»„æˆçš„list/tuple</em></strong>ï¼Œ<strong><em>dimä¸­0æ˜¯è¡Œ1æ˜¯åˆ—ï¼Œä¸€è¡Œä¸€è¡Œæ‹¼èµ·æ¥æˆ–æ˜¯ä¸€åˆ—ä¸€åˆ—æ‹¼èµ·æ¥ã€‚</em></strong></p>

<p>ä¾‹å¦‚dim=1æ—¶ï¼Œå¿…é¡»åœ¨0ç»´ï¼ˆè¡Œï¼‰ä¸Šè¦matchï¼Œæ­¤æ—¶ä¸€åˆ—ä¸€åˆ—æ‹¼æ¥ï¼Œè¡Œæ•°ä¸å˜ï¼Œåˆ—æ•°å¢åŠ ï¼Œå³ä¸€è¡Œä¸­çš„å…ƒç´ æ•°ç›®å¢åŠ ã€‚</p>

<h2 id="åˆ‡ç‰‡">åˆ‡ç‰‡</h2>

<p>tensor[:, -1:]å–æœ€åä¸€åˆ—ï¼Œå› ä¸ºç¬¬äºŒä¸ªå†’å·å­˜åœ¨æ‰€ä»¥ä¿æŒäºŒç»´ã€‚
tensor[:, -1]å–å‡ºæœ€åä¸€åˆ—ï¼Œä½œä¸ºä¸€ä¸ªä¸€ç»´tensorã€‚åˆ‡ç‰‡ä¸­ä¸€æ—¦å‡ºç°æ•°å­—ï¼Œå°±æ„å‘³ç€ç»´åº¦çš„æ¶ˆå¤±ã€‚</p>

<h2 id="tensorrepeatsize">tensor.repeat(*size)</h2>

<p><code class="highlighter-rouge">tensor.repeat(4,2)</code>æŠŠè¿™ä¸ªtensoråœ¨è¡Œä¸Šå¤åˆ¶å››æ¬¡ï¼Œåœ¨åˆ—ä¸Šå¤åˆ¶ä¸¤æ¬¡ã€‚</p>

<h2 id="torchtriutensor-dim0">torch.triu(tensor, dim=0)</h2>

<p>å°†tensorè¿”å›ä¸€ä¸ªä¸Šä¸‰è§’çŸ©é˜µï¼Œé»˜è®¤ä¿ç•™å¯¹è§’çº¿å…ƒç´ ã€‚å¦‚æœdim=1é‚£ä¹ˆå°±å°‘ä¿ç•™ä¸€æ–œçº¿ï¼Œdim=-1å¾€ä¸‹å¤šç•™ä¸€æ–œçº¿ã€‚</p>

<h2 id="tensorany">tensor.any()</h2>

<p>tensorå¿…é¡»æ˜¯boolç±»å‹ï¼Œåˆ¤æ–­æ˜¯ä¸æ˜¯æœ‰ä»»ä½•éé›¶å€¼ã€‚</p>

<h2 id="tensormased_fill_">tensor.mased_fill_()</h2>

<p><code class="highlighter-rouge">masked_fill_</code>å‡½æ•°çš„æ„æ€æ˜¯ï¼Œ<strong><em>maskæ˜¯ä¸€ä¸ªäºŒå€¼çš„ByteTensorï¼Œå…¶å€¼ä¸º1çš„ä½ç½®éœ€è¦ç”¨value(float)æ¥ä»£æ›¿</em></strong>ã€‚æ³¨æ„valueå¯ä»¥ç›´æ¥ç”¨æ•°å­—æˆ–è€…ç”¨0ç»´tensor(torch.tensor(5))ã€‚</p>

<h2 id="torchrandintlow--0-high-size-device">torch.randint(low = 0, high, size, device)</h2>

<p>sizeæ˜¯ä¸€ä¸ªtupleï¼Œæ¯”å¦‚torch.randnint(5, (1,1))ã€‚é»˜è®¤lowæ˜¯0ï¼Œç¬¬ä¸€ä¸ªå‚æ•°å°±æ˜¯highã€‚</p>

<p>âš ï¸<strong>low(inclusive) high(exclusive)</strong>.</p>

<h1 id="torchmultinomial">torch.multinomial</h1>

<p><code class="highlighter-rouge">torch.multinomial(input, num_samples, replacement=Flase)</code>ï¼Œç¬¬ä¸€ä¸ªå‚æ•°inputå¯ä»¥è®¤ä¸ºæ˜¯weight/probabilityï¼Œæ¯è¡Œæ˜¯ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼ˆå’Œä¸ä¸€å®šæ˜¯1ï¼‰ï¼ŒæŒ‰ç…§è¿™ä¸ªåˆ†å¸ƒä¸ºæƒé‡å–å€¼ï¼Œè¿”å›çš„æ˜¯num_samplesä¸ªè¢«å–åˆ°çš„ä¸‹æ ‡ï¼Œå³å‚æ•°inputçš„å½¢çŠ¶ä¸º<code class="highlighter-rouge">(num_rows, weight)</code>ï¼Œè¿”å›å€¼å½¢çŠ¶ä¸º<code class="highlighter-rouge">(num_rows, num_samples)</code>ã€‚replacementè¡¨ç¤ºæœ‰æ— æ”¾å›ã€‚<strong>ä¸æ˜¯ç»å¯¹çš„æŒ‰ç…§weightå–çš„ï¼Œæ¦‚ç‡æ„ä¹‰ä¸Šçš„æƒé‡ã€‚</strong></p>

<p>torch.multinomial(torch.ones(n), 1)å¯ä»¥åšåˆ°åœ¨1åˆ°nä¸­éšæœºå–ä¸€ä¸ªæ•°ã€‚</p>

<p><strong>âš ï¸æ³¨æ„inputä¸­çš„æƒé‡å€¼ä¸èƒ½ä¸ºè´Ÿæ•°ï¼</strong></p>

<h1 id="dataset--dataloader">dataset &amp; dataloader</h1>

<p>Tensors that have the same size of the first dimension. ç¬¬ä¸€ä¸ªç»´åº¦ç›¸ç­‰ï¼Œæ„å‘³ç€æ ·æœ¬æ•°é‡ç›¸ç­‰ï¼Œnumber * æ ·æœ¬ï¼ˆå¯ä»¥æ˜¯ä¸€ç»´æ ·æœ¬ä¾‹å¦‚å…¨éƒ½æ˜¯indexï¼Œå¯ä»¥æ˜¯äºŒç»´å›¾ç‰‡ç­‰ç­‰ï¼‰ã€‚å–batchå°±æ˜¯æ²¿ç€ç¬¬ä¸€ä¸ªç»´åº¦å–çš„ã€‚</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch.utils.data</span> <span class="k">as</span> <span class="n">Data</span>
<span class="n">torch_dataset</span> <span class="o">=</span> <span class="n">Data</span><span class="o">.</span><span class="n">TensorDataset</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>	<span class="c1">#è¿™é‡Œçš„xå’Œyå¿…é¡»æ˜¯tensorç±»å‹ï¼Œç¬¬ä¸€ä¸ªç»´åº¦ä¸ºbatch
</span><span class="n">loader</span> <span class="o">=</span> <span class="n">Data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span>
	<span class="n">dataset</span> <span class="o">=</span> <span class="n">torch_dataset</span><span class="p">,</span>
	<span class="n">batch_size</span> <span class="o">=</span> <span class="n">BATCH_SIZE</span><span class="p">,</span>
	<span class="n">shuffle</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span>
	<span class="n">num_workers</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>
	<span class="n">drop_last</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span>		<span class="c1">#æŠ›å¼ƒæœ€åä¸€ä¸ªï¼Œå› ä¸ºé•¿åº¦ä¸ä¸€å®š
</span>	<span class="n">pin_memory</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span>		<span class="c1">#æŠŠæ•°æ®æ”¾åˆ°CUDA
</span>	<span class="p">)</span>
<span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_x</span><span class="p">,</span> <span class="n">batch_y</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">loader</span><span class="p">):</span>
	<span class="k">pass</span>
</code></pre></div></div>

<ul>
  <li>pin_memory = torch.cuda_is_available</li>
  <li>shuffle=split==â€™trainâ€™</li>
  <li>num_workers=cpu_count()</li>
  <li>pin_memory=torch.cuda.is_available()</li>
</ul>

<h1 id="nnnllloss">nn.NLLLoss()</h1>

<p>Negative Log Likelihood Lossï¼Œè¾“å…¥ä¸º(batch_size, vocab)ï¼Œæ ‡ç­¾ä¸º(batch_size)ï¼Œæ³¨æ„æ ‡ç­¾ä¸éœ€è¦æ˜¾ç¤ºçš„ä½¿ç”¨one-hot tensorsï¼Œåªéœ€è¦ä½¿ç”¨æ•´æ•°è¡¨ç¤ºä½ç½®å³å¯ã€‚</p>

<p>åŸç†ä¸ºåªå…³æ³¨<strong>æ­£ç¡®æ ‡ç­¾å¯¹åº”çš„logits</strong>ï¼Œ<strong>è‹¥ä½¿ç”¨one-hotæ ‡ç­¾ï¼Œ</strong>å¯¹è¾“å…¥ä¸­çš„å€¼å…¨éƒ¨åŠ è´Ÿå·ï¼Œæœ‰batchç»´åº¦çš„è¯æ±‚å‡å€¼ï¼Œç„¶åè®©å®ƒå˜å°å³è®©æ­£ç¡®æ ‡ç­¾ä½ç½®æ•°æ®å˜å¤§ã€‚</p>

<p><strong>âš ï¸æ³¨æ„NLLossæœ¬èº«å¹¶æ²¡æœ‰Logsoftmaxä¹‹ç±»çš„æ“ä½œï¼ç›´æ¥æŠŠè¾“å…¥çš„inputæŒ‰ç…§labelæ¥è®¡ç®—ï¼</strong></p>

<p>âš ï¸<strong>æˆ‘ä»¬æœŸå¾…ä½¿ç”¨NLLçš„å‰å‘ä¼ æ’­æ—¶ï¼Œç»™çš„ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯logsoftmaxä¹‹åçš„æ¦‚ç‡ï¼æ‰€ä»¥ä½¿ç”¨NLLLossä¹‹å‰ä¸€èˆ¬åœ¨ç½‘ç»œçš„æœ€åä¸€å±‚åŠ ä¸€ä¸ªLogSofmaxå±‚ã€‚</strong></p>

<p>Softmaxè®©logitsä½äº[0, 1]èŒƒå›´å†…ï¼Œè€ŒLogSofmaxå¯ä»¥è®©æ•°æ®ä½äº[-æ— ç©·, 0]å†…ã€‚</p>

<h1 id="nncrossentropyloss">nn.CrossEntropyLoss</h1>

<p>å³LogSoftmax+NLLLoss()çš„ç»„åˆã€‚</p>

<p>æ¥æ”¶ä¸¤ä¸ªå‚æ•°ï¼Œç¬¬ä¸€ä¸ªæ˜¯(N, class)å½¢çŠ¶çš„çŸ©é˜µï¼Œå…¶ä¸­Nè¡¨ç¤ºæ ·æœ¬æ•°ï¼Œclassè¡¨ç¤ºç±»åˆ«æ•°é‡ï¼Œå…¶ä¸­çš„æ•°å­—æœªç»è¿‡softmaxçš„ï¼Œç¬¬äºŒä¸ªå‚æ•°æ˜¯ä¸€ç»´çš„ï¼Œ(N)å½¢çŠ¶ï¼Œå¿…é¡»ä¸º<strong>LongTensorç±»å‹</strong>ï¼Œå…¶ä¸­çš„æ•°æ®ä¸ºindicesï¼Œä¸å¯one-hotã€‚</p>

<p>æ‰€ä»¥å¯¹äºå¸¸è§çš„æ•°æ®æ ¼å¼è€Œè¨€ï¼Œç»è¿‡å‰å‘ä¼ æ’­å¾—åˆ°çš„outæ ¼å¼ä¸º(batch, time-step, vocab_size)ï¼Œéœ€è¦<code class="highlighter-rouge">out.view(-1, vocab_size)</code>ï¼Œç›¸å½“äºæŠŠæ‰€æœ‰å¥å­è¿èµ·æ¥ï¼Œä¸€è¡Œä¸€ä¸ªlogitsã€‚è€ŒtargetåŸæœ¬ä¸º(batch, time-step)ï¼Œéœ€è¦<code class="highlighter-rouge">target.view(-1)</code>ã€‚</p>

<p>è¾“å‡ºä¸ºä¸€ä¸ª0ç»´tensorï¼Œè¦è®°å½•åˆ°listçš„è¯ä½¿ç”¨<strong>loss.item()</strong>ã€‚</p>

<h1 id="nnkldivloss">nn.KLDivLoss</h1>

<p>è®¡ç®—ä¸¤ä¸ªåˆ†å¸ƒä¹‹é—´çš„è·ç¦»ï¼Œå‚æ•°å’Œäº¤å‰ç†µä¸åŒã€‚ä½†æ˜¯ä¹Ÿè¿”å›0ç»´tensorã€‚</p>

<h1 id="nnembedding">nn.Embedding</h1>

<ol>
  <li>ç´¢å¼•æ˜¯ç”¨<strong>LongTensor</strong>çš„indices(indexçš„å¤æ•°)ï¼Œä¸ç”¨one-hotå‘é‡ã€‚è¿™ä¸€ç‚¹åœ¨å®šä¹‰æ•°æ®é›†çš„æ—¶å€™å°±è¦æ³¨æ„ã€‚</li>
  <li>å¦‚æœä¸æŒ‡å®šåˆå§‹åŒ–æ–¹å¼ï¼Œé»˜è®¤ä½¿ç”¨(0, 1)çš„normal distributionæ­£æ€åˆ†å¸ƒã€‚Embeddingå±‚æœ‰ä¸€ä¸ªweightå±æ€§ï¼Œæ˜¯Parameteråˆå§‹åŒ–çš„ï¼Œå¯ä»¥é€šè¿‡<code class="highlighter-rouge">embedding.weight.data.uniform_(-1, 1)</code>æ”¹ä¸ºå‡åŒ€åˆ†å¸ƒã€‚</li>
  <li>ä¸ç®¡è¾“å…¥çš„æ˜¯ä»€ä¹ˆæ ¼å¼çš„æ•°æ®ï¼Œéƒ½ä¼šåœ¨å…¶<code class="highlighter-rouge">n * m</code>çš„ç»´åº¦ååŠ ä¸ŠåµŒå…¥ç»´åº¦dï¼Œå˜ä¸º<code class="highlighter-rouge">n * m * d</code>ç±»å‹çš„è¾“å‡ºã€‚æ¯”å¦‚æˆ‘ç›´æ¥è¾“å…¥ä¸€ä¸ªä¸€ç»´çš„ä¸€ç™¾ä¸ªè¯çš„indicesï¼Œ<code class="highlighter-rouge">tensor([100])</code>ï¼Œç»è¿‡Embeddingå±‚åå¾—åˆ°çš„æ˜¯<code class="highlighter-rouge">tensor([100, dim])</code>æ¯ä¸€è¡Œä¸ºä¸€ä¸ªè¯å‘é‡ã€‚æ²¡æœ‰å›ºå®šè¦æ±‚ï¼Œè§†åé¢çš„è¿ç®—è€Œå®šï¼ŒåŠ ä¸Šbatchçš„ä¸‰ç»´ä¿¡æ¯ä¹Ÿå¯ä»¥ã€‚</li>
</ol>

<h1 id="nndropout">nn.Dropout</h1>

<p>ç›®çš„æ˜¯æ­£åˆ™åŒ–ï¼Œé˜²æ­¢ç¥ç»å…ƒä¹‹é—´äº’ç›¸ä¾èµ–ã€‚å°†ä¸€ä¸ªtensorä¸­çš„éƒ¨åˆ†å…ƒç´ æŒ‰ç…§æ¦‚ç‡pç½®æ¢ä¸º0ï¼Œå…¶ä»–éƒ¨åˆ†ä¹˜ä»¥ç¼©æ”¾å› å­ï¼Œä½¿å¾—æ€»çš„å¤§å°ä¿æŒç›¸å¯¹ç¨³å®šã€‚</p>

<h1 id="nnlstm">nn.LSTM</h1>

<h2 id="å®šä¹‰">å®šä¹‰</h2>

<p>é¦–å…ˆLSTMæ˜¯ä¸€ä¸ªé‡å¤ä½¿ç”¨çš„å•å…ƒï¼Œåªæœ‰ä¸€å¥—å‚æ•°ï¼Œæ‰€ä»¥å®šä¹‰æ—¶åªè¦è§„å®šå¥½è¾“å…¥æ•°æ®ç»´åº¦ã€éšå±‚æ•°æ®ç»´åº¦ã€å±‚æ•°å’Œæ–¹å‘å°±å¯ä»¥äº†ï¼Œtime-stepæ²¡é™åˆ¶ï¼Œé‡å¤ä½¿ç”¨è¿™ä¸ªå•å…ƒå°±å¯ä»¥ï¼Œä½“ç°åœ¨è¾“å…¥ä¸­ã€‚</p>

<p>ç½‘ç»œå®šä¹‰æ—¶ï¼Œå‚æ•°ä¸º<code class="highlighter-rouge">nn.LSTM(input_size, hidden_size, num_layers, batch_first, bidirectional)</code>ï¼Œæ³¨æ„å®šä¹‰ç½‘ç»œæ—¶æ²¡æœ‰time_stepè¿™ä¸ªå±æ€§ã€‚å…¶ä¸­<strong>batch_firstç½®ä½ä¼šå½±å“inputå’Œoutputçš„å½¢çŠ¶ï¼Œè€Œå¯¹h_nå’Œc_næ— å½±å“ï¼Œå®ƒä»¬çš„batchä¿¡æ¯è¿˜æ˜¯åœ¨ä¸­é—´ç»´åº¦ã€‚</strong></p>

<h2 id="ç½‘ç»œè¾“å…¥">ç½‘ç»œè¾“å…¥</h2>

<p>lstmæ¨¡å—æ¥æ”¶çš„å‚æ•°ä¸º<code class="highlighter-rouge">(inputs, (h_n, c_n))</code>ï¼Œå‰è€…ä¸ºå–‚å…¥ç½‘ç»œçš„æ•°æ®ï¼Œæ ¼å¼<strong>å¿…é¡»</strong>ä¸º<code class="highlighter-rouge">(batch_size, time_step, feature_length)</code>ã€‚åè€…ä¸ä¼ ä¹Ÿå¯ä»¥ï¼Œç¼ºçœçŠ¶æ€ä¸ºå…¨0ã€‚</p>

<p>æ³¨æ„è®­ç»ƒæ—¶è¾“å…¥çš„time-stepå’Œåé¢evaluateæ—¶å€™çš„time-stepä¸å¿…ä¸€è‡´ï¼Œåªè¦ä¿å­˜ä¸‹(h_n, c_n)çš„stateï¼Œå†ä½œä¸ºä¸‹ä¸€æ¬¡å¾ªç¯çš„è¾“å…¥å°±å¯ä»¥äº†ã€‚<code class="highlighter-rouge">output, state = model(input, state)</code>ã€‚</p>

<h2 id="ç½‘ç»œè¾“å‡º">ç½‘ç»œè¾“å‡º</h2>

<p>ç½‘ç»œè¿”å›ä¸¤ï¼ˆä¸‰ï¼‰ä¸ªå‚æ•°ï¼Œ<code class="highlighter-rouge">output, (h_n, c_n)</code>ã€‚ç¬¬äºŒä¸ªå‚æ•°è¿™ç§tupleçš„å½¢å¼å’Œè¾“å…¥ç¬¬äºŒå‚æ•°æ˜¯ä¸€è‡´çš„ï¼Œä»¥ä¾›å¾ªç¯ä¼ é€’éšçŠ¶æ€ã€‚</p>

<ol>
  <li>outputçš„å½¢çŠ¶ä¸º<code class="highlighter-rouge">(batch(first), time_step, hidden_size * num_direction)</code>ï¼Œä¿å­˜äº†<strong>æœ€åä¸€å±‚</strong>LSTMçš„<strong>æ¯ä¸€ä¸ªtime-step</strong>éšçŠ¶æ€å³è¾“å‡ºã€‚å¦‚æœæ˜¯åŒå‘çš„ï¼Œh_nçš„å½¢çŠ¶ä¸º(num_direction * hidden_size)ï¼Œæ˜¯å•çº¯çš„æ‹¼æ¥èµ·æ¥ï¼Œå–å…¶<code class="highlighter-rouge">[:hidden_size]</code>ä¸ªå‘é‡å¾—åˆ°æ­£å‘çš„è¿™ä¸€æ­¥çš„è¾“å‡ºã€‚</li>
  <li>h_nå½¢çŠ¶ä¸º<code class="highlighter-rouge">(num_layers * num_directions, batch, hidden_size)</code>ï¼Œä¿å­˜äº†<strong>æ¯ä¸€å±‚</strong>LSTMçš„<strong>æœ€åä¸€ä¸ªtime-stepï¼ˆåŒ…æ‹¬æ­£åå‘ï¼‰</strong>çš„éšçŠ¶æ€è¾“å‡ºã€‚å¦‚æœæ˜¯åŒå‘çš„ï¼Œ<strong>ç¬¬ä¸€ä¸ªç»´åº¦æŒ‰ï¼ˆç¬¬ä¸€å±‚æ­£å‘ï¼Œç¬¬ä¸€å±‚åå‘ï¼Œç¬¬äºŒå±‚æ­£å‘ï¼Œç¬¬äºŒå±‚åå‘â€¦ï¼‰</strong>é¡ºåºæ¥ä¿å­˜ã€‚</li>
  <li>c_nåŒh_nï¼Œä¿å­˜çš„æ˜¯cell_stateã€‚</li>
</ol>

<p>SGDåœ¨RNNä¸­çš„åº”ç”¨ï¼Ÿä¸€ä¸ªbatchè€Œè¨€æ€ä¹ˆsgd</p>

<h1 id="nnlinear">nn.Linear</h1>

<p>Linearæ¥å—çš„è¾“å…¥æ˜¯<code class="highlighter-rouge">(N, *, in_features)</code>å¹¶è¾“å‡º<code class="highlighter-rouge">(N, *, out_features)</code>ï¼Œä¹Ÿå°±æ˜¯è¯´å¯¹æœ€åä¸€ä¸ªç»´åº¦è¿›è¡Œçº¿æ€§å¤„ç†ï¼Œè¾“å‡ºç»´åº¦ä¸ä¹‹å‰æ ¼å¼ç›¸åŒã€‚</p>

<h1 id="evaluation">Evaluation</h1>

<p>æµ‹è¯•æ—¶ï¼Œä½¿ç”¨<code class="highlighter-rouge">with torch.no_grad()</code>ç¯å¢ƒæ¥å…³é—­è®¡ç®—å›¾åŠŸèƒ½ã€‚ä¸€äº›å¸¸ç”¨çš„evaluateæ–¹æ³•ï¼š</p>

<ol>
  <li><code class="highlighter-rouge">_, prediction = torch.max(output.data, 1)</code>è¿”å›outputä¸­æœ€å¤§çš„æ•°çš„ä¸‹æ ‡ï¼Œè®¤ä¸ºå°±æ˜¯å…¶ç±»åˆ«ã€‚</li>
  <li><code class="highlighter-rouge">correct += (prediction == labels).sum().item()</code>ï¼Œå…¶ä¸­<code class="highlighter-rouge">tensor == tensor</code>æŒ‰ç…§å¯¹åº”ä½ç½®æ˜¯å¦ç›¸åŒè¿”å›æ˜¯1æˆ–0ï¼Œsumæ±‚å’Œï¼Œitemè½¬æ¢ä¸ºpythonçš„numberã€‚</li>
</ol>

<h1 id="batch_training">batch_training</h1>

<p>yunjeyçš„è¿™ç§æ„é€ batch_trainingçš„æ–¹æ³•å’Œtrain_loaderçš„ä¸åŒï¼Ÿå¥½åƒä»–è¿™æ ·æ‰æ˜¯çœŸæ­£çš„batch</p>

<p>æµ‹è¯•æ—¶å€™æ‰€è°“çš„çª—å£å¤§å°ä¿¡æ¯åˆç”¨åœ¨å“ªäº†ï¼Ÿè¿˜æ˜¯è¯´è®­ç»ƒå°±è¿™ä¹ˆè®­ç»ƒï¼Œæˆ‘åé¢å°±ä¸é‚£ä¹ˆç”¨ã€‚è®¤ä¸ºå¯ä»¥æ•æ‰æ‰€æœ‰çš„ä¸Šä¸‹æ–‡ã€‚</p>

<h1 id="å˜é•¿rnnåºåˆ—">å˜é•¿rnnåºåˆ—</h1>

<h2 id="padding--sort">padding &amp; sort</h2>

<p>é¦–å…ˆpaddingåˆ°max_lengthï¼Œä¸”è®°å½•æ¯ä¸ªæ ·æœ¬çš„lengthï¼ŒæŒ‰é•¿åº¦é™åºæ’åˆ—ã€‚é¦–å…ˆinputæ˜¯ä¸ªlistã€‚</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">length</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="nb">input</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="s">'&lt;pad&gt;'</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">max_length</span> <span class="o">-</span> <span class="n">length</span><span class="p">))</span>

<span class="n">sorted_lengths</span><span class="p">,</span> <span class="n">sorted_idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">length_list</span><span class="p">,</span> <span class="n">descending</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>

<span class="n">input_sequence</span> <span class="o">=</span> <span class="n">input_sequence</span><span class="p">[</span><span class="n">sorted_idx</span><span class="p">]</span>
</code></pre></div></div>

<h2 id="nnutilsrnnpack_padded_sequenceinput-length-batch_firsttrue">nn.utils.rnn.pack_padded_sequence(input, length, batch_first=True)</h2>

<ol>
  <li>ç›®çš„ï¼šä¸ºäº†RNNè®­ç»ƒæ—¶çš„mini-batch trainingï¼Œæ‰€ä»¥æœ‰è¡¥é½ä¸€ä¸ªminibatchä¸­æ•°æ®é•¿åº¦çš„éœ€è¦ï¼Œå¦‚æœæ˜¯æ¯æ¬¡ä¸€ä¸ªæ ·æœ¬çš„è¯ï¼Œå°±æ²¡æœ‰è¿™ä¸ªå¿…è¦äº†ã€‚</li>
  <li>è¾“å…¥ï¼šinputæ˜¯ä¸€ä¸ª<strong>ç»è¿‡paddingï¼Œembeddingï¼Œdescending_orderä¹‹åçš„</strong>ä¸‰ç»´tensorã€‚lengthæ˜¯ä¸€ä¸ªæ¯ä¸€è¡Œæ ·æœ¬æœ‰æ•ˆé•¿åº¦åºåˆ—ã€‚</li>
  <li>è¾“å‡ºï¼šè¿”å›ä¸€ä¸ªPackedSequenceå¯¹è±¡ï¼Œå¯ä»¥ç›´æ¥è¾“å…¥åˆ°RNNç½‘ç»œä¸­ã€‚</li>
</ol>

<h2 id="nnutilsrnnpad_packed_sequenceoutput-batch_firsttrue">nn.utils.rnn.pad_packed_sequence(output, batch_first=True)</h2>

<h1 id="å·ç§¯">å·ç§¯</h1>

<h2 id="nnconv2din_channels-out_channels-kernel_size">nn.Conv2d(in_channels, out_channels, kernel_size)</h2>

<p><strong>out_channelæ˜¯å·ç§¯æ ¸çš„æ•°é‡</strong>ï¼Œin_channelåœ¨è¾“å…¥å›¾ç‰‡çš„é€šé“æ•°ï¼Œè€Œå·ç§¯æ ¸ï¼ˆä¸‰ç»´çš„ï¼Œæ·±åº¦è‡ªåŠ¨ç­‰äºin_channelsï¼‰çš„æ·±åº¦é»˜è®¤å’Œè¾“å…¥é€šé“ä¸€è‡´ï¼Œè™½ç„¶æœ‰æ·±åº¦ä½†æ˜¯æ‰€æœ‰ä½ç½®æ•°å­—ç›¸ä¹˜åŠ èµ·æ¥ï¼Œç®—æ³•ä¸€æ ·çš„ã€‚æ‰€ä»¥ä¸€ä¸ªkernelæŠŠå¤šé€šé“çš„å›¾ç‰‡å‹ç¼©ä¸ºä¸€é€šé“çš„å¹³é¢çŸ©é˜µï¼Œæ‰€ä»¥æœ‰å¤šä¸ªkernelå°±ä¼šäº§ç”Ÿå¤šä¸ªchannelå †å ã€‚è¿™ä¸ªout_channelä¹Ÿå°±æ˜¯å†æ¥å…¥ä¸‹ä¸€å±‚çš„in_channelå¤§å°ã€‚</p>

<ul>
  <li>è™½ç„¶æ˜¯2dï¼Œå·ç§¯æ ¸ä¾ç„¶æ˜¯3ç»´æœ‰æ·±åº¦çš„ã€‚</li>
  <li>ä¸€ä¸ªå·ç§¯æ ¸å¯¹åº”ç€ä¸€ä¸ªout_channelï¼Œç”Ÿæˆä¸€å¼ feature mapã€‚</li>
  <li>Kernel_sizeä¹Ÿå¯ä»¥å«patch sizeï¼Œæ¯æ¬¡å…³æ³¨çš„åŒºåŸŸã€‚</li>
</ul>

<p>è¾“å…¥æ•°æ®<code class="highlighter-rouge">(batch, in_channels, height, width)</code>ï¼Œå¾—åˆ°<code class="highlighter-rouge">(batch, out_channels, new_height, new_width)</code>ï¼Œä»å›¾ç‰‡è§’åº¦è®²channelè‡ªç„¶è€Œç„¶çš„æ”¾åˆ°ç¬¬äºŒä¸ªç»´åº¦ã€‚</p>

<h2 id="nnconv1din_channels-out_channels-kernel_size">nn.Conv1d(in_channels, out_channels, kernel_size)</h2>

<p>ç±»æ¯”ï¼Œ<strong>in_channelæ˜¯ä¸€ä¸ªä¼šè‡ªåŠ¨è¢«kernelè¦†ç›–çš„é‡</strong>ï¼Œæˆ‘æƒ³è¦†ç›–æ•´ä¸ªè¯å‘é‡æ‰€ä»¥in_channelæ˜¯embedding_sizeï¼Œkernelæ­¤æ—¶åªæ˜¯äºŒç»´çš„ï¼Œç¬¬ä¸€ä¸ªç»´åº¦æ˜¯æŒ‡å®šçš„ï¼Œç¬¬äºŒä¸ªç»´åº¦è‡ªåŠ¨è¦†ç›–in_channelã€‚</p>

<p>æ³¨æ„è¾“å…¥æ•°æ®ç»´åº¦ä¸2dä¿æŒä¸€è‡´ï¼Œ<code class="highlighter-rouge">(batch, in_channels, length)</code>ï¼Œå³<strong>è¯å‘é‡çš„ç»´åº¦embedding_sizeè¦æ”¾åœ¨ä¸­é—´</strong>ï¼Œè€Œä¸€èˆ¬æƒ…å†µä¸‹<strong>ä»Embeddingå–å‡ºæ¥çš„è¯å‘é‡embedding_sizeå¤„äºæœ€åä¸€ä¸ªç»´åº¦</strong>ï¼Œæ‰€ä»¥è¦äº¤æ¢ç»´åº¦ã€‚
æœ€åå¾—åˆ°çš„è¾“å‡ºä¹Ÿä¸2dä¿æŒä¸€è‡´ï¼Œ<code class="highlighter-rouge">(batch, out_channels, new_length)</code>ã€‚</p>

<p>å·ç§¯è¿‡ç¨‹ç†è§£ä¸ºï¼Œå…ˆæŠŠå½¢çŠ¶è½¬ç½®ä¸º(N, C, L)ï¼Œç„¶åå·ç§¯æ ¸æ¨ªç€èµ°ä¸€éï¼Œå¾—åˆ°ä¸€ä¸ª<strong>è¡Œå‘é‡ï¼Œä½œä¸ºä¸€ä¸ªchannelçš„å€¼</strong>ï¼Œç„¶åä¸‹ä¸€ä¸ªkernelå†æ¥å·ä¸€éï¼Œå¾—åˆ°ä¸‹ä¸€è¡Œï¼Œæ‰€ä»¥æœ€ç»ˆè¡Œæ•°ç­‰äºout_channelsï¼Œåˆ—æ•°æ–°å®šã€‚</p>

<h3 id="å¯¹æ¯”2då’Œ1d">å¯¹æ¯”2då’Œ1d</h3>

<p>åœ¨2dä¸­ï¼Œæ¯ä¸ªåƒç´ ç‚¹çš„å±æ€§æ˜¯ä¸‰ç»´çš„(RGB)ï¼Œæ‰€ä»¥ä¸€ä¸ªkernelå¯¹åº”äºæ¯ä¸ªç‚¹çš„ä¹˜åŠ ç³»æ•°ä¹Ÿæ˜¯ä¸‰ç»´çš„(ä¸‰ä¸ªæ•°)ã€‚</p>

<p>è€Œåœ¨1dä¸­ï¼Œæ¯ä¸ªè¯çš„å±æ€§æ˜¯embedding_sizeé‚£ä¹ˆå¤šç»´åº¦çš„ï¼Œæ‰€ä»¥ä¸€ä¸ªkernelå¯¹åº”äºæ¯ä¸ªç‚¹çš„ä¹˜åŠ ç³»æ•°æ˜¯embedding_sizeç»´åº¦çš„ã€‚</p>

<p>åœ¨å®ç°ä¸­ï¼Œæ¯ä¸ªç‚¹æœ‰å¤šå°‘ä¸ªå±æ€§å°±å«in_channelsï¼Œkernel_sizeä¸­ä¸ç”¨æŒ‡å®šè¿™ä¸ªç»´åº¦ï¼Œæ‰€ä»¥åœ¨å›¾åƒä¸­åªéœ€è¦æŒ‡å®šå‰©ä½™çš„2då¹³é¢ä¸Šçš„(axb)ä¸ºsizeï¼Œè€Œåœ¨æ–‡æœ¬1då·ç§¯ä¸­åªéœ€è¦(k)è¿™ä¸ªpatch sizeå°±å¯ä»¥äº†ã€‚</p>

<p><img src="/img/conv1d.png" alt="" /></p>

<h2 id="nnmaxpool1d">nn.MaxPool1d</h2>

<p>ä¸€ç»´å·ç§¯æ˜¯è¦†ç›–æ‰€æœ‰in_channelï¼ˆå·ç§¯æ ¸å®è´¨ä¸Šä¹Ÿç†è§£ä¸ºäºŒç»´çš„ï¼‰ï¼Œä½†æ˜¯ä¸€ç»´æ± åŒ–ä¸æ˜¯ï¼Œä¸€ç»´æ± åŒ–çš„kernelå°±æ˜¯ä¸€ç»´çš„çª—(1xkernel_size)åœ¨æ¯ä¸€ä¸ªchannelä¸Šèµ°ï¼Œ<strong>é»˜è®¤stride=kernel_sizeï¼ˆconvé»˜è®¤stride=1ï¼‰</strong>ï¼Œä¸äº¤å ï¼Œå¯ä»¥è®¾ç½®strideæ¥æ”¹å˜ã€‚</p>

<p>è¾“å…¥<code class="highlighter-rouge">(N, Channel, Length)</code>ï¼Œè¾“å‡º<code class="highlighter-rouge">(N, Channel, New_Length)</code>ï¼Œå‰ä¸¤ä¸ªç»´åº¦ä¸å˜ã€‚</p>

<h2 id="nnconvtranspose1din_channels-out_channels-kernel_size-padding">nn.ConvTranspose1d(in_channels, out_channels, kernel_size, padding)</h2>

<ol>
  <li><strong>æ ¹æ®paddingå€¼æ·»åŠ (kernel_size - 1 - padding)ä¸ªzero-padding</strong>ï¼Œå³paddingå°±ç®—ç­‰äº0ï¼Œä¹Ÿä¼šæ·»åŠ kernel_size-1ä¸ªé›¶ã€‚</li>
  <li>è¿™ä¹ˆåšæ˜¯ä¸ºäº†è®©ç›¸åŒå‚æ•°çš„Convå±‚å’ŒConvTransposeå±‚å¯ä»¥æ˜¯äº’ä¸ºå¯¹åº”çš„ï¼Œåœ¨ç»´åº¦ä¸Šå¯ä»¥äº’ç›¸æ¢å¤ã€‚</li>
  <li>è®¡ç®—TransConvçš„è¾“å‡ºç»´åº¦æ—¶ï¼Œç”¨è®¡ç®—Conv1dçš„åå‘ç®—æ³•ï¼Œå³è®¤ä¸ºæˆ‘ä»¬è¦æ±‚çš„è¾“å‡ºlengthæ˜¯Conv1dçš„è¾“å…¥ï¼Œåè¿‡æ¥ç®—ï¼Œ<strong>æ— è§†ä¸Šé¢çš„1æ¡ã€‚</strong></li>
</ol>

<h1 id="æ‰¹æ ‡å‡†åŒ–">æ‰¹æ ‡å‡†åŒ–</h1>

<h2 id="nnbatchnorm1d">nn.BatchNorm1d()</h2>

<p>æ¨¡å‹æ”¶æ•›çš„æ›´åŠ å¿«é€Ÿï¼Œå¯¹æ¯ä¸ªdimensionè¿›è¡Œnormalizationã€‚</p>

<p>åœ¨testçš„æ—¶å€™åªæœ‰1ä¸ªdataè¿›æ¥ï¼Œä¸èƒ½åœ¨batchä¸Šè®¡ç®—å‡å€¼å’Œæ–¹å·®äº†ï¼Œæ‰€ä»¥åœ¨testçš„æ—¶å€™ï¼Œè®¡ç®—æ•´ä¸ªè®­ç»ƒé›†çš„å‡å€¼å’Œæ–¹å·®ï¼Œåº”ç”¨åœ¨testçš„æ—¶å€™ã€‚</p>

<h1 id="numpy">Numpy</h1>

<h2 id="xtolist">x.tolist()</h2>

<p>npçš„å…ƒç´ æ‰“å°å‡ºæ¥è™½ç„¶ä¹Ÿæœ‰ä¸ªä¸­æ‹¬å·ï¼Œä½†æ˜¯æ¯•ç«Ÿæ˜¯numpyç±»ï¼Œä¸”æ‰“å°å‡ºæ¥ä»¥ç©ºæ ¼éš”å¼€çš„ã€‚è½¬ä¸ºlistä»¥é€—å·éš”å¼€ã€‚</p>

<h2 id="nprandintlow-highnone-size">np.randint(low, high=None, size)</h2>

<p>è‹¥æ˜¯np.randint(5, size=10)é»˜è®¤5æ˜¯ä¸Šé™ï¼Œç”Ÿæˆä¸€ä¸ªä¸€ç»´çš„ä¸è¶…è¿‡5çš„10ä¸ªæ•°ã€‚</p>

<h2 id="npcopyx">np.copy(X)</h2>

<p>è‹¥æ˜¯ç›´æ¥<code class="highlighter-rouge">y=x</code>åˆ™ä¸¤ä¸ªå˜é‡æŒ‡å‘åŒä¸€å†…å­˜ç©ºé—´ï¼Œä¿®æ”¹ä¸€ä¸ªå¦ä¸€ä¸ªä¹Ÿè·Ÿç€åŒæ—¶æ”¹å˜ã€‚è‹¥æ˜¯<code class="highlighter-rouge">y = np.copy(x)</code>åˆ™å¼€è¾Ÿäº†ä¸€å—æ–°çš„å†…å­˜ç©ºé—´ï¼Œä¸å…±äº«ã€‚</p>

<h2 id="åŠ æ³•æ“ä½œ">åŠ æ³•æ“ä½œ</h2>

<p>åœ¨listæ“ä½œä¸­ï¼Œç›´æ¥ä¸¤ä¸ª<code class="highlighter-rouge">[]</code>listä½œåŠ æ³•ï¼Œæ˜¯è¿æ¥èµ·æ¥ã€‚ä½†æ˜¯å¦‚æœæ˜¯ä¸¤ä¸ªnumpyï¼Œä½œåŠ æ³•ï¼Œæ˜¯broadcastã€‚</p>

<h1 id="å®ç°word_dropout">å®ç°word_dropout</h1>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">word_dropout_rate</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
	<span class="c1"># randomly replace decoder input with &lt;unk&gt;
</span>	<span class="n">prob</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">input_sequence</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
	<span class="n">prob</span><span class="p">[(</span><span class="n">input_sequence</span><span class="o">.</span><span class="n">data</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">sos_idx</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">input_sequence</span><span class="o">.</span><span class="n">data</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">pad_idx</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
	<span class="n">decoder_input_sequence</span> <span class="o">=</span> <span class="n">input_sequence</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>
	<span class="n">decoder_input_sequence</span><span class="p">[</span><span class="n">prob</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">word_dropout_rate</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">unk_idx</span>
	 <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">decoder_input_sequence</span><span class="p">)</span>
</code></pre></div></div>

<h1 id="å†…å­˜åˆ†é…">å†…å­˜åˆ†é…</h1>

<p>å¯ä½¿ç”¨<code class="highlighter-rouge">torch.cuda.memory_allocated(device)</code>æŸ¥çœ‹å†…å­˜ä½¿ç”¨æƒ…å†µï¼Œå•ä½æ˜¯bytesï¼Œé™¤10^9è½¬æ¢ä¸ºGBã€‚</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">print_allocated_memory</span><span class="p">(</span><span class="n">device</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"{:.2f} GB"</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">memory_allocated</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="o">/</span> <span class="mi">1024</span> <span class="o">**</span> <span class="mi">3</span><span class="p">))</span> <span class="c1"># å…ˆç®—power
</span></code></pre></div></div>

<p>æ³¨æ„è¿™é‡Œçš„deviceæ˜¯ä¸€ä¸ªtorch.deviceå±æ€§å˜é‡ã€‚</p>

<h1 id="æ¢¯åº¦è£å‰ª">æ¢¯åº¦è£å‰ª</h1>

<p><code class="highlighter-rouge">torch.nn.utils.clip_grad_norm_(model.parameters(), clip)</code>ï¼Œclip is a float number.</p>

:ET