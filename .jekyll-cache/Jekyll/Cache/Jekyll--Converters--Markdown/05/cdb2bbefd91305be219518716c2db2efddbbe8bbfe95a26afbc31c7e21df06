I"5<p>最近读PyTorch文档，写得很精彩。里面NLP应用章节介绍到一个简单的词袋（Bag-of-Words）模型，记录一下。</p>

<h1 id="概念">概念</h1>

<p>词袋已经是十几年前的概念了，每次学到这种古老的东西就有种学也不是，绕也绕不过去的感觉……</p>

<p>词袋的核心思想是单纯统计文本中每个词出现的频率，而与其出现的顺序无关。就像把一篇文章所有的词倒入一个袋子中，只在乎每个词出现几次，而不在乎其出现的位置。利用生成的词袋向量来表征这篇文章，常用于分类任务。目前词袋模型也推广到图片分类任务中了，将图片的特征看作词，获得图片的词袋向量。</p>

<p>例如我们的词表内只有两个单词”hello”和”world”，分别位于位置1和0，那句子”hello world”的词袋向量（BoW Vector）就是[1, 1]，”hello hello world”就是[2, 1]。</p>

<p>将这个BoW向量作为输入，经过softmax层，就可以得到属于各个类别的概率。</p>

<h1 id="实现">实现</h1>

<h2 id="从语料库构建词袋">从语料库构建词袋</h2>

<p>构建词到向量的映射，首先构建词袋。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span> <span class="o">=</span> <span class="p">[(</span><span class="s">"me gusta comer en la cafeteria"</span><span class="o">.</span><span class="n">split</span><span class="p">(),</span> <span class="s">"SPANISH"</span><span class="p">),</span>
        <span class="p">(</span><span class="s">"Give it to me"</span><span class="o">.</span><span class="n">split</span><span class="p">(),</span> <span class="s">"ENGLISH"</span><span class="p">),</span>
        <span class="p">(</span><span class="s">"No creo que sea una buena idea"</span><span class="o">.</span><span class="n">split</span><span class="p">(),</span> <span class="s">"SPANISH"</span><span class="p">),</span>
        <span class="p">(</span><span class="s">"No it is not a good idea to get lost at sea"</span><span class="o">.</span><span class="n">split</span><span class="p">(),</span> <span class="s">"ENGLISH"</span><span class="p">)]</span>

<span class="n">test_data</span> <span class="o">=</span> <span class="p">[(</span><span class="s">"Yo creo que si"</span><span class="o">.</span><span class="n">split</span><span class="p">(),</span> <span class="s">"SPANISH"</span><span class="p">),</span>
             <span class="p">(</span><span class="s">"it is lost on me"</span><span class="o">.</span><span class="n">split</span><span class="p">(),</span> <span class="s">"ENGLISH"</span><span class="p">)]</span>

<span class="n">word_to_ix</span> <span class="o">=</span> <span class="p">{}</span>
<span class="k">for</span> <span class="n">sent</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">data</span> <span class="o">+</span> <span class="n">test_data</span><span class="p">:</span>
	<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sent</span><span class="p">:</span>
		<span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">word_to_ix</span><span class="p">:</span>
			<span class="n">word_to_ix</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_to_ix</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">word_to_ix</span><span class="p">)</span>

<span class="n">VOCAB_SIZE</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">word_to_ix</span><span class="p">)</span>
<span class="n">NUM_LABELS</span> <span class="o">=</span> <span class="mi">2</span>
</code></pre></div></div>

<ol>
  <li>观察数据结构，是以tuple为元素的list。</li>
  <li><code class="highlighter-rouge">string.split()</code>函数以参数为分隔符切割字符串，返回一个多string元素的list。</li>
  <li>for循环中可以直接用<code class="highlighter-rouge">+</code>连接两个list。</li>
  <li>构建词表部分，注意dict的key和value，key是索引，这里以词为索引，序号为值，所以名为word_to_ix。</li>
  <li>注意从0开始算，len(dict)是下一个元素序号。</li>
  <li>词袋构建完成后，VOCAB_SIZE表示总的词袋大小，NUM_LABELS表示分类标签，这里为不同的语言。</li>
</ol>

<h2 id="定义网络">定义网络</h2>

<p>这里用一层FC+softmax直接分类。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">BoWClassifier</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
	<span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">num_labels</span><span class="p">):</span>
		<span class="nb">super</span><span class="p">(</span><span class="n">BoWClassifier</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
		<span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">num_labels</span><span class="p">)</span>

	<span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">bow_vec</span><span class="p">):</span>
		<span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">bow_vec</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<p>分类问题在FC之后，每行为一个样本的输出，所以log_softmax的dim设置为1，表示按行来归一化并求log。注意softmax之后每个元素的范围(0, 1)，再取log都是负的，越接近于0表示概率越大。</p>

<p>使用<code class="highlighter-rouge">log_softmax</code>的原因是普通softmax的输出值域仅在(0, 1)内，经过对数运算可以提高精度。</p>

<h2 id="对输入生成bow向量">对输入生成BoW向量</h2>

<p>注意逻辑，上面先利用所有的语料来生成词袋，这里针对每一句输入（不管是训练集还是测试集中的句子），生成一个属于它们自己的词袋向量。</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">sent_to_vec</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">word_to_ix</span><span class="p">):</span>
	<span class="n">vector</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">VOCAB_SIZE</span><span class="p">)</span>
	<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">sentence</span><span class="p">:</span>
		<span class="n">vector</span><span class="p">[</span><span class="n">word_to_ix</span><span class="p">(</span><span class="n">word</span><span class="p">)]</span> <span class="o">+=</span> <span class="mi">1</span>
	<span class="k">return</span> <span class="n">vector</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<ol>
  <li>注意初始化时Tensor要初始化为全0向量。</li>
  <li>BoW Vector的意义，统计每个词出现的频率，而与顺序无关。</li>
  <li>PyTorch中一个样本一般用一行来表示。</li>
</ol>

<h2 id="标签向量化">标签向量化</h2>

<p>不同的标签用不同的向量表示。</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">label_to_ix</span> <span class="o">=</span> <span class="p">{</span><span class="s">"SPANISH"</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s">"ENGLISH"</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span>

<span class="k">def</span> <span class="nf">make_target</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">label_to_ix</span><span class="p">):</span>
	<span class="k">return</span> <span class="n">t</span><span class="o">.</span><span class="n">LongTensor</span><span class="p">([</span><span class="n">label_to_ix</span><span class="p">[</span><span class="n">label</span><span class="p">]])</span>
</code></pre></div></div>

<ol>
  <li>标签要用LongTensor类型。</li>
  <li>注意这里生成LongTensor用的是一个实际的List，最外层有中括号的。</li>
</ol>

<h2 id="训练过程">训练过程</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">loss_function</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">NLLLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
	<span class="k">for</span> <span class="n">instance</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
		<span class="n">mode</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
		<span class="n">bow_vec</span> <span class="o">=</span> <span class="n">sent_to_vec</span><span class="p">(</span><span class="n">instance</span><span class="p">,</span> <span class="n">word_to_ix</span><span class="p">)</span>
		<span class="n">target</span> <span class="o">=</span> <span class="n">make_target</span><span class="p">(</span><span class="n">label</span><span class="p">,</span> <span class="n">label_to_ix</span><span class="p">)</span>
		<span class="n">log_probs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">bow_vec</span><span class="p">)</span>
		<span class="n">loss</span> <span class="o">=</span> <span class="n">loss_function</span><span class="p">(</span><span class="n">log_probs</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
		<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
		<span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</code></pre></div></div>

<ol>
  <li><code class="highlighter-rouge">nn.NLLLoss()</code>是Negative Log Likelihood Loss，与<code class="highlighter-rouge">nn.CrossEntropyLoss()</code>的区别是，后者继承了<strong>LogSoftmax + NLLLoss</strong>，而前者需要在网络的最后一层使用log_softmax函数。即<strong>CrossEntropy = LogSoftmax + NLLLoss</strong>。</li>
  <li>新的epoch开始时要梯度清零<code class="highlighter-rouge">model.zero_grad()</code>，因为是是累积的。</li>
</ol>

<h2 id="测试过程">测试过程</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">t</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
	<span class="k">for</span> <span class="n">instance</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">test_data</span><span class="p">:</span>
		<span class="n">bow_vec</span> <span class="o">=</span> <span class="n">sent_to_vec</span><span class="p">(</span><span class="n">instance</span><span class="p">,</span> <span class="n">word_to_ix</span><span class="p">)</span>
		<span class="n">log_probs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">bow_vec</span><span class="p">)</span>
		<span class="k">print</span><span class="p">(</span><span class="n">log_probs</span><span class="p">)</span>
</code></pre></div></div>

<p>测试过程必须用<code class="highlighter-rouge">torch.no_grad()</code>来wrap起来，否则将在计算图中引入不必要的麻烦。</p>
:ET