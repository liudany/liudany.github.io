I"%<h1 id="word-embedding">word embedding</h1>

<p>representations of words.</p>

<h2 id="word-meaning">word meaning</h2>

<p>一个词想表征出来的想法/物体。</p>

<p>最初使用WordNet等taxonomy(分类学)的方法，是一种语义网络，有着hypernym上位词，同义词等等，是一种离散化的表征。这种方法存在以下问题：</p>

<ol>
  <li>这种方法missing nuances（细微差别），缺乏词语之间的关联和细微差别，且缺少新词，很难keep up to date。</li>
  <li>discrete representation的问题是vector space是one-hot encoding，太长了。</li>
  <li>没给出词之间的内在联系。例如搜索中，有时要搜索相近的词，one-hot很难表征similarity。</li>
</ol>

<p>改进想法是利用dense vector to reprensente word，利用dot计算来求相似性，dot作为一种简单的计算方式很适合衡量similarity。bigger if more similar.</p>

<p>利用<strong>distribution similatiry</strong>来计算一个词义，利用分布相似性！<strong>通过一个词上下分布的词语来构建词语的意义。</strong>词汇的意义，依赖于他的上下文，也可以说由上下文来决定。</p>

<h2 id="word2vec">word2vec</h2>

<p>Neural word embedding，是通过训练得到的representation，想法是让<strong>词向量可以预测周围的词语</strong>，整个训练过程也是通过中心词预测窗口内词语来进行的。</p>

<h3 id="skip-gram">skip-gram</h3>

<h4 id="model">model</h4>

<p>每一步取一个词作为center word. predict一定窗口大小内context中的词语，调整词向量使得最大化上下文词语所出现的概率。</p>

<p><img src="/img/skip-gram.jpeg" alt="" /></p>

<ol>
  <li>注意矩阵维度，one-hot编码的V维向量是每个词语的标签，V表示词表大小。而词向量是一个d维的列向量，表征词语的语义。</li>
  <li>center word和context是两个不同的矩阵，维度是相似的。</li>
  <li>centor word的词向量和context做dot product来计算similarity，得到的维度是V维，每个位置是该位置对应的词在context中出现的概率。</li>
  <li>假设词向量维度100，词表大小20,000，那么参数矩阵共有2000000个，但是每次我们更新的其实只有一个窗口内的词对应部分的向量，若窗口为5则每次我们其实只更新500个参数，其他位置都是0，所以是非常sparse的一个matrix。所以每次只更新出现的词的参数 就可以了。</li>
  <li>用非常小的数字初始化。</li>
</ol>

<h4 id="remark">Remark</h4>

<ul>
  <li>
    <p>Loss_func：其中T为center word数量，m为窗口大小，v为词表大小。
<script type="math/tex">\begin{array} { l } { J ( \theta ) = - \frac { 1 } { T } \log L ( \theta ) = - \frac { 1 } { T } \sum _ { t = 1 } ^ { T } \sum _ { - m \leq j \leq m } \log P \left( w _ { t + j } | w _ { t } ; \theta \right) } \\\\ { P ( o | w ) = \frac { \exp \left( u _ { w } ^ { T } v _ { c } \right) } { \sum _ { w = 1 } ^ { v } \exp \left( u _ { W } ^ { T } v _ { C } \right) } } \end{array}</script></p>
  </li>
  <li>
    <p>Negative Log Likelyhood: Log：乘法变成加法。前面加个1/T平均一下。Negative：大家往往喜欢最小化一个东西，所以我们在log后面加个负号。</p>
  </li>
  <li>
    <p>NLL和Cross-entropy的关系：<strong>多分类问题中，instance标签为one-hot encoding，输出经过softmax层后，再做NLL就和cross entropy是等价的。</strong>利用one-hot编码，只预测真正出现的这个词，corss-entropy各项中就只剩下要预测的这个词的负对数概率。</p>
  </li>
  <li>
    <p>关于softmax：<strong>number -&gt; prob.</strong> 指数是因为都转化成正的。归一化。为什么叫softmax，因为<strong>取指数操作基本上就是保留最大的那个了</strong>，跟取最大值类似。但是是个软性的。</p>
  </li>
  <li>
    <p>这里的softmax分母部分是整个context矩阵与centor word的dot product，因为要预测词表中每一个词出现的概率，而不仅仅是窗口中的这几个词。</p>
  </li>
  <li>
    <p><strong>2 vectors for each word, centor and context.</strong></p>
  </li>
  <li>
    <p>不关心词出现的位置，只要在窗口之内的算概率就完事儿了。</p>
  </li>
  <li>
    <p>Stochastic gradient descent：用SGD是避免整个epoch求梯度浪费太久，看起来不靠谱but works like a gem。</p>
  </li>
</ul>

<h4 id="negative-sampleing">negative sampleing</h4>

<p>上面的loss function在计算时分母运算量很大，每个window更新一次，所以下一次窗口移动后必须要重新计算所有的dot product的和，因此改为按P(w)来随机抽取k个负样本做运算。</p>

<script type="math/tex; mode=display">J _ { t } ( \theta ) = \log \sigma \left( u _ { o } ^ { T } v _ { c } \right) + \sum _ { j \sim P ( w ) } \left[ \log \sigma \left( - u _ { j } ^ { T } v _ { c } \right) \right]</script>

<p>上面是第T个时间步或者说第T个窗口的损失，注意正负号，第一项为real outside word出现的概率，第二项为随机抽取的负样本出现的概率。注意正负号，最大化第一项，最小化第二项。</p>

<h3 id="the-continous-bag-of-words-modelcbow">The continous bag of words model(CBOW)</h3>

<p>从周围<strong>词向量的和</strong>去预测中心词，而不是通过中心词去预测周围单个词出现的概率（Skig-gram）。</p>

<p><img src="/img/cbow-skip-gram.png" alt="" /></p>

<h3 id="co-occurrence-matrix-based-基于共现矩阵">Co-occurrence matrix based 基于共现矩阵</h3>

<p>首先是一个对称阵，你出现在我旁边我就出现在你旁边。直接用这个矩阵的行列作为词向量也行，但是有不足之处：</p>

<ul>
  <li>新词的加入会对其他词产生影响，都要改变。</li>
  <li>维度为词表大小，太大了。</li>
</ul>

<p>所以需要进行降维，但是依然要保留重要信息，通常维持在25-10000维。降低维度的方法是<strong>SVD分解</strong>。</p>

<p>Hack we can do：</p>

<ul>
  <li>有一些词语产生一些噪声，比如the, he, has没有实际的含义，所以我们把他们在共现矩阵中的值设置一个上限，例如100。或者直接无视掉它们。</li>
  <li>根据离中心词的距离，设置不同的权重。例如紧挨着中心词，在共现矩阵中记为1，离着3步以上记为0.5等。</li>
  <li>计算词相关性(Pearson correlations)代替单纯的记数，设置副样本值为0。</li>
</ul>

<h3 id="count-based-vs-window-based">count-based vs window-based</h3>

<ul>
  <li>count-based更倾向于词的相似性，计数量大的词得到了不应有的重要性（有一些功能词）。</li>
  <li>window-based遍历语料效率低（scale with corpus size缩放语料），没有用到语料的全局统计信息。</li>
  <li>实际证明skig-gram是比较稳的。</li>
</ul>

<p>结合两者，得到了GloVe(Global Vector)。</p>

<h3 id="glove">GloVe</h3>

<p>结合计数和窗口的优点，将目光放在词语出现在全局的次数，并不像skip-gram一样走窗口的模式，将损失函数优化为：</p>

<script type="math/tex; mode=display">J ( \theta ) = \frac { 1 } { 2 } \sum _ { i , j = 1 } ^ { W } f \left( P _ { i j } \right) \left( u _ { i } ^ { T } v _ { j } - \log P _ { i j } \right) ^ { 2 }</script>

<p>其中P是共现矩阵，u和v分别是词的行向量和列向量，W是词表大小。最终的词向量x=u+v，因为这两个向量都包含了信息。</p>

<p>即使在skig-gram中也会把context vector和centor vector加起来作为最终的词向量，这是因为<strong>实践得出的效果好</strong>。</p>

<h2 id="多义词">多义词</h2>

<p>一个词有很多意思，用很多意思的线性叠加作为最终的词向量。</p>

<p>使用sparse coding方法线性叠加context vectors，可以恢复出多义词不同的语义。</p>

<h2 id="evaluate-word-vectors">evaluate word vectors</h2>

<p>NLP的evalutaion一般分为两种，<strong>intrinsic和extrinsic</strong>。Intrinsic是着眼于某个中间目标，或者子任务的结果，比如vector differences or similarities。Extrinsic着眼于一个高层级的目标，直接看最终结果，比如词向量直接用于翻译任务中看BLEU。</p>

<h3 id="intrinsic-evaluation">intrinsic evaluation</h3>

<h4 id="word-vector-analogies">Word Vector Analogies</h4>

<p>word2vec提出的<strong>Word Vector Analogies</strong>词向量类比方法，词向量之间做加减法，再用余弦相似度捕捉相似度最大的词看看准确与否。</p>

<p>Man和Woman的向量差，和King与Queen的向量差是类似的，Slow/Slower和Short/Shorter也是类似的等等。已经有数据集了，是四元组的形势（CITY1-COUNTRY1-CITY2-COUNTRY2），可以下载。</p>

<p>已有的结果表明，1000维度的向量并不会比300维的更能播捉相似性（在WVA任务中取得更高分）。但可以肯定的是数据越多表现越好！窗口大小为8比较适合GloVe。</p>

<h4 id="word-vector-correlation">Word Vector Correlation</h4>

<p>人工评估了一组词汇的相似度，然后用cosine相似度去衡量你的词向量是否表现出相同的特征。也可以下载了，WordSim353。</p>

<h3 id="extrinsic-evaluation">extrinsic evaluation</h3>

<p>选择不同的下游任务(downstream task)，命名实体识别是个不错的选择。</p>
:ET