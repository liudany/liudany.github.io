I"L.<p>Reading Notes of the graduation thesis of Hareesh Bahuleyan and tutorial of Carl Doersch.</p>

<h2 id="出发点">出发点</h2>

<p>最大化训练集中每个X经过我们生成过程之后<strong>重现</strong>的概率：
<script type="math/tex">P ( X ) = \int P ( X | z ; \theta ) P ( z ) d z</script>
由这个恒等式来看，我们取遍所有的z并让它经过生成模型P(X|z)，然后看生成的东西是X的概率。</p>

<table>
  <tbody>
    <tr>
      <td>⚠️我们指定生成模型P(X</td>
      <td>z)为一个<strong>各向独立的、方差确定的高斯分布</strong>，其均值由z通过神经网络得到。这里要注意<strong>均值</strong>的含义，<strong>均值是一个和样本拥有同样形状和单位的变量</strong>，即假设X是一张图片，这里的均值也是一张图片，理解为<strong>由当前的z，最可能生成的一个样本</strong>。</td>
    </tr>
  </tbody>
</table>

<p>⚠️<strong>再重复一遍，神经网络得到的是分布P（包括后面的Q）的均值，并没有得到一个分布，我们定义了参数化的分布族，有了均值就可以通过计算得到分布了。</strong></p>

<p><strong>换言之，生成网络根据z求均值mu的过程，实质上就是由z解码出其最可能的样本的过程。</strong></p>

<h2 id="pxz--pz">P(X|z) &amp; P(z)</h2>

<table>
  <tbody>
    <tr>
      <td>首先选择P(X</td>
      <td>z)分布要遵循的两个原则：</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>其<strong>参数空间是连续</strong>的，这样能保证其表达式<strong>可导</strong>，可以<strong>反向传播</strong>。</li>
  <li>要选便于使用神经网络建模的，即形式固定，只需要确定参数的分布，即<strong>参数化分布族</strong>。</li>
</ul>

<p>隐空间变量z是表征样本各种属性特征的，理应是一个十分复杂的分布，我们难以建模。但是<strong>再复杂的分布也可以通过把一个标准正态分布经过复杂的函数计算得到</strong>。所以我们<strong>直接把P(z)建模为标准正态分布</strong>，保证了生成阶段采样的便捷性。</p>

<table>
  <tbody>
    <tr>
      <td>这样P(X</td>
      <td>z)P(z)这个过程为：首先从标准正态分布中随机采样一个z，之后<strong>生成网络的前几层完成z到标准的特征向量之间的映射，生成网络的后几层根据这个中间的特征向量重构出其对应的样本</strong>。</td>
    </tr>
  </tbody>
</table>

<h2 id="sample">Sample</h2>

<p>这样最上面的公式中，只剩下积分符号了。积分往往是难处理的，这里我们采用采样操作来进行近似计算，首先将它看作一个均值操作：
<script type="math/tex">P ( X ) = \int P ( X | z ; \theta ) P ( z ) d z=E _ { z \sim P(z) } [P(X|z;\theta)]</script>
之后用采样的方法来估计均值：
<script type="math/tex">P ( X ) \approx \frac { 1 } { n } \sum _ { i } P \left( X | z _ { i } \right), z_i\sim P(z)</script>
但是由于这是一个高维空间，所以<strong>采样的次数要非常非常大才能完成一定精度的估计</strong>，且<strong>大部分的z其实并不能产生这一次生成过程针对的训练集样本X，即大部分抽取的z求出的P(X|z)几乎为0</strong>，所以这种计算方法效率很低。</p>

<h2 id="引入变分分布">引入变分分布</h2>

<p>所以我们倾向于<strong>采样那些更倾向于生成样本X的z</strong>，由此想到了<strong>真实</strong>后验概率P(z|X)，但是我们并不知道这个真实分布，所以期待使用Q(z|X)来模拟它，此时生成过程变为：
<script type="math/tex">E _ { z \sim Q ( z|X ) } [ P ( X | z ) ]</script>
但是这个过程和我们的最初目标P(X)之间是什么关系呢，利用Q(z|X)（有时省略这里的条件X，理解即可）和后验P(z|X)之间的KL距离来推导：
<script type="math/tex">D[Q(z)||P(z|X)]=E_{z\sim Q}[log\frac{Q(z)}{P(z|X)}]</script>
使用贝叶斯公式拆解真实后验概率P(z|X)的表达，引入P(z)和P(X|z)：
<script type="math/tex">\mathcal { D } [ Q ( z ) \| P ( z | X ) ] = E _ { z \sim Q } [ \log Q ( z ) - \log P ( X | z ) - \log P ( z ) ] + \log P ( X )</script>
移项，把变分分布Q的条件X加上去，得到最终的表达式：
<script type="math/tex">\log P ( X ) - \mathcal { D } [ Q ( z | X ) \| P ( z | X ) ] = E _ { z \sim Q } [ \log P ( X | z ) ] - \mathcal { D } [ Q ( z | X ) \| P ( z ) ]</script>
右边这个也叫做<strong>ELBO，也叫做variational lower bound</strong>，因为the second term on LHS of the equation是一个非负的KL距离，所以我们认为右边这个ELBO是logP(X)的下界，我们转而优化这个下界。</p>

<p><strong>⚠️优化ELBO可以concurrently优化左边的两项，一是最大似然，二是Q逼近真实后验。</strong></p>

<h2 id="变分分布的选择">变分分布的选择</h2>

<table>
  <tbody>
    <tr>
      <td>一般选择Q(z</td>
      <td>X)为高斯分布，原因跟P(X</td>
      <td>z)类似，因为是参数化的分布族，可以通过网络来学习参数。同样的，Q网络输出均值和方差，均值即当前的X对应的最可能的z值，方差为一定的噪声。</td>
    </tr>
  </tbody>
</table>

<p><strong>⚠️这里方差存在的意义是，如果没有方差，那么网络输出均值为z，这就是一个deterministic的网络，退化为一个普通的autoencoder了，并没有将输入映射到一个分布。</strong></p>

<h2 id="各项的意义">各项的意义</h2>

<h3 id="重构误差">重构误差</h3>

<p>第一项$E _ { z \sim Q } [ \log P ( X | z ) ]$代表一个重构的过程，这里和<strong>最初的意义相同，仍是对于所有的z求均值的过程</strong>，这依然是难以计算的，我们依然<strong>使用采样法来计算</strong>，即：
<script type="math/tex">E _ { z \sim Q } [ \log P ( X | z ) ] \approx \frac { 1 } { n } \sum _ { i } P \left( X | z _ { i } \right) , z _ { i } \sim Q ( z|X )</script>
得到一个好的估计仍然需要采样很多的z，所以我们<strong>借鉴SGD的思想，我们从Q中取一个z，并认为这样经过生成网络得到的X是一个上式的估计</strong>，即：
<script type="math/tex">E _ { z \sim Q } [ \log P ( X | z ) ] \approx  P \left( X | z  \right) , z  \sim Q ( z|X )</script>
所以重构过程简化为：根据X得到分布Q，从中取一个z，<strong>再通过生成网络得到一个均值，这个均值是当前的生成器根据z最倾向于生成的假样本</strong>，之后<strong>将均值带入正态分布的公式得到这个概率值</strong>。</p>

<p>所以，针对一个样本z和X，损失变为：
<script type="math/tex">\log P ( X | z ) - \mathcal { D } [ Q ( z | X ) \| P ( z ) ]</script>
但是总体优化目标还是在数据集上的：
<script type="math/tex">E _ { X \sim D } [ \log P ( X ) - \mathcal { D } [ Q ( z | X ) \| P ( z | X ) ] ] =E _ { X \sim D } \left[ \log P ( X | z ) - \mathcal { D } [ Q ( z | X ) \| P ( z ) ] \right]</script>
这里可以采用mini-batch的方法，对很多个X的梯度求平均，再下降。</p>

<p>** ⚠️刚才借鉴SGD是是针对于内部的z这个均值的一种处理方法，这里的mini-batch是针对X这一个均值上的。**</p>

<p>再往下看，假设生成模型算出的均值是f(z)，加上P为高斯分布和前面的log符号，那这个重构误差可写为：
<script type="math/tex">\log P ( X | z ) = C - \frac { 1 } { 2 } \frac{\| X - f ( z ) \| ^ { 2 }} { \sigma ^ { 2 }}</script>
这里平方项是欧式距离，分母是预先定义的方差项（生成模型的方差不是算出来的），和常数C（忽略）。</p>

<p><strong>这个sigma可以认为是重构误差和KLLoss之间的一个weighting factor，越大重构项的比重越小，所以这一项可以认为是我们希望模型重构X的精度。</strong></p>

<h3 id="kl-loss">KL Loss</h3>

<p>确定了Q的性质之后，又因为P(z)之前讨论过了是标准正态，所以两个高斯分布之间的KL散度表示为：
<script type="math/tex">KL = \frac { 1 } { 2 } \left( \operatorname { tr } ( \Sigma ( X ) ) + ( \mu ( X ) ) ^ { \top } ( \mu ( X ) ) - k - \log \operatorname { det } ( \Sigma ( X ) ) \right)</script>
其中k是z的维度。</p>

<p>这里注意<strong>KL项可以看作是一个广义的regularization parameter</strong>，相当于在重构任务的同时让模型也注意一下将这个Q分布往标准正态上面靠。<strong>没了KL，模型退化为Deterministic Autoencoder，模型将X映射到一个未知的latent space，然后对于某个点可以进行重构。有了这个KL，这个latent space将会被强制靠近N(0, 1)。</strong></p>

<h3 id="dqzxpzx">D[Q(z|X)||P(z|X)]</h3>

<p>这是我们前面忽略的一项，也是VAE中的一个折中的假设。只有在<strong>这项完全收敛到0之后</strong>模型才会如我们所想的运行，但是可能P是一个高斯分布无法拟合的分布，这就使得这一项不可能为0。</p>

<table>
  <tbody>
    <tr>
      <td>我们需要找到一个f是的P(z</td>
      <td>X)为一个高斯，且能够最大化logP(X)，但是这个的存在性仍然未知。</td>
    </tr>
  </tbody>
</table>

<h2 id="reparameterization">Reparameterization</h2>

<p>Sample操作是不能BP的，所以我们将从N(u,sigma)中采样操作改为从N(0, 1)采样并乘以<strong>根号sigma</strong>加上u。</p>

<h2 id="loss中的矛盾">Loss中的矛盾</h2>

<table>
  <tbody>
    <tr>
      <td>第一项我们称为<strong>重构误差</strong>，其表述的过程为：**先取一个样本x（从此x便固定了），之后通过p(z</td>
      <td>x)生成其隐向量z的分布空间，然后再利用生成模型q(x</td>
      <td>z)针对每个空间内的z去重构原来的x，这个条件概率表示生成模型所重构的像不像**。</td>
    </tr>
  </tbody>
</table>

<p>这个概率越大，Loss越小，即我们希望重构的像一些。</p>

<table>
  <tbody>
    <tr>
      <td>第二项称为KL损失，其表述为：**先取一个样本x（固定），之后通过p(z</td>
      <td>x)生成其隐向量的分布空间，然后衡量通过x生成的隐向量的分布，和从z的先验分布q(z)的分布（具有随机性）之间的距离，表示z的随机程度**。</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>这个距离越小，Loss越小，但**这表示p(z</td>
      <td>x)所提取的x的隐向量z更加的随机，没有任何辨识度，不能很好的蕴含x的信息**，这是我们不想要的。</td>
    </tr>
  </tbody>
</table>

<p>综合这两项，若重构误差小，说明z中很好的蕴含了x的信息，这时候z是不随机的，与随机抽取的q(z)距离肯定是大的，所以第二项KL损失就会大。这说明<strong><em>这两项之间是互相对抗的</em></strong>，增减性相反，不可以分开求最小值。</p>

<p>⚠️<strong>如果KL损失降到0，那z是完全随机的隐向量，其中几乎不蕴含x的信息，可以认为模型退化为一个语言模型（理论上讲这时候的重构误差永远比利用有效的z时要小，所以总损失不会达到最小值）。</strong></p>

<p>⚠️<strong>我们的出发点是构造的q(x, z)更加接近于p(x, z)，之后化简到上面的形式，所以总的损失的确是越小越好的。只是通过化简得到了这对抗的两部分，且增减性相反，若因为某些原因，KL散度倾向于最小化到0，那损失不会达到理论上的最小值，反而退化为语言模型，这时模型以一个有干扰（随机的z）的语言模型的形式去重构样本，背离了我们利用z的初衷。</strong></p>

<p>⚠️上面这一段我并不知道对不对</p>
:ET